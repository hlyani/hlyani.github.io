{"./":{"url":"./","title":"Introduction","keywords":"","body":" I always forget to tell you that how lucky I am to encounter you. &message=author&color=success\"> "},"notes/proxy.html":{"url":"notes/proxy.html","title":"proxy 相关","keywords":"","body":"proxy 相关 一、linux 代理 HTTP_PROXY=192.168.0.127:1080 COMMAND export {all_proxy,https_proxy,http_proxy,ftp_proxy,HTTP_PROXY,HTTPS_PROXY,FTP_RPOXY,ALL_PROXY}=http://127.0.0.1:1080 export no_proxy=\"localhost, 127.0.0.1, ::1\" unset proxy all_proxy https_proxy http_proxy ftp_proxy HTTP_PROXY HTTPS_PROXY FTP_RPOXY ALL_PROXY no_proxy NO_PROXY export proxy=127.0.0.1:33210 export http_proxy=http://$proxy export HTTP_PROXY=$http_proxy export https_proxy=$http_proxy export HTTPS_PROXY=$http_proxy export ftp_proxy=$http_proxy export FTP_RPOXY=$http_proxy export all_proxy=socks5://$proxy export ALL_PROXY=socks5://$proxy export no_proxy=\"localhost, 127.0.0.1, ::1\" export NO_PROXY=\"localhost, 127.0.0.1, ::1\" 二、curl 使用代理 curl -O XX --socks5 192.168.0.127:1080 三、wget 使用代理 1、修改环境变量 export http_proxy=http://192.168.0.127:1080 2、修改 /etc/wgetrc 或 ~/.wgetrc #You can set the default proxies for Wget to use for http, https, and ftp. # They will override the value in the environment. https_proxy = http://192.168.0.127:1080/ http_proxy = http://192.168.0.127:1080/ ftp_proxy = http://192.168.0.127:1080/ # If you do not want to use proxy at all, set this to off. use_proxy = on 3、使用 -e wget -e \"http_proxy=http://192.168.0.127:1080\" 4、安装 tsocks apt-get -y install tsocks vim /etc/tsocks.conf server = 192.168.0.127 server_type = 5 server_port = 1080 tsocks wget http://* 四、docker 使用代理 proxy mkdir -p /etc/systemd/system/docker.service.d echo '[Service] Environment=\"HTTP_PROXY=http://192.168.0.127:1080\" Environment=\"HTTPS_PROXY=http://192.168.0.127:1080\" Environment=\"NO_PROXY=localhost,127.0.0.1\"' | tee > /etc/systemd/system/docker.service.d/http-proxy.conf systemctl daemon-reload systemctl restart docker noohub.ru mkdir -p /etc/dockersudo tee /etc/docker/daemon.json 中科大 echo '{ \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn/\"] }' | tee /etc/docker/daemon.json systemctl daemon-reload systemctl restart docker 其他镜像源 { \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn/\", \"http://hub-mirror.c.163.com\", \"http://hub-mirror.c.163.com\"], \"insecure-registries\": [\"registry.docker-cn.com\", \"docker.mirrors.ustc.edu.cn\"], \"debug\": true, \"experimental\": true } # 使用中科大镜像源 docker pull docker.mirrors.ustc.edu.cn/library/mysql:5.7 # 使用 Azure 中国镜像源 docker pull dockerhub.azk8s.cn/library/mysql:5.7 dockerproxy dockerproxy.com { \"registry-mirrors\": [ \"https://dockerproxy.com\" ] } ustc ustc { \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn/\"] } Docker Hub docker pull stilleshan/frpc:latest docker pull nginx:latest docker pull dockerproxy.com/stilleshan/frpc:latest docker pull dockerproxy.com/library/nginx:latest ghcr.io - GitHub Container Registry docker pull ghcr.io/username/image:tag docker pull ghcr.dockerproxy.com/username/image:tag gcr.io - Google Container Registry docker pull gcr.io/username/image:tag docker pull gcr.dockerproxy.com/username/image:tag docker pull gcr.mirrors.ustc.edu.cn/xxx/yyy:zzz docker pull gcr.azk8s.cn/xxx/yyy:zzz k8s.gcr.io/registry.k8s.io - Google Kubernetes docker pull k8s.gcr.io/username/image:tag docker pull registry.k8s.io/username/image:tag docker pull k8s.dockerproxy.com/username/image:tag quay.io docker pull quay.io/username/image:tag docker pull quay.dockerproxy.com/username/image:tag docker pull quay.mirrors.ustc.edu.cn/xxx/yyy:zzz docker pull quay.azk8s.cn/xxx/yyy:zzz docker-wrapper docker_wrapper 一个 Python 编写的工具脚本，可以替代系统的 Docker 命令，自动从 Azure 中国拉取镜像并自动 Tag 为目标镜像和删除 Azure 镜像。 git clone https://github.com/silenceshell/docker-wrapper.git cp docker-wrapper/docker-wrapper.py /usr/local/bin/ docker-wrapper pull k8s.gcr.io/kube-apiserver:v1.14.1 azk8spull azk8spull 一个 Shell 编写的脚本，这个脚本功能和 docker-wrapper 类似。同样可以自动从 Azure 中国拉取镜像并自动 Tag 为目标镜像和删除 Azure 镜像。 git clone https://github.com/xuxinkun/littleTools cd littleTools chmod +x install.sh ./install.sh azk8spull quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.24.1 五、containerd使用代理 mkdir -p /etc/systemd/system/containerd.service.d echo '[Service] Environment=\"HTTP_PROXY=http://192.168.0.127:1080\" Environment=\"HTTPS_PROXY=http://192.168.0.127:1080\" Environment=\"NO_PROXY=localhost,127.0.0.1\"' | tee > /etc/systemd/system/containerd.service.d/http_proxy.conf systemctl daemon-reload systemctl restart containerd 六、git 代理 git config --global http.proxy socks5://192.168.0.127:1080 git config --global https.proxy socks5://192.168.0.127:1080 git config --global url.\"https://github.com/\".insteadOf https://github.com.cnpmjs.org git config --global url.\"https://github.com/\".insteadOf https://git.sdut.me/ https://ghproxy.com/ git clone https://ghproxy.com/https://github.com/stilleshan/ServerStatus wget https://ghproxy.com/https://github.com/stilleshan/ServerStatus/archive/master.zip curl -O https://ghproxy.com/https://github.com/stilleshan/ServerStatus/archive/master.zip 七、linux 代理 export http_proxy=socks5://192.168.0.127:1080 export https_proxy=socks5://192.168.0.127:1080 export ftp_proxy=http://192.168.0.127:1080 export no_proxy=localhost,127.0.0.1 # 强制终端中的 wget、curl 等都走 SOCKS5 代理 export ALL_PROXY=socks5://192.168.0.127:1080 八、apt 代理 vim /etc/apt/apt.conf Acquire::http::proxy \"http://192.168.0.127:1080\"; Acquire::https::proxy \"http://192.168.0.127:1080\"; echo 'Acquire::http::Proxy \"http://192.168.0.127:1080\";' > /etc/apt/apt.conf.d/proxy.conf echo 'Acquire::https::Proxy \"http://192.168.0.127:1080\";' >> /etc/apt/apt.conf.d/proxy.conf 九、yum 代理 vim /etc/yum.conf # proxy=http://192.168.0.127:1080 # proxy_username=username # proxy_password=password # proxy=http://username:password@proxy_ip:port/ proxy=http://192.168.0.127:1080/ 十、go 代理 # 启用 Go Modules 功能 go env -w GO111MODULE=on # 配置 GOPROXY 环境变量 # go env -w GOPROXY=https://goproxy.io,direct go env -w GOPROXY=https://goproxy.cn,direct export GOPROXY=\"https://athens.azurefd.net\" export GOPROXY=\"https://mirrors.tencent.com/go/\" export GOPROXY=\"https://mirrors.aliyun.com/goproxy/\" 十一、GitHub 提高访问 github 以下是 GitHub 完全同步仓库 https://github.com.cnpmjs.org https://hub.fastgit.org 十一二、systemd vim /etc/systemd/system.conf [Manager] DefaultEnvironment=\"http_proxy=http://proxy:port\" \"https_proxy=http://proxy:port\" "},"notes/pip.html":{"url":"notes/pip.html","title":"pip 源相关","keywords":"","body":"pip 源相关 一、清华源 tsinghua 1、临时使用 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package 3、更新pip pip install -U pip -i https://pypi.tuna.tsinghua.edu.cn/simple 4、设置默认 pip install pip -U -i https://pypi.tuna.tsinghua.edu.cn/simple pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 5、写入配置文件 mkdir -p ~/.pip/ tee > ~/.pip/pip.conf 二、阿里源 aliyun 1、临时使用 pip install -i https://mirrors.aliyun.com/pypi/simple some-package 3、更新pip pip install -U pip -i https://mirrors.aliyun.com/pypi/simple 4、设置默认 pip install pip -U -i https://mirrors.aliyun.com/pypi/simple pip config set global.index-url https://mirrors.aliyun.com/pypi/simple 5、写入配置文件 mkdir -p ~/.pip/ tee > ~/.pip/pip.conf 三、中科大 ustc pip install -i https://mirrors.ustc.edu.cn/pypi/web/simple package 四、其他 pip config get global.cache-dir pip config set global.cache-dir /hl/pip_cache pip install --download-cache https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl pip install --download-directory=\"$PIP_SDIST_INDEX\" "},"notes/linux_source.html":{"url":"notes/linux_source.html","title":"linux 源相关","keywords":"","body":"linux 源相关 一、alpine echo -e \"http://192.168.0.90:81/alpine/alpine/v3.13/main/\\nhttp://192.168.0.90:81/alpine/alpine/v3.13/community/\" > /etc/apk/repositories sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories sed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories 二、ubuntu sed -i \"s@http://.*archive.ubuntu.com@https://mirrors.tuna.tsinghua.edu.cn@g\" /etc/apt/sources.list sed -i \"s@http://.*security.ubuntu.com@https://mirrors.tuna.tsinghua.edu.cn@g\" /etc/apt/sources.list 1、x86 echo \"deb DIB_RESOURCE DIB_RELEASE main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE main restricted universe multiverse deb DIB_RESOURCE DIB_RELEASE-security main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE-security main restricted universe multiverse deb DIB_RESOURCE DIB_RELEASE-updates main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE-updates main restricted universe multiverse deb DIB_RESOURCE DIB_RELEASE-proposed main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE-proposed main restricted universe multiverse deb DIB_RESOURCE DIB_RELEASE-backports main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE-backports main restricted universe multiverse\" > /etc/apt/sources.list DIB_RELEASE=focal DIB_RESOURCE=http://mirrors.aliyun.com/ubuntu/ #DIB_RESOURCE=https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ #DIB_RESOURCE=http://mirrors.ustc.edu.cn/ubuntu/ sed -i \"s#DIB_RESOURCE#${DIB_RESOURCE}#g\" /etc/apt/sources.list sed -i \"s#DIB_RELEASE#${DIB_RELEASE}#g\" /etc/apt/sources.list echo \"deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse\" > /etc/apt/sources.list 2、arm64 echo \"deb DIB_RESOURCE DIB_RELEASE main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE main restricted universe multiverse deb DIB_RESOURCE DIB_RELEASE-security main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE-security main restricted universe multiverse deb DIB_RESOURCE DIB_RELEASE-updates main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE-updates main restricted universe multiverse deb DIB_RESOURCE DIB_RELEASE-proposed main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE-proposed main restricted universe multiverse deb DIB_RESOURCE DIB_RELEASE-backports main restricted universe multiverse deb-src DIB_RESOURCE DIB_RELEASE-backports main restricted universe multiverse\" > /etc/apt/sources.list DIB_RELEASE=focal DIB_RESOURCE=https://mirrors.aliyun.com/ubuntu-ports/ #DIB_RESOURCE=https://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ #DIB_RESOURCE=http://mirrors.ustc.edu.cn/ubuntu-ports/ sed -i \"s#DIB_RESOURCE#${DIB_RESOURCE}#g\" /etc/apt/sources.list sed -i \"s#DIB_RELEASE#${DIB_RELEASE}#g\" /etc/apt/sources.list "},"notes/git.html":{"url":"notes/git.html","title":"git 相关","keywords":"","body":"git 相关 一、git 1.Usual git config --global credential.helper store git fetch --all && git reset --hard origin/master # 获取分支名，以 '/' 拆分，取最后一个值 git rev-parse --abbrev-ref HEAD | awk -F'/' '{print $NF}' git format-patch -s -1 git reset --hard # 撤销所有已暂存和未暂存的更改（基于最新提交） git clean -fd # 删除所有未跟踪的文件和目录（-f 强制，-d 包括目录） 2.Other 1、Git 配置 /etc/gitconfig 所有用户，git config --system ~/.gitconfig 当前用户，git config --global .git/config #覆盖优先级 .git/config > ~/.gitconfig > /etc/gitconfig git config --local http.postBuffer 524288000 git config --global http.lowSpeedLimit 0 git config --global http.lowSpeedTime 999999 git config --global user.name \"hl\" git config --global core.editor vim git config --global merge.tool vimdiff git config --list #15 分钟 git config --global credential.helper cache #永久 git config --global credential.helper store #自定义时间 git config credential.helper 'cache --timeout=3600' #删除远程分支 git push origin --delete feature-x git push origin :feature-x git fetch --all --prune #撤销git add 添加的文件 git reset git checkout -- #撤销git add 添加的文件，同时丢弃add中的更改 git reset --hard git reset --hard HEAD^ #Git使用了一个内部数据库来存储对象（如提交、树和blob）。随着时间的推移，这个数据库可能会包含一些不再被任何分支或标签引用的对象。 git gc git gc --prune=now # 列出所有远程分支 git branch -r # 列出本地和远程分支 git branch -a # 查看某段代码是谁写的 git blame # 跳过 ssl 验证 git config --global http.sslVerify false # 更新 author git commit --amend --author=\"hl \" --no-edit 2、查看状态 git status -s git log --oneline git tags git branch 3、rebase git fetch --all git rebase origin/master git rebase -i HEAD~4 git rebase -i 113sff3e4f 4、cherry-pick git cherry-pick sdfs32233 git add . git commit --amend git cherry-pick --continue git push origin hl -f 5、amend git add . git commit --amend git rebase continue git push origin hl -f 6、checkout git checkout . git checkout -b dev origin/master 7、remote add git remote add test http://192.168.0.1/test/test.git git remote -vv 8、format am apply git format-patch -s -2 git am 0001-let-chinese-text-no-wrap.patch git format-patch 32399a0518ed22bc69ed413310eca4a5f50fa0d1^..8a03a7f472664641023fb08f5d55d16dfb192af9 # 检查是否有冲突 git apply --check 4.2.2.2_to_4.2.2.6/*.patch git format-patch -s -1 git apply --3way 0001-let-chinese-text-no-wrap.patch # 解决冲突 git add . git commit -am \"xxxx\" --3way 选项会尝试三向合并，若失败则在工作区文件中插入冲突标记。 git format-patch -s -1 git am 0001-let-chinese-text-no-wrap.patch git apply --3way 0001-let-chinese-text-no-wrap.patch # 解决冲突 git am --continue 9、reset git reset --hard 7f575c8 10、reset-author git commit --amend --reset-author 11、color git config --global color.status auto git config --global color.diff auto git config --global color.branch auto git config --global color.interactive auto 12、log git log --pretty=oneline 32399a0518ed22bc69ed413310eca4a5f50fa0d1..8a03a7f472664641023fb08f5d55d16dfb192af9 git log --pretty=format:\"%h; author: %cn; date: %ci; subject:%s\" tagA...tagB 比较本地的仓库和远程仓库的区别 git log -p master..origin/master git log -p 13、stash git stash git stash save \"test-cmd-stash\" git stash list git stash drop # 删除stash暂存区 git stash pop # 不删除stash暂存区，可多次应用 git stash apply # 清空栈中所有记录 git stash clear 13、submodule submodule git submodule add git@github.com:jjz/pod-library.git pod-library git add .gitmodules pod-ibrary git commit -m \"pod-library submodule\" git submodule init git push 14、merge 合并dev分支到master分支 git checkout dev git pull git checkout master git merge dev git push -u origin master 把远程下载下来的代码合并到本地仓库，远程的和本地的合并 git merge origin/master 15、branch 删除temp git branch -d temp 查看分支 git branch -vv git branch --all 16、diff 比较master分支和temp分支的不同 git diff temp 17、fetch 从远程的origin仓库的master分支下载代码到本地的origin master git fetch origin master 从远程的origin仓库的master分支下载到本地并新建一个分支temp git fetch origin master:temp 18、add 交互地添加文件至缓存区 git add -i 19、config 彩色的 git 输出： git config color.ui true 显示历史记录时，只显示一行注释信息： git config format.pretty oneline 20、archive # --format 表示打包的格式，如 zip，-v 表示对应的 tag 名，后面跟的是 tag 名，如 v0.1 git archive -v -format=zip v0.1>v0.1.zip git archive --format=tar --output /full/path/to/zipfile.zip master | gzip 21、subtree git subtree pull --prefix= #将 B 仓库添加为 A 仓库的一个子目录 git subtree add --prefix=SubFolder/B https://github.com/walterlv/walterlv.git master #将 A 仓库中的 B 子目录推送回 B 仓库 git subtree push --prefix=SubFolder/B https://github.com/walterlv/walterlv.git master #将 B 仓库中的新内容拉回 A 仓库的子目录 git subtree pull --prefix=SubFolder/B walterlv master # 将需要分离的目录的提交日志分离成一个独立的临时版本 git subtree split -P -b # example - name: Publish uses: hlyani/gitbook-deploy-action@master env: ACCESS_TOKEN: ${{ secrets.testaction }} BASE_BRANCH: source BRANCH: master FOLDER: docs git add -f $FOLDER git commit -m \"Deploying to ${BRANCH} from ${BASE_BRANCH:-master} ${GITHUB_SHA}\" --quiet git push $REPOSITORY_PATH `git subtree split --prefix $FOLDER ${BASE_BRANCH:-master}`:$BRANCH --force 22、使用用户名和密码 git clone http://邮箱(或用户名):密码@仓库 git clone -b master --depth 1 http://admin:123@192.168.0.1/demo/demo.git 23、tag git tag -a \"v1.0\" -m \"v1.0\" 推送所有tag git push origin --tags 删除本地记录 git tag -d v1.0 删除远程记录 git push origin :refs/tags/v1.0 24、拉取单个分支到指定目录 git clone --single-branch --branch=v1.0.0 --depth=1 http://192.168.0.1/test/tmp test 25、强制推送所有分支 需要推送的分支需要每个都 git checkout xx git push --all origin -f 26、只拉取单个分支 git clone --single-branch --branch=v1.0.0 --depth=1 http://XXX build/src/XXX 27、重置分支 git reset --hard origin/master 28、统计分支直接的差异 git diff --numstat main dev | awk '{add+=$1; del+=$2} END {print \"Added:\", add, \"Deleted:\", del, \"Total:\", add+del}' git ls-files | xargs cat | wc -l 29、忽略空格变化 # 使用 --ignore-space-change 选项忽略空格变化 git apply --ignore-space-change ..\\0001-feat.patch # 使用 --ignore-whitespace 选项忽略所有空白字符差异 git apply --ignore-whitespace ..\\0001-feat.patch # 使用 --whitespace=nowarn 选项忽略空白字符警告 git apply --whitespace=nowarn ..\\0001-feat.patch 二、常用 （一）、修改之前的commit 1、将HEAD移动到需要修改的commit上 git rebase eb69dddd^ --interactive 2、找到需要修改的commit,将首行的pick改成edit后保存 3、开始修改内容 4、添加改动文件到暂存 git add 5、追加改动到提交 git commit --amend 6、移动HEAD回到最新的commit git rebase --continue 7、强制提交 git push origin master -f （二）、合并 commit 1、找到要合并 commit 的前一个commit git rebase -i sdfs1easdasd # git rebase -i HEAD~3 2、将要合并的 commit 前改为 squash pick asdasd Add second commit squash asdasd Add third commit :wq 3、操作失误 --abort 撤销 git rebase --abort 4、提交 git add . git push origin master -f （三）、lfs 1、安装 yum install git-lfs apt install git-lfs git lfs install 2、标记大文件 git lfs track kernel_resource/4.19.37-rt19.arm64.gz git lfs track *.gz 3、推送 git add my.gz git commit -m \"add gz file\" git push origin master 4、查看 git lfs track git lfs ls-files vim .gitattributes （四）、gitlab-ci 1、拉取gitlab容器镜像 docker pull gitlab/gitlab-runner:latest 2、运行gitlab docker run -d --name gitlab-runner --restart always \\ -v /srv/gitlab-runner/config:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:latest mkdir -p /home/hl/gitlabrunner/DeltaOS/opt /home/hl/gitlabrunner/DeltaOS/builds docker run -d --name deltaos-gitlab-runner --restart always \\ --privileged=true \\ -v /home/hl/gitlabrunner/DeltaOS/opt:/opt \\ -v /home/hl/gitlabrunner/DeltaOS/builds:/home/gitlab-runner/builds \\ gitlab/gitlab-runner:latest docker exec deltaos-gitlab-runner gitlab-runner register \\ --non-interactive \\ --name my-runner \\ --url http://192.168.1.1:8000/ \\ --registration-token sDrr7KDi6UXyzwuzPqx- \\ --executor shell \\ --tag-list \\ common-runner 3、容器内 mknod -m 0660 /dev/loop1 b 7 1 echo \"192.168.21.8 gitlab.tmp.com\" |tee >> /etc/hosts #添加root权限 sed -i 's/gitlab-runner:x:999:999/gitlab-runner:x:0:0/g' /etc/passwd 4、编辑gitlab-ci vim .gitlab-ci.yml stages: - deploy deploy: stage: deploy script: - bash /opt/deploy.sh only: - master tags: - common-runner when 可以设置为以下值之一： on_success - 只有当前一个阶段的所有工作成功时才​​执行工作。这是默认值。 on_failure - 仅当前一个阶段的至少一个作业发生故障时才执行作业。 always - 无论前一阶段的工作状况如何，执行工作。 manual - 手动执行作业 stages: - build - test - deploy 首先，所有build的jobs都是并行执行的。 所有build的jobs执行成功后，test的jobs才会开始并行执行。 所有test的jobs执行成功，deploy的jobs才会开始并行执行。 所有的deploy的jobs执行成功，commit才会标记为success 任何一个前置的jobs失败了，commit会标记为failed并且下一个stages的jobs都不会执行。 有时，script命令需要用单引号或双引号括起来。例如，包含冒号（:）的命令需要用引号括起来，以便YAML解析器知道将整个事物解释为字符串而不是“key：value”对。使用特殊字符时要小心：:，{，}，[，]，,，&，*，#，?，|，-，，=，!，%，@，`。 例如： stages: - build - cleanup_build - test - deploy - cleanup build_job: stage: build script: - make build cleanup_build_job: stage: cleanup_build script: - cleanup build when failed when: on_failure test_job: stage: test script: - make test deploy_job: stage: deploy script: - make deploy when: manual cleanup_job: stage: cleanup script: - cleanup after jobs when: always 以上脚本将： cleanup_build_job仅在build_job失败时执行。 始终执行cleanup_job作为流水线的最后一步，无论成功或失败。 允许您deploy_job从GitLab的UI 手动执行。 stages: - build - upload - release variables: PACKAGE_VERSION: ${CI_COMMIT_BRANCH}-${CI_COMMIT_SHORT_SHA} REGISTRY: \"192.168.0.1\" DIR: \"dist\" AMD64_TAR: \"aa-amd64.tar.gz\" ARM64_TAR: \"aa-arm64.tar.gz\" PACKAGE_REGISTRY_URL: \"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/Release/${PACKAGE_VERSION}\" build-job: stage: build image: ${REGISTRY}/cicd/build-base:v1.0.1 rules: - if: $CI_COMMIT_TAG # Do not run this job when a tag is created manually when: never - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH # Run this job when commits are pushed or merged to the default branch script: - echo \"running build-job for ${PACKAGE_VERSION}\" - make # - yes|docker system prune -a artifacts: untracked: false when: on_success expire_in: 30 days paths: - ${DIR}/${AMD64_TAR} - ${DIR}/${ARM64_TAR} upload-job: stage: upload image: ${REGISTRY}/cicd/curl:latest rules: - if: $CI_COMMIT_TAG # Do not run this job when a tag is created manually when: never - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH # Run this job when commits are pushed or merged to the default branch needs: - job: build-job artifacts: true script: - echo \"running upload-job for ${PACKAGE_VERSION}\" - | curl --header \"JOB-TOKEN: ${CI_JOB_TOKEN}\" --upload-file ${DIR}/${AMD64_TAR} \"${PACKAGE_REGISTRY_URL}/${AMD64_TAR}\" curl --header \"JOB-TOKEN: ${CI_JOB_TOKEN}\" --upload-file ${DIR}/${ARM64_TAR} \"${PACKAGE_REGISTRY_URL}/${ARM64_TAR}\" - echo ${PACKAGE_REGISTRY_URL}/${AMD64_TAR} - echo ${PACKAGE_REGISTRY_URL}/${ARM64_TAR} release-job: stage: release image: ${REGISTRY}/cicd/release-cli:latest rules: - if: $CI_COMMIT_TAG # Do not run this job when a tag is created manually when: never - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH # Run this job when commits are pushed or merged to the default branch needs: - job: upload-job script: - echo \"running release-job for ${PACKAGE_VERSION}\" release: name: ${PACKAGE_VERSION} tag_name: ${PACKAGE_VERSION} description: ${PACKAGE_VERSION} assets: links: - name: ${AMD64_TAR} url: \"${PACKAGE_REGISTRY_URL}/${AMD64_TAR}\" filepath: \"/${AMD64_TAR}\" link_type: package - name: ${ARM64_TAR} url: \"${PACKAGE_REGISTRY_URL}/${ARM64_TAR}\" filepath: \"/${ARM64_TAR}\" link_type: package 5、界面查找url和token Settings —> Pipelines —> Specific Runners 6、注册 gitlab-runner register --non-interactive --name my-runner --url http://192.168.21.8:8000/ --registration-token Kgu3z28pqfZEZsBmquUh --executor shell --tag-list common-runner vim /etc/passwd gitlab-runner:x:0:0:GitLab Runner:/home/gitlab-runner:/bin/bash #手动注册 docker exec -it gitlab-runner gitlab-ci-multi-runner register #引导会让你输入gitlab的url，输入自己的url，例如http://gitlab.example.com/ #引导会让你输入token，去相应的项目下找到token，例如ase12c235qazd32 #引导会让你输入tag，一个项目可能有多个runner，是根据tag来区别runner的，输入若干个就好了，比如web,hook,deploy #引导会让你输入executor，这个是要用什么方式来执行脚本，图方便输入shell就好了。 7、查看gitlab-runner gitlab-runner list 8、宿主机安装 yum 安装 curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.rpm.sh | bash yum install -y gitlab-ci-multi-runner echo '[gitlab-ci-multi-runner] name=gitlab-ci-multi-runner baseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ci-multi-runner/yum/el7 repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key' |tee >> /etc/yum.repos.d/gitlab-ci-multi-runner.repo yum makecache yum install -y gitlab-ci-multi-runner #设置gitlab-runner root权限 gitlab-runner:x:0:0:GitLab Runner:/home/gitlab-runner:/bin/bash 9、其他 jenkins http://blog.csdn.net/abcdocker/article/details/53840629 https://docs.gitlab.com/ee/ci/yaml/README.html （五）、规范 type：commit 的类型 feat：新特性 fix：修改问题 refactor：代码重构 docs：文档修改 style：代码格式修改，注意不是 css 修改 test：测试用例修改 chore：其他修改，比如构建流程，依赖管理 scope：commit 影响的范围，比如：route，component，utils，build…… subject：commit 的概述 body：commit 具体修改内容，可以分为多行 footer：一些备注，通常是 BREAKING CHANGE 或修复的 bug 的链接 三、Gitlab备份和恢复 （一）备份 1、修改配置 gitlab_rails['manage_backup_path'] = true gitlab_rails['backup_path'] = \"/var/opt/gitlab/backups\" gitlab_rails['backup_keep_time'] = 604800 #这个是秒，7天的时间 2、重新加载配置，让配置生效 gitlab-cli reconfigure gitlab-cli restart 3、备份命令 /usr/bin/gitlab-rake gitlab:backup:create 1674018900_2023_01_18_13.5.4_gitlab_backup.tar 可通过date命令查看 date -d @1674018900 4、其他需要备份文件 /etc/gitlab/gitlab.rb 配置文件须备份 /var/opt/gitlab/nginx/conf nginx配置文件 /etc/postfix/main.cfpostfix 邮件配置备份 5、定时任务自动备份 每周备份一次 0 1 * * 1 docker exec gitlab sh -c \"/usr/bin/gitlab-rake gitlab:backup:create\" （二）恢复 1、停止数据写入任务 gitlab-ctl stop unicorn gitlab-ctl stop sidekiq 2、恢复数据 gitlab-rake gitlab:backup:restore BACKUP=1674018900 3、重启服务 gitlab-ctl restart （三）只备份代码 cd /var/opt/gitlab/git-data/repositories /usr/bin/gitlab-rake gitlab:backup:create 四、autobak cat > /hl/auto_bak.sh /dev/null; then log \"Git is not installed. Please install Git and retry.\" exit 1 fi # Check for uncommitted changes if [[ -n $(git status --porcelain) ]]; then log \"Changes detected. Preparing to commit and push.\" # Add all changes git add . # Commit changes and push git commit -am \"Auto backup $(date -u +\"%Y%m%d%H%M\")\" if git push; then log \"Changes have been successfully pushed to the remote repository.\" else log \"Push failed. Please check your network connection or remote repository settings.\" exit 1 fi else log \"No changes detected. Nothing to commit.\" fi EOF chmod +x /hl/auto_bak.sh cluster_branch=hl git init git remote add origin https://root:123@gitlab.xxx.com/hl git checkout -b ${cluster_branch} git push --set-upstream origin ${cluster_branch} crontab -e crontab -l crontab -d 五、FAQ error: RPC failed; curl 56 GnuTLS recv error (-9): Error decoding the received TLS packet. error: 4254 bytes of body are still expected fetch-pack: unexpected disconnect while reading sideband packet fatal: early EOF fatal: fetch-pack: invalid index-pack output git config --global http.postBuffer 1048576000 git config --global https.postBuffer 1048576000 "},"notes/skills.html":{"url":"notes/skills.html","title":"skills","keywords":"","body":"Skills 一、清理硬盘 Windows的Format命令已经支持安全擦除整个分区。Format命令加上/P参数后，就会把每个扇区先清零，再用随机数覆盖。而且可以覆盖多次。比如“format D: /P:8”就表示把D盘用随机数覆盖8次。 format D: /P:8 Cipher命令本来是用于磁盘加密的，但有个/W参数可对磁盘上未使用的空间进行覆盖。比如你刚刚删除了D:\\Private目录下的几个文件，那么执行一下“cipher /w:D:\\Private”。D盘上未使用空间就会被覆盖三次：一次0x00、一次0xFF，一次随机数。所有被删除的文件就都不可能被恢复了。 cipher.exe /w:D:\\ 二、跳过Microsoft登录 跳过登录Microsoft账户可能会影响某些微软服务的使用，如OneDrive和商店等。因此，是否跳过登录取决于个人偏好和需求。 1.在安装Windows 11时，不要连接到Wi-Fi网络，使用有线连接。完成系统安装后，会出现需要登录Microsoft账户的界面。此时，拔掉网线，断开网络连接，然后点击左上角的“返回箭头”即可跳过登录。 2.如果已经安装了Windows 11，可以通过更改账户设置来跳过登录。进入“设置”>“账户”>“登录选项”，将登录方式从“密码”更改为“PIN码”，并设置一个简单的PIN码，这样就可以实现自动登录。 3.在开机时按下Shift+F10键，打开命令提示符，然后输入特定的命令，如“oobe\\bypassnro.cmd”或“taskmgr”，来绕过登录界面。 4.在联网界面选择“我没有Internet连接”，然后继续执行，在许可协议界面接受后，可以创建本地账户。 三、word 快捷键 Ctrl+Shift+n 清除格式 Ctrl+e 居中 Ctrl+l Ctrl+r Ctrl+a F9 更新所有域 Ctrl+1 单倍行距 Ctrl+Shift+Home 文档第一页 F12 另存为 四、iOS安装历史版本应用 https://www.52pojie.cn/thread-1756628-1-1.html iTunes 12.6.5 iOS任意版本号APP下载v6.0 搜不到APP历史版本号？ 先不要拦截，在iTunes商店中下载此软件，等待下载完成。 在本工具中【安装管理】下找到对应IPA安装包，右键选择【查找版本ID】。 即可列出软件所有历史版本ID，版本号按新版到旧版排序。 PS：暂时没有通过版本ID，查版本号的接口，所以抓下来，看吧。 五、win10 无法访问samba win10 你不能访问此共享文件夹，因为你组织的安全策略... 此问题需要修改Win10网络策略 按window+R键输入gpedit.msc 来启动本地组策略编辑器。 依次找到“计算机配置-管理模板-网络-Lanman工作站”这个节点，在右侧内容区可以看到“启用不安全的来宾登录”这一条策略设置。状态是“未配置”。 六、一键自动化 下载、安装、激活 Office 的利器。 office激活 七、diamond 预算1-3万：选50分，颜色G-H，净度VS，3EX切工，无荧光。 "},"notes/docker/usual_software.html":{"url":"notes/docker/usual_software.html","title":"常用软件安装","keywords":"","body":"常用软件安装 一、docker 构建 FROM ubuntu:22.04 ARG http_proxy=http://http://127.0.0.1:1080 https_proxy=$http_proxy RUN apt update && \\ apt install -y supervisor net-tools iputils-ping vim iproute2 openssh-server openssh-client && \\ apt clean && \\ sed -i \"s/#PermitRootLogin prohibit-password/PermitRootLogin yes/g\" /etc/ssh/sshd_config && \\ sed -i \"s/#PasswordAuthentication yes/PasswordAuthentication yes/g\" /etc/ssh/sshd_config && \\ echo \"root:123\" | chpasswd && \\ touch /entrypoint.sh && \\ chmod +x /entrypoint.sh && \\ echo \"service ssh start\" >> /entrypoint.sh && \\ echo \"sleep infinity\" >> /entrypoint.sh CMD [\"/usr/bin/sh\", \"-c\", \"/entrypoint.sh\"] RUN apt update && apt install -y perftest iperf3 infiniband-diags net-tools iputils-ping wrk vim iproute2 && apt clean 二、安装 1、rocket https://hub.docker.com/_/rocket-chat https://rocket.chat/install docker run -d --name rocketchat-mongo mongo:4.0.10 --smallfiles --oplogSize 128 --replSet rs1 --storageEngine=mmapv1 docker exec -d rocketchat-mongo bash -c 'echo -e \"replication:\\n replSetName: \\\"rs01\\\"\" | tee -a /etc/mongod.conf && mongo --eval \"printjson(rs.initiate())\"' docker run -d --name rocketchat --link rocketchat-mongo -e \"MONGO_URL=mongodb://rocketchat-mongo:27017/rocketchat\" -e MONGO_OPLOG_URL=mongodb://rocketchat-mongo:27017/local?replSet=rs01 -e ROOT_URL=http://192.168.21.87:3001 -p 3001:3000 rocketchat/rocket.chat:1.2.1 2、samba https://github.com/dperson/samba -s \"[;browse;readonly;guest;users;admins;writelist;comment]\" Configure a share required arg: \";\" is how it's called for clients path to share NOTE: for the default values, just leave blank [browsable] default:'yes' or 'no' [readonly] default:'yes' or 'no' [guest] allowed default:'yes' or 'no' NOTE: for user lists below, usernames are separated by ',' [users] allowed default:'all' or list of allowed users [admins] allowed default:'none' or list of admin users [writelist] list of users that can write to a RO share [comment] description of share docker run -it --name samba -p 139:139 -p 445:445 \\ --restart always \\ -e TZ=EST5EDT \\ -v /fs/samba:/mount \\ -d dperson/samba -p \\ -s \"samba;/mount/;yes;no;yes;all;all;all;all\" mkdir /opt/test chmod 777 -R /opt/test docker run -it -p 139:139 -p 445:445 --name samba -v /opt/test:/mount -d dperson/samba \\ -u \"test;qwe\" \\ -s \"test;/mount/;yes;no;yes;all;all;all\" \\ -w \"WORKGROUP\" \\ -g \"force user= test\" \\ -g \"guest account= test\" # 查看正在运行的配置参数 testparm -v 3、gitlab https://docs.gitlab.com/omnibus/docker/ docker run --detach \\ --hostname gitlab.example.com \\ --publish 8443:443 --publish 8080:80 --publish 8022:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest 默认帐户的用户名是root，第一次访问时，将被重定向到密码重置屏幕,登录后，您可以更改用户名。 gitlab-ctl reconfigure gitlab-ctl restart gitlab-ctl status gitlab-ctl stop gitlab-ctl tail gitlab-ctl stop unicorn gitlab-ctl stop sideki FAQ fail to initialize orm engine: Sqlstore::Migration failed err: unable to open database file chmod 777 -R ./grafana/data 2020-10-15_07:07:14.07056 time=\"2020-10-15T07:07:14Z\" level=fatal msg=\"find gitaly\" error=\"open /var/opt/gitlab/gitaly/gitaly.pid: permission denied\" wrapper=3997 chown 998 gitaly.pid chgrp 988 gitaly.pid 4、wiki https://www.dokuwiki.org/dokuwiki https://github.com/bitnami/bitnami-docker-dokuwiki docker run -d -p 9080:8080 -p 9443:8443 --restart=always --name dokuwiki \\ -e DOKUWIKI_USERNAME=admin \\ -e DOKUWIKI_PASSWORD=qwe \\ -e ALLOW_EMPTY_PASSWORD=yes \\ -v /fs/wiki/data:/bitnami/dokuwiki \\ bitnami/dokuwiki:latest 可用参数 DOKUWIKI_USERNAME: Dokuwiki application username. Default: user DOKUWIKI_FULL_NAME: Dokuwiki application user full name. Default: Full Name DOKUWIKI_PASSWORD: Dokuwiki application password. Default: bitnami1 DOKUWIKI_EMAIL: Dokuwiki application email. Default: user@example.com DOKUWIKI_WIKI_NAME: Dokuwiki wiki name. Default: Bitnami DokuWiki 5、redmine https://hub.docker.com/redmine use SQLite3 docker run -d --name some-redmine redmine use PostgreSQL docker run -d --name some-postgres --network some-network -e POSTGRES_PASSWORD=secret -e POSTGRES_USER=redmine postgres docker run -d --name some-redmine --network some-network -e REDMINE_DB_POSTGRES=some-postgres -e REDMINE_DB_USERNAME=redmine -e REDMINE_DB_PASSWORD=secret redmine use MySQL docker run -d --name some-mysql --network some-network -e MYSQL_USER=redmine -e MYSQL_PASSWORD=secret -e MYSQL_DATABASE=redmine -e MYSQL_RANDOM_ROOT_PASSWORD=1 mysql:5.7 docker run -d --name some-redmine --network some-network -e REDMINE_DB_POSTGRES=some-postgres -e REDMINE_DB_USERNAME=redmine -e REDMINE_DB_PASSWORD=secret redmine 6、vsftpd https://github.com/panubo/docker-vsftpd 生成秘钥 openssl req -x509 -nodes -days 3650 -newkey rsa:1024 -keyout /opt/pub/vsftpd.pem -out /opt/pub/vsftpd.pem 运行容器 docker run -d \\ -p 21:21 -p 4559-4564:4559-4564 \\ -e FTP_USER=root -e FTP_PASSWORD=qwe \\ -v /home/vsftpd:/home/vsftpd \\ -v /var/log/ftp:/var/log \\ -v /opt/pub/vsftpd.pem:/etc/ssl/certs/vsftpd.crt:ro \\ -v /opt/pub/vsftpd.pem:/etc/ssl/private/vsftpd.key:ro \\ -v /home/vsftpd:/srv \\ --restart=always \\ docker.io/panubo/vsftpd vsftpd /etc/vsftpd_ssl.conf 7、jenkins https://www.jenkins.io/doc/book/installing/docker/ docker network create jenkins docker run -d \\ --restart always \\ --network host \\ --network-alias docker \\ --name jenkins -u root \\ -p 8080:8080 \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /opt/jenkins:/var/jenkins_home \\ jenkinsci/blueocean:1.25.2 cat /jenkins/secrets/initialAdminPassword 1.webhook 需要安装插件 GitLab Multibranch Scan Webhook Trigger Docker git配置 http://192.168.0.190:8080/multibranch-webhook-trigger/invoke?token=mytoken Url is blocked: Requests to the local network are not allowed Admin Area > Settings > Network > Outbound requests Allow requests to the local network from web hooks and services Allow requests to the local network from system hooks jenkins配置 Scan by webhook Trigger token mytoken 2.新加节点 安装 https://gitee.com/hlyani/apps/blob/master/jenkins_agent/Dockerfile mkdir /opt/Jenkins apt install openjdk-11-jdk apk add docker openssh openjdk11 git 8、harbor https://github.com/goharbor/harbor http://hlyani.gitee.io/hlyani.github.io/notes/docker/harbor.html ./prepare ./install.sh --with-chartmuseum docker-compose down -v docker-compose up -d docker-compose stop -v 9、nextcloud docker run -d \\ --restart always \\ --name nextcloud \\ -p 8000:80 \\ -v /data/nextcloud:/var/www/html \\ nextcloud 10、svn docker run --restart always --name svn -d -v /root/dockers/svn:/var/opt/svn -p 3690:3690 garethflowers/svn-server docker exec -it svn /bin/sh svnadmin create svn vim svnserve.conf anon-access = none # 匿名用户不可读写，也可设置为只读 read auth-access = write # 授权用户可写 password-db = passwd # 密码文件路径，相对于当前目录 authz-db = authz # 访问控制文件 realm = /var/opt/svn/svn # 认证命名空间，会在认证提示界面显示，并作为凭证缓存的关键字，可以写仓库名称比如svn vim passwd [users] # harry = harryssecret # sally = sallyssecret admin = 123456 vim authz [groups] owner = admin [/] # / 表示所有仓库 admin = rw # 用户 admin 在所有仓库拥有读写权限 [svn:/] # 表示以下用户在仓库 svn 的所有目录有相应权限 @owner = rw # 表示 owner 组下的用户拥有读写权限 svn co svn://127.0.0.1:3690/svn 11、mariadb docker pull mariadb:10.4 docker run -d -name mariadb -e MYSQL_ROOT_PASSWORD=qwe -p 3306:3306 mariadb:10.4 12、redis docker run -d --name redis -p 6379:6379 redis 13、squid https://hub.docker.com/r/ubuntu/squid docker run -d --name squid-container -e TZ=UTC -p 3128:3128 ubuntu/squid:5.2-22.04_beta 14、clash # clash-linux-amd64-latest # https://github.com/szkzn/Clash_Core_Latest_Bak_2023-09-05 wget -O config.yaml \"http://XXX/api/v1/client/subscribe?token=XXX&flag=clash\" wget https://cdn.jsdelivr.net/gh/Dreamacro/maxmind-geoip@release/Country.mmdb FROM alpine COPY clash cache.db config.yaml Country.mmdb /opt/ CMD [\"/opt/clash\", \"-d\", \"/opt\"] docker run -d --net=host --restart=always --name clash clash:1.0.0 15、easyconnect docker run -d \\ --device /dev/net/tun \\ --cap-add NET_ADMIN \\ --name easyconnect \\ --restart=always \\ -p 127.0.0.1:1080:1080 \\ -p 127.0.0.1:8888:8888 \\ -e EC_VER=7.6.3 \\ -e CLI_OPTS=\"-d https://aa.com -u hl -p 123\" \\ hagb/docker-easyconnect:cli "},"notes/docker/kubeflow.html":{"url":"notes/docker/kubeflow.html","title":"kubeflow","keywords":"","body":"kubeflow 一、安装 git clone https://github.com/kubeflow/training-operator.git cd training-operator git checkout v1.8.1 kubectl apply -k manifests/overlays/standalone/ kubectl get pods -n kubeflow kubectl get crd kubectl edit -n kubeflow deployments training-operator kubectl patch -n kubeflow deployments training-operator --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/command/1\", \"value\": \"--gang-scheduler-name=volcano\"}]' kubectl patch -n kubeflow deployments training-operator --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/command/2\", \"value\": \"--pytorch-init-container-image=easzlab.io.local:5000/alpine:3.10\"}]' 二、使用 1、tfjob MNIST 示例 https://www.kubeflow.org/docs/components/training/user-guides/tensorflow/ kubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/master/examples/tensorflow/simple.yaml https://github.com/kubeflow/training-operator/blob/master/examples/tensorflow/mnist_with_summaries/mnist_with_summaries.py tf-mnist.yaml apiVersion: \"kubeflow.org/v1\" kind: TFJob metadata: name: tfjob-simple namespace: kubeflow spec: tfReplicaSpecs: Worker: replicas: 2 restartPolicy: OnFailure template: spec: containers: - name: tensorflow image: kubeflow/tf-mnist-with-summaries:latest imagePullPolicy: IfNotPresent command: - \"python\" - \"/var/tf_mnist/mnist_with_summaries.py\" kubectl apply -f tf-mnist.yaml kubectl -n kubeflow logs tfjob-simple-worker-0 kubectl get tfjob -n kubeflow kubectl delete -f tf-mnist.yaml 使用 gpu tf-gpu.yaml apiVersion: \"kubeflow.org/v1\" kind: \"TFJob\" metadata: name: \"tf-smoke-gpu\" spec: tfReplicaSpecs: PS: replicas: 1 template: metadata: creationTimestamp: null spec: containers: - args: - python - tf_cnn_benchmarks.py - --batch_size=32 - --model=resnet50 - --variable_update=parameter_server - --flush_stdout=true - --num_gpus=1 - --local_parameter_device=cpu - --device=cpu - --data_format=NHWC image: docker.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3 name: tensorflow ports: - containerPort: 2222 name: tfjob-port resources: limits: cpu: \"1\" workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks restartPolicy: OnFailure Worker: replicas: 1 template: metadata: creationTimestamp: null spec: containers: - args: - python - tf_cnn_benchmarks.py - --batch_size=32 - --model=resnet50 - --variable_update=parameter_server - --flush_stdout=true - --num_gpus=1 - --local_parameter_device=cpu - --device=gpu - --data_format=NHWC image: docker.io/kubeflow/tf-benchmarks-gpu:v20171202-bdab599-dirty-284af3 name: tensorflow ports: - containerPort: 2222 name: tfjob-port resources: limits: nvidia.com/gpu: 1 # GPU数量 workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks restartPolicy: OnFailure kubectl apply -f tf-gpu.yaml kubectl logs tf-smoke-gpu-worker-0 "},"notes/docker/kube-scheduler.html":{"url":"notes/docker/kube-scheduler.html","title":"kube-scheduler","keywords":"","body":"kube-scheduler https://kubernetes.io/docs/reference/scheduling/config/ https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/ https://tanjunchen.github.io/post/2024-04-08-scheduler-framework-03/ 插件 ImageLocality: score TaintToleration: filter, preScore, score NodeName: filter NodePorts: preFilter, filter NodeAffinity: filter, score PodTopologySpread: preFilter, filter, preScore, score NodeUnschedulable: filter NodeResourcesFit：LeastAllocated、MostAllocated、RequestedToCapacityRatio : preFilter, filter, score NodeResourcesBalancedAllocation: score VolumeBinding: preFilter, filter, reserve, preBind, score VolumeRestrictions: filter VolumeZone: filter NodeVolumeLimits: filter EBSLimits: filter GCEPDLimits: filter AzureDiskLimits: filter InterPodAffinity: preFilter, filter, preScore, score PrioritySort: queueSort DefaultBinder: bind DefaultPreemption: postFilter CinderLimits: filter 扩展点 queueSort preFilter filter postFilter preScore score reserve permit preBind bind postBind multiPoint 一、调度插件 1.DefaultPreemption 默认抢占机制，当调度器发现当前急群中的节点无法满足新Pod的资源需求时，可能会通过抢占低优先级的Pod，腾出足够的资源来满足高优先级Pod的需求。 spec.PreemptionPolicy PriorityClass 默认为 PreemptLowerPriority，即该 Pod 可以抢占低优先级的 Pod 2.InterPodAffinity 允许用户指定两个或多个 Pod 之间的亲和性要求，确保它们在特定条件下可以调度到相同或相关的节点上。 Affinity AntiAffinity requiredDuringSchedulingIgnoredDuringExecution：这是一个强制性的亲和性规则，表示调度时必须满足这些条件。 preferredDuringSchedulingIgnoredDuringExecution：表示这是一个软要求，调度器会尽量满足，但如果无法满足，也不会阻止调度。 3.NodeAffinity 允许用户基于节点的标签来控制 Pod 的调度位置。 nodeSelector 4.NodeResourcesBalancedAllocation 在多个节点中选择资源分配更加均衡的节点来调度 Pod，通过评估节点的 CPU 和内存利用率，确保 Pod 不会集中在资源过度的使用的节点上，也不会导致某些节点的资源闲置不被利用。主要目的是在节点的 CPU 和内存使用之前取得平衡。 1.CPU 和内存使用的均衡性：根据资源利用率，优先选择资源更为均衡的节点。 2.打分机制：节点的打分介于0到100分之间，得分越高表示资源分配越均衡。 5.NodeResourcesFit 根据接的可用资源（如 CPU 和内存）判断 Pod 是否可以调度该节点。确保每个 Pod 都能分配到满足其资源请求的节点。主要依据 requests 和节点的资源可用性来进行调度决策。 6.PodTopologySpread 确保 Pods 在指定的拓扑域（例如可用区、区域或其他标签）之间均匀分布。该功能对与增强应用程序的弹性和可用性非常有用，因为可以防止 Pods 集中在单个故障域中。 1.拓扑域：pods 应该分布的区域。包括不同的节点、可用区或区域。 2.分布约束：可以定义约束，指定在每个拓扑域中可以存在多少 Pods。例如 ，可能希望在每个可用区中至少有一个 Pod。 3.权重：可以为不同的拓扑域分配权重，从而允许优先考虑在某些域之间的分布。 示例 apiVersion: v1 kind: ServiceAccount metadata: name: multipoint-scheduler-sa namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: multipoint-scheduler-cr rules: - apiGroups: [\"\"] resources: - pods - pods/logs - pods/status - pods/binding - bindings - nodes - events - services - namespaces - configmaps - secrets - serviceaccounts - resourcequotas - replicationcontrollers - persistentvolumes - persistentvolumeclaims verbs: - get - list - watch - create - update - patch - delete - apiGroups: [\"apps\"] resources: - replicasets - statefulsets - deployments - daemonsets verbs: - get - list - watch - create - update - patch - delete - apiGroups: [\"storage.k8s.io\"] resources: - storageclasses - volumeattachments - csinodes - csidrivers - csistoragecapacities verbs: - get - list - watch - apiGroups: [\"policy\"] resources: - poddisruptionbudgets verbs: - get - list - watch - apiGroups: [\"k8s.io\", \"events.k8s.io\"] resources: - priorityclasses - events verbs: - get - list - watch - create - update - patch - delete - apiGroups: [\"node\"] resources: - runtimeclasses verbs: - get - list - watch - apiGroups: [\"coordination.k8s.io\"] resources: - leases verbs: - get - list - watch - create - update - patch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: multipoint-scheduler-crb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: multipoint-scheduler-cr subjects: - kind: ServiceAccount name: multipoint-scheduler-sa namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: multipoint-scheduler-config namespace: kube-system data: scheduler-config.yaml: | apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: d-scheduler - pluginConfig: - args: apiVersion: kubescheduler.config.k8s.io/v1 hardPodAffinityWeight: 1 kind: InterPodAffinityArgs name: InterPodAffinity - args: apiVersion: kubescheduler.config.k8s.io/v1 kind: NodeAffinityArgs name: NodeAffinity - args: apiVersion: kubescheduler.config.k8s.io/v1 kind: NodeResourcesFitArgs scoringStrategy: resources: - name: cpu weight: 1 - name: memory weight: 1 type: MostAllocated name: NodeResourcesFit plugins: multiPoint: enabled: - name: TaintToleration weight: 2 - name: NodeAffinity weight: 2 - name: NodeResourcesFit weight: 2 - name: InterPodAffinity weight: 2 - name: ImageLocality weight: 25 disabled: - name: NodeResourcesBalancedAllocation - name: PodTopologySpread schedulerName: image-locality-scheduler --- apiVersion: apps/v1 kind: Deployment metadata: name: multipoint-scheduler namespace: kube-system labels: component: multipoint-scheduler spec: replicas: 2 selector: matchLabels: component: multipoint-scheduler template: metadata: labels: component: multipoint-scheduler name: multipoint-scheduler tier: control-plane spec: containers: - name: multipoint-scheduler image: easzlab.io.local:5000/k8s.gcr.io/kube-scheduler:v1.26.8 imagePullPolicy: IfNotPresent command: - kube-scheduler - --config=/etc/kubernetes/scheduler-config.yaml - --leader-elect=true - --leader-elect-resource-name=multipoint-scheduler - --logging-format=text - --v=6 resources: requests: cpu: 200m memory: 128Mi limits: memory: 128Mi livenessProbe: httpGet: path: /healthz port: 10259 scheme: HTTPS volumeMounts: - name: config-volume mountPath: /etc/kubernetes serviceAccountName: multipoint-scheduler-sa volumes: - name: config-volume configMap: name: multipoint-scheduler-config https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/ - --logging-format=json - --v=10 - --logging-format=text - --v=6 - --leader-elect=false - --leader-elect=true - --leader-elect-resource-name=multipoint-scheduler apiVersion: apps/v1 kind: Deployment metadata: name: yani spec: replicas: 1 selector: matchLabels: app: yani template: metadata: labels: app: yani spec: schedulerName: image-locality-scheduler containers: - name: yani image: docker.io/library/nginx:1.21.3 imagePullPolicy: IfNotPresent "},"notes/docker/volcano.html":{"url":"notes/docker/volcano.html","title":"volcano","keywords":"","body":"Volcano https://github.com/volcano-sh/volcano https://docs.otc.t-systems.com/cloud-container-engine/umn/add-ons/volcano_scheduler.html 一、安装 1、helm helm repo add volcano-sh https://volcano-sh.github.io/helm-charts helm repo update helm pull volcano-sh/volcano helm install volcano volcano-sh/volcano -n volcano-system --create-namespace 2、镜像 volcanosh/vc-controller-manager:v1.9.0 volcanosh/vc-webhook-manager:v1.9.0 volcanosh/vc-scheduler:v1.9.0 二、binpack priority，优先调度，逻辑是打分机制，默认打分机制是0-100，根据资源情况使用情况，使用越高，分数越低。 binpack 刚好相反，资源使用越低，分数越低。这种调度算法能够尽可能减小节点内的碎片，在空闲的机器上为申请了更大资源请求的Pod预留足够的资源空间，使集群下空闲资源得到最大化的利用。LeastRequestedPriority 1、queue apiVersion: scheduling.volcano.sh/v1beta1 kind: Queue metadata: name: default spec: reclaimable: true # 表示该queue在资源使用量超过该queue所应得的资源份额时，是否允许其他queue回收该queue使用超额的资源，默认值为true。（需要开启reclaim action） weight: 1 # 软限制 表示该queue在集群资源划分中所占的相对比重，该queue应得资源总量为 (weight/total-weight) * total-resource。当queue中任务较多时可以超过weight限制，去借用其他空闲queue的部分资源。 capability: # 硬限制 表示该queue内所有podgroup使用资源量之和的上限。 cpu: 4 memory: 4096Mi quarantee： # 硬限制 表示资源预留配置，即使queue空闲，也为其保留这些计算资源，这部分不许被其他queue共享。 resource: cpu: 500m memory: 1024Mi cat queue.yaml # create test queue and then run deployment. apiVersion: scheduling.volcano.sh/v1beta1 kind: Queue metadata: name: queue spec: weight: 1 reclaimable: false capability: cpu: 1 cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: deploy-with-volcano labels: app: nginx spec: replicas: 10 selector: matchLabels: app: nginx template: metadata: labels: app: nginx annotations: # create test queue and use this annotation # to make deployment be scheduled into test queue scheduling.volcano.sh/queue-name: queue spec: # set spec.schedulerName to 'volcano' instead of # 'default-scheduler' for deployment. schedulerName: volcano containers: - name: nginx image: easzlab.io.local:5000/nginx:1.21.3 ports: - containerPort: 80 resources: requests: cpu: 100m cat volcano-scheduler-binpack.conf actions: \"enqueue, allocate, backfill\" tiers: - plugins: - name: binpack - name: gang enablePreemptable: false - name: conformance - plugins: - name: overcommit enablePreemptable: false - name: predicates cat volcano-scheduler-default.conf actions: \"enqueue, allocate, backfill\" tiers: - plugins: - name: priority - name: gang enablePreemptable: false - name: conformance - plugins: - name: overcommit - name: drf enablePreemptable: false - name: predicates - name: proportion - name: nodeorder - name: binpack actions: \"enqueue, allocate, backfill\" tiers: - plugins: - name: gang # gang插件没有参数，开启即可。 podgroup的minMember依赖于gang插件 - name: predicates arguments: predicate.ProportionalEnable: true predicate.resources: nvidia.com/gpu predicate.resources.nvidia.com/gpu.cpu: 8 predicate.resources.nvidia.com/gpu.memory: 8 - name: nodeorder arguments: leastrequested.weight: 1 mostrequested.weight: 0 nodeaffinity.weight: 1 podaffinity.weight: 1 balancedresource.weight: 1 tainttoleration.weight: 1 imagelocality.weight: 2 - plugins: - name: binpack arguments: binpack.weight: 10 # binpack插件的权重（allocate action有许多计算插件，可以把某个插件权重调大，作为主要计算标准） binpack.cpu: 2 # binpack计算时，cpu所占权重较高 binpack.memory: 1 # binpack计算时，mem所占权重较低 volcano scheduler colocation_enable: '' default_scheduler_conf: actions: 'allocate, backfill, preempt' tiers: - plugins: - name: 'priority' - name: 'gang' - name: 'conformance' - name: 'lifecycle' arguments: lifecycle.MaxGrade: 10 lifecycle.MaxScore: 200.0 lifecycle.SaturatedTresh: 1.0 lifecycle.WindowSize: 10 - plugins: - name: 'drf' - name: 'predicates' - name: 'nodeorder' - plugins: - name: 'cce-gpu-topology-predicate' - name: 'cce-gpu-topology-priority' - name: 'cce-gpu' - plugins: - name: 'nodelocalvolume' - name: 'nodeemptydirvolume' - name: 'nodeCSIscheduling' - name: 'networkresource' tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 60 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 60 actions The following options are supported: enqueue: uses a series of filtering algorithms to filter out tasks to be scheduled and sends them to the queue to wait for scheduling. After this action, the task status changes from pending to inqueue. allocate: selects the most suitable node based on a series of pre-selection and selection algorithms. preempt: performs preemption scheduling for tasks with higher priorities in the same queue based on priority rules. backfill: schedules pending tasks as much as possible to maximize the utilization of node resources. plugins 1.binpack - plugins: - name: binpack arguments: binpack.weight: 10 binpack.cpu: 1 binpack.memory: 1 binpack.resources: nvidia.com/gpu, example.com/foo binpack.resources.nvidia.com/gpu: 2 binpack.resources.example.com/foo: 3 2.conformance - plugins: - name: 'priority' - name: 'gang' enablePreemptable: false - name: 'conformance' 3.lifecycle - plugins: - name: priority - name: gang enablePreemptable: false - name: conformance - name: lifecycle arguments: lifecycle.MaxGrade: 10 lifecycle.MaxScore: 200.0 lifecycle.SaturatedTresh: 1.0 lifecycle.WindowSize: 10 4.Gang - plugins: - name: priority - name: gang enablePreemptable: false enableJobStarving: false - name: conformance 5.priority - plugins: - name: priority - name: gang enablePreemptable: false - name: conformance 6.overcommit - plugins: - name: overcommit arguments: overcommit-factor: 2.0 7.drf - plugins: - name: 'drf' - name: 'predicates' - name: 'nodeorder' 8.predicates - plugins: - name: 'drf' - name: 'predicates' - name: 'nodeorder' 9.nodeorder - plugins: - name: nodeorder arguments: leastrequested.weight: 1 mostrequested.weight: 0 nodeaffinity.weight: 2 podaffinity.weight: 2 balancedresource.weight: 1 tainttoleration.weight: 3 imagelocality.weight: 1 podtopologyspread.weight: 2 三、架构 Volcano 核心组件主要包含三个：Admission、ControllerManager、Scheduler Admission 对 Volcano CRD API 提供校验能力 ControllerManager 负责对 Volcano CRD 进行资源管理 Scheduler 对任务提供丰富的调度能力 1.PodGroup 定义一组强关联的 Pod apiVersion: scheduling.volcano.sh/v1beta1 kind: PodGroup metadata: name: test namespace: default spec: minMember: 1 # 被gang插件使用，表示该podgroup下最少需要运行的pod数量。如果集群资源不满足miniMember数量的pod运行，调度器将不会调度任何一个该podgroup内的pod。 minResources: # minResources表示运行该podgroup所需要的最少资源。当集群可分配资源不满足minResources时，调度器将不会调度任何一个该podgroup内的pod。 cpu: \"3\" memory: \"2048Mi\" priorityClassName: high-prority queue: default # queue表示该podgroup所属的queue。queue必须提前已创建且状态为open。 2.Queue 划分集群中的计算资源，也是容纳PodGroup的队列 apiVersion: scheduling.volcano.sh/v1beta1 kind: Queue metadata: name: default spec: reclaimable: true # 表示该queue在资源使用量超过该queue所应得的资源份额时，是否允许其他queue回收该queue使用超额的资源，默认值为true。（需要开启reclaim action） weight: 1 # 软限制 表示该queue在集群资源划分中所占的相对比重，该queue应得资源总量为 (weight/total-weight) * total-resource。当queue中任务较多时可以超过weight限制，去借用其他空闲queue的部分资源。 capability: # 硬限制 表示该queue内所有podgroup使用资源量之和的上限。 cpu: 4 memory: 4096Mi quarantee： # 硬限制 表示资源预留配置，即使queue空闲，也为其保留这些计算资源，这部分不许被其他queue共享。 resource: cpu: 500m memory: 1024Mi 3.Volcano Job vcjob，是 volcano 自定义的 job 资源类型。区别于 k8s job，vcjob 提供了更多高级功能，如可指定调度器、支持最小运行 pod 数、支持 task、支持生命周期管理、支持指定队列、支持优先级调度等。 apiVersion: batch.volcano.sh/v1alpha1 kind: Job metadata: name: volcano-sample-job spec: minAvailable: 2 # Gang Scheduling：至少 2 个任务必须可用，任务才会被调度 schedulerName: volcano tasks: - replicas: 2 # 任务的副本数 name: \"task1\" template: spec: containers: - image: nginx name: nginx command: [\"/bin/sh\", \"-c\", \"sleep 30\"] restartPolicy: Never - replicas: 1 # 另一个任务 name: \"task2\" policies: - event: TaskCompleted action: CompleteJob template: spec: containers: - image: busybox name: busybox command: [\"/bin/sh\", \"-c\", \"echo 'Task 2 completed'\"] restartPolicy: Never 4.Queue、PodGroup、VolcanoJob 关系 Queue 是一个 PodGroup 队列，PodGroup 是一组强关联的 Pod 集合。 vcjob 对应的下一级资源是 PodGroup。 5.插件 actions: \"enqueue, allocate, backfill\" tiers: - plugins: - name: gang ## gang插件没有参数，开启即可。 podgroup的minMember依赖于gang插件 - name: predicates arguments: predicate.ProportionalEnable: true predicate.resources: nvidia.com/gpu predicate.resources.nvidia.com/gpu.cpu: 8 predicate.resources.nvidia.com/gpu.memory: 8 - name: nodeorder arguments: leastrequested.weight: 1 mostrequested.weight: 0 nodeaffinity.weight: 1 podaffinity.weight: 1 balancedresource.weight: 1 tainttoleration.weight: 1 imagelocality.weight: 2 - plugins: - name: binpack arguments: binpack.weight: 10 ## binpack插件的权重（allocate action有许多计算插件，可以把某个插件权重调大，作为主要计算标准） binpack.cpu: 2 ## binpack计算时，cpu所占权重较高 binpack.memory: 1 ## binpack计算时，mem所占权重较低 "},"notes/docker/kubeasz.html":{"url":"notes/docker/kubeasz.html","title":"kubeasz","keywords":"","body":"Kubeasz https://github.com/easzlab/kubeasz 一、准备 1.下载工具ezdown export release=3.6.4 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown 2.下载kubeasz代码、二进制、默认容器镜像 # inside ./ezdown -D # outside ./ezdown -D -m standard 2.1下载额外容器镜像（可选） ./ezdown -X flannel ./ezdown -X prometheus ./ezdown -X cilium 2.2下载离线系统包（yum/apt）（可选） ./ezdown -P 3.目录结构 /etc/kubeasz 包含kubeasz版本为${release}的代码 /etc/kubeasz/bin 包含k8s/etcd/docker/cni等二进制文件 /etc/kubeasz/down 包含集群安装时需要的离线容器镜像 /etc/kubeasz/down/packages 包含集群安装时需要的系统基础软件 二、安装 容器化运行kubeasz ./ezdown -S 使用默认配置安装aio集群 docker exec -it kubeasz ezctl start-aio 如果安装失败，查看日志排查后，重装 docker exec -it kubeasz ezctl setup default all 三、验证 source ~/.bashrc kubectl version kubectl get node kubectl get pod -A kubectl get svc -A 四、安装dashboard 1.部署 # ezctl 集成部署组件，xxxx 代表集群部署名 # dashboard 部署文件位于 /etc/kubeasz/clusters/xxxx/yml/dashboard/ 目录 ./ezctl setup xxxx 07 2.验证 # 查看pod 运行状态 kubectl get pod -n kube-system | grep dashboard dashboard-metrics-scraper-856586f554-l6bf4 1/1 Running 0 35m kubernetes-dashboard-698d4c759b-67gzg 1/1 Running 0 35m # 查看dashboard service kubectl get svc -n kube-system|grep dashboard kubernetes-dashboard NodePort 10.68.219.38 443:24108/TCP 53s # 查看pod 运行日志 kubectl logs -n kube-system kubernetes-dashboard-698d4c759b-67gzg 3.登录 因为dashboard 作为k8s 原生UI，能够展示各种资源信息，甚至可以有修改、增加、删除权限，所以有必要对访问进行认证和控制，为演示方便这里使用 https://NodeIP:NodePort 方式访问 dashboard，支持两种登录方式：Kubeconfig、令牌(Token) Token令牌登录（admin） # 获取 Bearer Token，找到输出中 ‘token:’ 开头的后面部分 $ kubectl describe -n kube-system secrets admin-user Token令牌登录（只读） # 获取 Bearer Token，找到输出中 ‘token:’ 开头的后面部分 $ kubectl describe -n kube-system secrets dashboard-read-user Kubeconfig登录（admin） Admin kubeconfig文件默认位置：/root/.kube/config，该文件中默认没有token字段，使用Kubeconfig方式登录，还需要将token追加到该文件中，完整的文件格式如下： apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdxxxxxxxxxxxxxx server: https://192.168.0.127:6443 name: kubernetes contexts: - context: cluster: kubernetes user: admin name: kubernetes current-context: kubernetes kind: Config preferences: {} users: - name: admin user: client-certificate-data: LS0tLS1CRUdJTiBDRxxxxxxxxxxx client-key-data: LS0tLS1CRUdJTxxxxxxxxxxxxxx token: eyJhbGcixxxxxxxxxxxxxxxx 五、清理 docker exec -it kubeasz ezctl destroy default "},"notes/docker/calico.html":{"url":"notes/docker/calico.html","title":"calico","keywords":"","body":"Calico Calico是三层虚拟网络解决方案（BGP）。每一个节点都是一个vRouter，都需要通过BGP协议学习生成路由规则，从而实现各节点上Pod之间互联互通。 一、BGP通信模型 BGP模型要求所有节点在同一个二层网络中。不一定所有的底层网络都支持BGP。 1、BGP peer（小规模网络使用） 点对点BGP，如果一个网络中有10个BGP，即是1:9的通信模型，形成n*(n-1)个通信网络。 所以在此模型下如果网络规模较大BGP路由学习报文会占据很大的网络带宽。 BGP peer不存在单点问题，BGP peer宕机会有其他的进行替代。 2、BGP Reflector（大规模网络使用） 反射器模型，所有节点都将自己所有拥有的路由信息汇总给Reflector，由Reflector用1:n-1的方式向外进行反射。 BGP Reflector需要做冗余。 二、Overlay Network 1、IPIP 用IP报文来封装IP报文，因此其开销更小。 2、VXLAN 类似于Flannel的VXLAN启动DirectRouting的网络模型，Calico也支持混合使用路由和叠加网络模型。 如果节点在同一子网内使用BGP，如果跨子网则使用VXLAN。 三、架构 Flannel中host-gw模型使用veth-pair Calico使用内核Iptables和Routes表来完成其中部分功能。 主要组件： 1.每个节点都需要运行组件： BGP客户端（默认启用）：需要运行于每个节点，负责将Felix生成的路由信息载入内核并通告到整个网络中。 BGP Reflector：专用反射各BGP客户端发来路由信息，将N->N-1转为N->1模型。 Felix：需要运行于各节点之上的守护进程，主要负责完成接口管理、路由规划、acl规划（即网络策略，借助iptables实现）、状态报告。 BIRD：是vRouter的关键实现，整个BGP的路由表是由BIRD生成的，而路由规划是Felix完成的。BIRD自身可以扮演两种角色。 2.在节点之外需要运行组件： etcd：Calico也会Flannel一样需要依靠etcd来保存一些自身的状态数据，也可以像Flannel一样将API Server当为自身的存储后端。大规模集群中建议额外部署etcd专用于Calico集群，以免和K8S性能上冲突。 Route Reflector：路由反射器。 Calico编排系统插件：Calico不仅支持给K8S提供虚拟网络，也支持OpenShift、OpenStack。所以Calico是一个通用的虚拟网络。要让Calico能适用于K8S，需要一个Calico的编排系统插件让etcd和Calico插件之间能双向转换通信。 3.K8S所需组件： calico-node：类似于Flanneld需要运行于每个节点之上。calico-node中封装了Felix和BIRD。 calico-kube-controller：运行于K8S集群上的中央控制系统。负责Calico和整个K8S的协同，也包括其他核心功能实现。 四、部署 https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises 1、下载calico资源清单 curl https://docs.projectcalico.org/manifests/calico.yaml -O 2、配置 确保Pod的CIDR为192.168.0.0/16，如果非此网点则需要修改calico.yaml。 3、部署 kubectl apply -f calico.yaml 4、验证 calico默认使用IPIP模型。 # 查看路由信息 ip r list blackhole 10.244.0.0/24 proto bird # blackhole 表示当前节点。 10.244.0.4 dev cali5411bb555f9 scope link # 此处可以看到 pod 的数据流出是直接到达内核的 10.244.1.0/24 via 172.16.11.81 dev tunl0 proto bird onlink # 出现 tunl0 接口，现在报文发送时会发送给 tunl0 接口 # tunl0 宿主机内核 查看calico的地址池 kubectl get ippools -o yaml 验证ipip工作逻辑 tcpdump -i eth0 -nn ip host k8s-node01 and host k8s-node03 02:33:55.805615 IP 172.16.11.81 > 172.16.11.83: IP 10.244.1.16.49490 > 10.244.3.16.10016: Flags [P.], seq 1133911456:1133911482, ack 3769557439, win 85, options [nop,nop,TS val 3535292140 ecr 2812652948], length 26 (ipip-proto-4) # 从以上抓包结果中可以看出 ipip 在通信时，分为外部 ip 和内部 ip 2层。 # ipip-proto-4 表示此报文为 ipip 报文。 五、calicoctl curl -o /usr/bin/kubectl-calico -O -L \"https://github.com/projectcalico/calicoctl/releases/download/v3.21.5/calicoctl-linux-amd64\" kubectl-calico -h kubectl calico -h vim /etc/calico/calicoctl.cfg apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \"kubernetes\" # 此处申明后端存储为kubernetes kubeconfig: \"/root/.kube/config\" 获取节点 kubectl calico get nodes 查看节点状态 kubectl-calico node status 获取地址池 kubectl calico get ippool kubectl calico get ippool -o yaml 查看地址分配信息 kubectl calico ipam show --allow-version-mismatch 查看每个节点上的地址分配信息 kubectl calico ipam show --show-blocks 查看ipam配置信息 kubectl calico ipam show --show-configuration +--------------------+-------+ | PROPERTY | VALUE | +--------------------+-------+ | StrictAffinity | false | # pod被重建后是否使用原有地址 | AutoAllocateBlocks | true | # 是否支持自动分配地址 | MaxBlocksPerHost | 0 | +--------------------+-------+ 六、配置 kubectl calico get ippools -o yaml apiVersion: projectcalico.org/v3 items: - apiVersion: projectcalico.org/v3 kind: IPPool metadata: creationTimestamp: \"2024-05-06T06:00:24Z\" name: default-ipv4-ippool resourceVersion: \"6789\" uid: 943b85b2-9759-49ce-8f73-78f1f3f8a111 spec: blockSize: 24 cidr: 192.168.0.0/16 ipipMode: CrossSubnet # 将ipipMode改为CrossSubnet或Never natOutgoing: true nodeSelector: all() vxlanMode: Never kind: IPPoolList metadata: resourceVersion: \"9418\" # 改BGP模式需要修改ipipMode # CrossSubnet表示混杂模式也就是混合模式，表示跨节点子网时才使用IPIP # Never表示纯BGP模式 # vxlanMode: CrossSubnet ipipMode: Never 表示VxLan的混合模型 将其重新应用到网络中 kubectl calico apply -f default-ipv4-ippool.yaml BGP生效再次查看路由信息 ip route list 其他 1、在部署Calico时，发现网卡的过程通常涉及配置Calico以正确识别和使用宿主机的网络接口。 IP自动检测机制：默认情况下，它可能会选择第一个找到的有效网卡。 first-found：列出所有网卡IP并选择第一个（忽略特定网卡如docker0和lo）。 修改IP_AUTODETECTION_METHOD can-reach：指定一个目标IP或域名，Calico将尝试从所有网卡中找到能够到达该目标IP的网卡，并使用其IP地址。can-reach=x.x.x.x：选择能够到达指定IP地址的网卡。 interface：使用正则表达式来指定Calico应该使用的网卡名称。 kubernetes-internal-ip：从Kubernetes API获取Pod的IP地址（适用于Kubernetes环境）。 first-found：列出所有网卡IP并选择第一个（忽略特定网卡如docker0和lo）。 env: - name: IP_AUTODETECTION_METHOD value: \"interface=eth0\" env: - name: IP_AUTODETECTION_METHOD value: \"first-found\" # 显式指定first-found方法，尽管这通常是默认行为 "},"notes/docker/ingress.html":{"url":"notes/docker/ingress.html","title":"ingress","keywords":"","body":"Ingress https://github.com/kubernetes/ingress-nginx https://github.com/nginxinc/kubernetes-ingress https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/ https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/configmap.md https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/ https://github.com/bitnami/charts/tree/main/bitnami/nginx-ingress-controller 一、安装 1、ingress-nginx-4.10.1.tgz helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update 2、修改配置 vim ingress-nginx/values.yaml controller: image: registry: easzlab.io.local:5000 image: ingress-nginx/controller tag: \"v1.10.1\" nodeSelector: nginx/ingress: ai admissionWebhooks: enabled: false allowSnippetAnnotations: true metrics: port: 10254 portName: metrics enabled: true service: annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"10254\" serviceMonitor: enabled: true additionalLabels: release: \"ai-kube-prometheus-stack\" annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"10254\" prometheusRule: enabled: true additionalLabels: release: \"ai-kube-prometheus-stack\" enable-vts-status: \"true\" ，以导出 Prometheus 指标. prometheus.io/scrape: \"true\" ，以启用自动发现. prometheus.io/port: \"10254\" ，以指定度量标准端口. admissionWebhooks.enabled.false 如果为true创建ingress会报错 3、部署 helm install -n kube-system nginx-ingress . 4、检查 kubectl get servicemonitor -A|grep ingress kubectl get svc -A|grep ingress kubectl get po -A|grep ingress 二、验证 1、创建测试应用 vim example_nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment-1 labels: app: nginx-1 spec: replicas: 1 selector: matchLabels: app: nginx-1 template: metadata: labels: app: nginx-1 spec: containers: - name: nginx image: easzlab.io.local:5000/nginx:1.21.3 command: [\"/bin/sh\", \"-c\", \"mkdir /usr/share/nginx/html/v1;echo 'hello nginx-1'>/usr/share/nginx/html/v1/index.html;nginx -g 'daemon off;'\"] ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment-2 labels: app: nginx-2 spec: replicas: 1 selector: matchLabels: app: nginx-2 template: metadata: labels: app: nginx-2 spec: containers: - name: nginx image: easzlab.io.local:5000/nginx:1.21.3 command: [\"/bin/sh\", \"-c\", \"mkdir /usr/share/nginx/html/v2;echo 'hello nginx-2'>/usr/share/nginx/html/v2/index.html;nginx -g 'daemon off;'\"] ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx1-svc labels: spec: ports: - port: 80 targetPort: 80 protocol: TCP name: http selector: app: nginx-1 --- apiVersion: v1 kind: Service metadata: name: nginx2-svc labels: spec: ports: - port: 80 targetPort: 80 protocol: TCP name: http selector: app: nginx-2 docker pull kennethreitz/httpbin kubectl run httpbin --image kennethreitz/httpbin --port 80 kubectl expose pod httpbin --port 80 curl --location -H \"Host: k8s.ingress.org\" --request GET \"http://10.15.200.45:32431/get?foo1=bar1&foo2=bar2\" 2、创建ingress vim ingress-k8s.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-ingress spec: ingressClassName: nginx rules: - host: k8s.ingress.org http: paths: - path: /v1 pathType: Prefix backend: service: name: nginx1-svc port: number: 80 - path: /v2 pathType: Prefix backend: service: name: nginx2-svc port: number: 80 annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 nginx.ingress.kubernetes.io/proxy-connect-timeout: 10s nginx.ingress.kubernetes.io/rewrite-target: /$1 - path: /foo(/|$)(.*) 3、配置hosts kubectl get ingress|grep test-ingress test-ingress nginx testnginx.com 80 36m kubectl get po -A -o wide|grep ingress kube-system ai-nginx-ingress-ingress-nginx-controller-6fc498bdb7-8js29 1/1 Running 0 38m 172.22.163.246 192.168.0.127 vim /etc/hosts 172.22.163.246 testnginx.com 4、测试 curl testnginx.com/v1 curl testnginx.com/v2 三、主要监控指标 https://github.com/kubernetes/ingress-nginx/blob/helm-chart-4.10.1/docs/user-guide/monitoring.md https://grafana.com/grafana/dashboards/14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana/ https://grafana.com/grafana/dashboards/?search=ingress-nginx https://grafana.com/api/dashboards/14314/revisions/2/download https://grafana.com/api/dashboards/20510/revisions/1/download nginx_ingress_controller_requests 四、config https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#plugins https://github.com/kubernetes/ingress-nginx/blob/helm-chart-4.10.1/docs/user-guide/nginx-configuration/annotations.md kubectl edit configmap -n kube-system ai-nginx-ingress-ingress-nginx-controller apiVersion: v1 data: allow-snippet-annotations: \"true\" plugins: hello_world, hello_hl vim ingress-k8s.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: k8s-ingress annotations: nginx.ingress.kubernetes.io/server-snippet: | gzip on; gzip_comp_level 5; gzip_min_length 256; gzip_proxied any; gzip_vary on; gzip_types application/atom+xml application/javascript application/x-javascript application/json application/rss+xml application/vnd.ms-fontobject application/x-font-ttf application/x-web-app-manifest+json application/xhtml+xml application/xml font/opentype image/svg+xml image/x-icon text/css text/plain text/x-component; 五、gzip 配置项 作用 示例 gzip 是否开启gzip压缩 gzip on; gzip_types 指定要压缩的MIME类型 gzip_types text/html text/plain application/javascript; gzip_min_length 指定最小压缩文件大小 gzip_min_length 1000; gzip_comp_level 指定压缩级别 范围为1到9,值越大压缩程度越大 gzip_comp_level 6; gzip_buffers 指定用于gzip压缩的内存缓冲区大小 gzip_buffers 16 8k; gzip_disable 指定不使用gzip压缩的User-Agent gzip_disable “MSIE [1-6].(?!.*SV1)”; gzip_proxied 根据客户端请求中的\"Accept-Encoding\"头部决定是否压缩响应，取值可以是 “off”、“expired”、“no-cache”、“no-store”、“private”、“no_last_modified”、“no_etag”、“auth” 或 “any” gzip_proxied any； gzip_vary 如果发送的响应被gzip压缩，则在响应头部加上\"Vary: Accept-Encoding\"，以通知缓存服务器响应内容可能以压缩或非压缩形式存在 gzip_vary:on; gzip_http_version 设置进行gzip压缩的HTTP协议版本。 gzip_http_version:1.0 Accept-Encoding: gzip, deflate Content-Encoding: gzip 六、Websocket server.py import asyncio import websockets import logging # 配置日志记录到标准输出 logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\") async def echo(websocket, path): async for message in websocket: logging.info(f\"Received message: {message}\") await websocket.send(f\"Echo: {message}\") # WebSocket 服务地址和端口 start_server = websockets.serve(echo, \"0.0.0.0\", 8765) if __name__ == \"__main__\": logging.info(\"Starting WebSocket server...\") asyncio.get_event_loop().run_until_complete(start_server) asyncio.get_event_loop().run_forever() FROM python:3.8.14 COPY server.py /opt/server.py RUN pip install websockets CMD [\"python\", \"/opt/server.py\"] docker build -t ingress_websocket . kubectl run websocket --image easzlab.io.local:5000/ingress_websocket --port 8765 kubectl expose pod websocket --port 8765 ingress_websocket.py apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: websocket annotations: nginx.ingress.kubernetes.io/proxy-read-timeout: \"3600\" nginx.ingress.kubernetes.io/proxy-send-timeout: \"3600\" nginx.ingress.kubernetes.io/server-snippets: | location / { proxy_set_header Upgrade $http_upgrade; proxy_http_version 1.1; proxy_set_header Connection \"upgrade\"; } spec: ingressClassName: nginx rules: - host: websocket.aa.com http: paths: - backend: service: name: websocket port: number: 8765 path: / pathType: Exact kubectl apply -f ingress_websocket.py ws://：用于普通 WebSocket 连接。 wss://：用于安全 WebSocket 连接（SSL/TLS 支持）。 websockt 连接 wss://websocket.aa.com:8888 七、timeout from flask import Flask import time app = Flask(__name__) @app.route(\"/\") def slow_response(): time.sleep(10) # 模拟 10 秒的延迟 return \"Hello, World!\" if __name__ == \"__main__\": app.run(debug=True) curl --max-time 10 --connect-timeout 2 -H \"Host: timeout.aa.com\" 192.168.0.127:32000/headers 八、性能参数优化 config: worker-processes: \"auto\" worker-cpu-affinity: \"auto\" max-worker-connections: \"8192\" reuse-port: \"true\" server-tokens: \"false\" ssl-redirect: \"false\" proxy-connect-timeout: \"900\" proxy-read-timeout: \"900\" proxy-send-timeout: \"900\" proxy-body-size: \"10m\" proxy-buffer-size: \"16k\" proxy-buffers-number: \"4\" upstream-keepalive-timeout: \"900\" upstream-keepalive-requests: \"900\" upstream-keepalive-connections: \"900\" keep-alive: \"900\" keep-alive-requests: \"900\" client-body-timeout: \"900\" client-body-buffer-size: \"64k\" use-http2: \"true\" use-gzip: \"true\" gzip-min-length: \"1024\" gzip-level: \"6\" gzip-types: \"text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss\" custom-http-errors: \"400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,421,422,423,424,425,426,428,429,431,451,500,501,502,503,504,505,506,507,508,510,511\" 九、性能测试 nginx.conf: | master_process on; worker_processes 1; events { worker_connections 4096; } http { access_log off; server_tokens off; keepalive_requests 10000000; server { listen 80; server_name _; location / { proxy_set_header Connection \"\"; return 200 \"hello world\\n\"; } } } image.ac.com:5000/k8s/bitnami/nginx:1.27.3-debian-12-r0 /opt/bitnami/nginx/conf/nginx.conf kubectl expose pod websocket --port 80 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress namespace: default spec: ingressClassName: nginx rules: - host: ingress.sugon.fun http: paths: - backend: service: name: ingress-default-backend port: number: 80 path: / pathType: Prefix "},"notes/docker/apisix.html":{"url":"notes/docker/apisix.html","title":"apisix","keywords":"","body":"APISIX usual curl -H \"Host: apisix.ingress.org\" --request GET \"http://192.168.0.127:32060/headers\" curl -s http://10.66.117.129:9180/apisix/admin/global_rules -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X PUT -d '{ \"id\": \"1\", \"plugins\": { \"prometheus\": { \"prefer_name\": true }, \"cors\": {} } }' curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/routes -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/plugins/list -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules/1 -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X DELETE curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq 一、安装 1、chart 包 helm repo add apisix https://charts.apiseven.com helm repo update helm pull apisix/apisix-ingress-controller helm install apisix apisix/apisix --create-namespace --namespace ingress-apisix 2、镜像 docker pull busybox:1.28 docker pull apache/apisix:3.9.1-debian docker pull bitnami/etcd:3.5.10-debian-11-r2 docker pull apache/apisix-ingress-controller:1.8.2 docker pull apache/apisix-dashboard:3.0.1-alpine docker save busybox:1.28 apache/apisix:3.9.1-debian bitnami/etcd:3.5.10-debian-11-r2 apache/apisix-ingress-controller:1.8.2 apache/apisix-dashboard:3.0.1-alpine |gzip > ingress-apisix_3.9.1_imgs.tar.gz 3、配置 vim apisix/values.yaml image: repository: easzlab.io.local:5000/apache/apisix replicaCount: 1 nodeSelector: nginx/ingress: ai initContainer: image: easzlab.io.local:5000/busybox metrics: serviceMonitor: enabled: true namespace: \"ingress-apisix\" labels: release: \"ai-kube-prometheus-stack\" prometheus: enabled: true etcd: enabled: true dashboard: enabled: true ingress-controller: enabled: true config: apisix: adminAPIVersion: \"v3\" nodeSelector: kubernetes.io/hostname: 10.13.1.12 nodeSelector: nginx/ingress: ai resources: requests: cpu: 100m memory: 128Mi limits: cpu: 16 memory: 32Gi tolerations: - key: \"dedicated\" operator: \"Equal\" value: \"ingress\" effect: \"NoExecute\" affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx-ingress topologyKey: kubernetes.io/hostname vim apisix/charts/etcd/values.yaml image: registry: easzlab.io.local:5000 replicaCount: 1 nodeSelector: nginx/ingress: ai persistence: enabled: false vim apisix/charts/apisix-dashboard/values.yaml replicaCount: 1 image: repository: easzlab.io.local:5000/apache/apisix-dashboard nodeSelector: nginx/ingress: ai vim apisix/charts/apisix-ingress-controller/values.yaml replicaCount: 1 image: repository: easzlab.io.local:5000/apache/apisix-ingress-controller apisix: serviceNamespace: ingress-apisix adminAPIVersion: \"v3\" initContainer: image: easzlab.io.local:5000/busybox nodeSelector: nginx/ingress: ai serviceMonitor: enabled: false namespace: \"ingress-apisix\" labels: release: \"ai-kube-prometheus-stack\" 修改Ingress class vim charts/apisix-ingress-controller/values.yaml .Values.config.kubernetes.ingressClass ingressClass: \"apisix\" -> ingressClass: \"nginx\" 4、部署 helm install -n ingress-apisix apisix . 5、test kubectl apply -f -二、Ingress 1、ingress vim ingress-apisix.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: apisix-ingress spec: ingressClassName: apisix rules: - host: apisix.ingress.org http: paths: - backend: service: name: nginx1-svc port: number: 80 path: /v1 pathType: Prefix - backend: service: name: nginx2-svc port: number: 80 path: /v2 pathType: Prefix - backend: service: name: httpbin port: number: 80 path: /get pathType: Prefix 2、ApisixRoute apiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: httpserver-route spec: http: - name: rule1 match: hosts: - httpbin.example.com paths: - /* backends: - serviceName: httpbin servicePort: 80 配置多个域名和路径 - name: rule1 match: hosts: - local.example.com - local01.example.com paths: - /* - /api backends: - serviceName: httpbin servicePort: 80 regex - name: httpserver-route match: hosts: - local.example.com paths: - /api* backends: - serviceName: httpbin servicePort: 8000 plugins: - name: proxy-rewrite enable: true config: regex_uri: [\"^/api(/|$)(.*)\", \"/$2\"] gzip plugins: - name: gzip enable: true http to https plugins: - name: redirect enable: true config: http_to_https: true 域名跳转 plugins: - name: redirect enable: true config: uri: \"https://local01.example.com$request_uri\" rewrite路径跳转，/api/header -> /header plugins: - name: redirect enable: true config: regex_uri: [\"^/api(/|$)(.*)\", \"/$2\"] plugins: - name: proxy-rewrite enable: true config: regex_uri: [\"^/api(/|$)(.*)\", \"/$2\"] /header -> /api/header plugins: - name: proxy-rewrite enable: true config: uri: /api/$uri 基于用户名和密码认证 apiVersion: apisix.apache.org/v2 kind: ApisixConsumer metadata: name: httpserver-basicauth spec: authParameter: basicAuth: value: username: admin password: admin --- apiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: httpserver-route spec: http: - name: httpserver-route match: hosts: - local.example.com paths: - /* backends: - serviceName: httpbin servicePort: 8000 authentication: enable: true type: basicAuth curl -i -uadmin:admin https://local.example.com/ 三、instance kubectl run httpbin --image kennethreitz/httpbin --port 80 kubectl expose pod httpbin --port 80 curl --location -H \"Host: apisix.ingress.org\" --request GET \"http://192.168.0.127:32431/get?foo1=bar1&foo2=bar2\" example-nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment-1 labels: app: nginx-1 spec: replicas: 1 selector: matchLabels: app: nginx-1 template: metadata: labels: app: nginx-1 spec: containers: - name: nginx image: easzlab.io.local:5000/nginx:1.21.3 command: [\"/bin/sh\", \"-c\", \"mkdir /usr/share/nginx/html/v1;echo 'hello nginx-1'>/usr/share/nginx/html/v1/index.html;nginx -g 'daemon off;'\"] ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment-2 labels: app: nginx-2 spec: replicas: 1 selector: matchLabels: app: nginx-2 template: metadata: labels: app: nginx-2 spec: containers: - name: nginx image: easzlab.io.local:5000/nginx:1.21.3 command: [\"/bin/sh\", \"-c\", \"mkdir /usr/share/nginx/html/v2;echo 'hello nginx-2'>/usr/share/nginx/html/v2/index.html;nginx -g 'daemon off;'\"] ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx1-svc labels: spec: ports: - port: 80 targetPort: 80 protocol: TCP name: http selector: app: nginx-1 --- apiVersion: v1 kind: Service metadata: name: nginx2-svc labels: spec: ports: - port: 80 targetPort: 80 protocol: TCP name: http selector: app: nginx-2 httpbin-route.yaml apiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: httpbin-route spec: http: - name: route-1 match: hosts: - httpbin.org paths: - /* backends: - serviceName: httpbin servicePort: 80 curl httpbin.org:32060/headers 四、监控 curl -i http://$(kubectl get svc -n ingress-apisix apisix-prometheus-metrics -o jsonpath=\"{.spec.clusterIP}\"):9091/apisix/prometheus/metrics 主要指标 https://apisix.apache.org/docs/apisix/plugins/prometheus/#using-grafana-to-graph-the-metrics https://apisix.apache.org/zh/docs/apisix/plugins/prometheus/ apisix_http_status apisix_bandwidth apisix_http_requests_total apisix_nginx_http_current_connections 五、Annotations and Config https://apisix.apache.org/zh/docs/ingress-controller/concepts/annotations/ https://github.com/apache/apisix/blob/release/3.3/conf/config-default.yaml 六、使用 1、dashbord port kubectl get svc -n ingress-apisix -o jsonpath=\"{.spec.ports[0].nodePort}\" apisix-dashboard username/password admin/admin bug https://github.com/apache/apisix-dashboard/issues/2791 apisix dashboard与apisix的配置文件中都配置plugins 2、gateway 连接信息 export NODE_PORT=$(kubectl get -n ingress-apisix -o jsonpath=\"{.spec.ports[0].nodePort}\" services apisix-gateway) export NODE_IP=$(kubectl get nodes -n ingress-apisix -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT export NODE_PORT=$(kubectl get -n ingress-apisix -o jsonpath=\"{.spec.ports[0].nodePort}\" services apisix-gateway) export apisix=$(echo http://apisix.ingress.org:$NODE_PORT) 3、查看已注册路由信息 curl \"http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/routes?page=1&page_size=10\" -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X GET curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/routes -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq 4、API 使用 admin-api control-api curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/routes -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/plugins/list -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq 5、gzip _meta: disable: false buffers.number: 32 buffers.size: 4096 comp_level: 5 http_version: 1.1 min_length: 20 types: '*' vary: true { \"_meta\": { \"disable\": false }, \"buffers\": { \"number\": 32, \"size\": 4096 }, \"comp_level\": 5, \"http_version\": 1.1, \"min_length\": 20, \"types\": \"*\", \"vary\": true } enable plugin curl -i http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules/gzip \\ -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X PUT -d ' { \"plugins\": { \"gzip\": { \"_meta\": { \"disable\": false }, \"comp_level\": 5, \"http_version\": 1.1, \"min_length\": 20, \"types\": \"*\", \"vary\": true, \"buffers\": { \"number\": 32, \"size\": 4096 } } } }' 查看 curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules/gzip -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq delete plugin curl http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules/gzip -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X DELETE 验证 Accept-Encoding: gzip, deflate Content-Encoding: gzip curl apisix.ingress.org:30962/index.html -i -H \"Accept-Encoding: gzip\" 根据代码中所做的更改重新加载插件 curl http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/plugins/reload -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X PUT 5、http2 config.yaml apisix: enable_http2: true 6、header k8s.apisix.apache.org/response-rewrite-add-header: \"testkey1:testval1,testkey2:testval2\" k8s.apisix.apache.org/response-rewrite-set-header: \"testkey1:testval1,testkey2:testval2\" k8s.apisix.apache.org/response-rewrite-remove-header: \"testkey1,testkey2\" 7、upstream timeout annotations: k8s.apisix.apache.org/upstream-read-timeout.: \"5s\" k8s.apisix.apache.org/upstream-connect-timeout: \"10s\" k8s.apisix.apache.org/upstream-send-timeout: \"10s\" 8、http-to-https annotations: k8s.apisix.apache.org/http-to-https: \"true\" 9、use-regex apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: apisix-ingress annotations: k8s.apisix.apache.org/use-regex: \"true\" spec: ingressClassName: apisix rules: - host: apisix.ingress.org http: paths: - backend: service: name: nginx1-svc port: number: 80 path: /v1/.*/action1 pathType: ImplementationSpecific 10、websocket nginx location / { // 启用支持websocket连接 proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } annotation curl http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/routes/665bd12e \\ -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X PATCH -i -d ' { \"enable_websocket\": true }' curl http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/routes/f7c122fc \\ -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X PATCH -i -d ' { \"enable_websocket\": false }' curl -s $(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/routes/f7c122fc -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: apisix-ingress annotations: k8s.apisix.apache.org/enable-websocket: \"true\" spec: ingressClassName: apisix rules: - host: apisix.ingress.org http: paths: - backend: service: name: websocket port: number: 8765 path: /ws pathType: Exact client import asyncio import websockets async def main(): async with websockets.connect(\"ws://localhost:8765\") as websocket: message = \"Hello, server\" await websocket.send(message) print(f\"Sent: {message}\") response = await websocket.recv() print(f\"Received: {response}\") # asyncio.run(main()) # python3 asyncio.get_event_loop().run_until_complete(main()) server import asyncio import websockets # 连接处理器 async def echo(websocket, path): async for message in websocket: print(message) await websocket.send(message) # 启动服务器 start_server = websockets.serve(echo, \"0.0.0.0\", 8765) # 创建一个事件循环并运行服务器 asyncio.get_event_loop().run_until_complete(start_server) asyncio.get_event_loop().run_forever() FROM python:3.12.4-alpine3.20 COPY socket_server.py /opt/ RUN pip install websockets -i https://pypi.tuna.tsinghua.edu.cn/simple CMD [\"python\", \"/opt/socket_server.py\"] kubectl run websocket --image easzlab.io.local:5000/websocket-test:latest --port 8765 kubectl expose pod websocket --port 8765 postman ws://apisix.ingress.org:30962/ws 11、Customize Nginx configuration https://github.com/apache/apisix/blob/master/docs/zh/latest/customize-nginx-configuration.md https://github.com/apache/apisix/blob/release/3.3/conf/config-default.yaml http://nginx.org/en/docs/http/ngx_http_core_module.html#http https://nginx.org/en/docs/ 12、prometheus 插件 启用插件 curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X PUT -d ' { \"id\":\"1\", \"plugins\":{ \"prometheus\":{ } } }' curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules -X PUT -d ' { \"uri\": \"/test/index.html\", \"plugins\": { \"redirect\": { \"uri\": \"https://192.168.0.127/error\", \"ret_code\": 301 } } }' 13、limit-req limit-req插件使用漏桶算法限制单个客户端对服务的请求速率。 https://apisix.apache.org/zh/docs/apisix/plugins/limit-req/ 插件主要参数： key用来做请求计数的依据 rate请求速率（以秒为单位） burst支持突发请求量 nodelay不延迟突发请求 curl http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/routes/1 -H 'X-API-KEY: edd1c9f034335f136f87ad84b635c8f1' -X PUT -d ' { \"uri\": \"/search/*\", \"plugins\": { \"limit-conn\": { \"conn\": 10, \"burst\": 0, \"default_conn_delay\": 0.1, \"rejected_code\": 503, \"key\": \"remote_addr\" } }, \"upstream\": { \"type\": \"roundrobin\", \"nodes\": { \"39.97.63.215:80\": 1 } } }' 14、limit-conn limit-conn插件用于限制客户端对单个服务的并发请求数。当客户端对路由的并发请求数达到限制时，可以返回自定义的状态码和响应信息。 15、limit-count limit-count插件使用固定的窗口算法，主要用于限制单个客户端在指定时间范围内对服务的总请求数，并且会在HTTP响应头中返回剩余可以请求的个数。 16、synchronizing Kubernetes resources to APISIX kubectl get cm -n ingress-apisix apisix-configmap -o yaml|sed -e 's/6h/30s/g'|kubectl apply -f - kubectl get pod -n ingress-apisix -o name|grep controller|xargs kubectl delete -n ingress-apisix kubectl get cm -n ingress-apisix apisix-configmap -o yaml|sed -e 's/30s/6h/g'|kubectl apply -f - 七、通过 lua 实现限速 limit-rate https://www.daxuxu.info/blog/post/nginx-lua-jie-shao/ https://blog.donatas.net/blog/2017/07/25/limit-bandwidth-openresty/ https://developer.moduyun.com/article/c65f7549-169e-4544-b4d9-654bd9389bed.html https://nginx.org/en/docs/http/ngx_http_core_module.html#limit_rate https://github.com/apache/apisix/blob/master/example/apisix/plugins/3rd-party.lua local core = require(\"apisix.core\") local ngx = ngx local type = type local schema = { type = \"object\", properties = { limit_rate_after = {type =\"string\"}, limit_rate = {type =\"string\"} }, required = {\"limit_rate_after\",\"limit_rate\"}, } local plugin_name = \"limit_rate\" local _M={ version = 0.1, priority = 99, name = plugin_name, schema = schema } function _M.check_schema(conf) return core.schema.check(schema, conf) end function _M.access(conf,ctx) ngx.var.limit_rate_after = conf.limit_rate_after ngx.var.limit_rate = conf.limit_rate --return 203, conf.limit_rate_after end function _M.header_filter(ctx) core.response.add_header(\"X-Custom-Header\", \"hlyani\") end --function _M.body_filter(ctx) -- core.log.warn(\"hit body_filter phase\") --end function _M.log(conf, ctx) core.log.warn(\"limit_rate_after: \", conf.limit_rate_after, \", limit_rate: \", conf.limit_rate) end -- 注册插件 return _M kubectl cp limit-rate.lua -n ingress-apisix apisix-649cb68c96-4d8cr:/usr/local/apisix/apisix/plugins/ai.lua kubectl exec -it -n ingress-apisix apisix-649cb68c96-4d8cr apisix reload curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules -H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' -X PUT -d ' { \"id\":\"1\", \"plugins\":{ \"limit-rate\":{ \"limit_rate\": \"2k\", \"limit_rate_after\": \"500k\" } } }' curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/global_rules -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/plugins/list -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq curl -s http://$(kubectl get svc -n ingress-apisix apisix-admin -o jsonpath=\"{.spec.clusterIP}\"):9180/apisix/admin/plugins/list -H 'X-API-Key: edd1c9f034335f136f87ad84b625c8f1'|jq|grep ai kubectl edit cm -n ingress-apisix apisix http_server_location_configuration_snippet: | set $limit_rate 0; set $limit_rate_after 0; curl apisix.ingress.org:32060/headers -v curl -o /dev/null apisix.ingress.org:32060/image/jpeg apiVersion: apisix.apache.org/v2 kind: ApisixPluginConfig metadata: name: limit-rate spec: plugins: - name: limit-rate enable: true config: limit_rate: 20k limit_rate_after: 500k --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-httpbin-ingress annotations: k8s.apisix.apache.org/plugin-config-name: limit-rate spec: ingressClassName: nginx rules: - host: apisix.ingress.org http: paths: - path: / pathType: Prefix backend: service: name: test-httpbin-svc port: number: 80 apiVersion: apisix.apache.org/v2 kind: ApisixRoute metadata: name: test-limit-rate spec: http: - name: route match: hosts: - apisix.ingress.org paths: - / backends: - serviceName: test-httpbin-svc servicePort: 80 plugins: - name: limit-rate enable: true config: limit_rate: 20k limit_rate_after: 500k 修改 chart 包 vim apisix/templates/limit-rate.yaml {{- if .Values.apisix.customPlugins.enabled }} kind: ConfigMap apiVersion: v1 metadata: name: limit-rate data: limit-rate.lua: | local core = require(\"apisix.core\") local ngx = ngx local type = type local schema = { type = \"object\", properties = { limit_rate_after = {type =\"string\"}, limit_rate = {type =\"string\"} }, required = {\"limit_rate_after\",\"limit_rate\"}, } local plugin_name = \"limit_rate\" local _M={ version = 0.1, priority = 1004, name = plugin_name, schema = schema } function _M.check_schema(conf) return core.schema.check(schema, conf) end function _M.access(conf,ctx) ngx.var.limit_rate_after = conf.limit_rate_after ngx.var.limit_rate = conf.limit_rate end function _M.log(conf, ctx) core.log.warn(\"limit_rate_after: \", conf.limit_rate_after, \", limit_rate: \", conf.limit_rate) end return _M {{- end }} luaPath: 因为已经放在了默认插件目录，所以可以不需要配置 luaPath 的路径会默认添加 “/apisix/plugins” chart comfigmap 逻辑 如果 plugins 不为空，会自动加载 .Values.apisix.customPlugins.plugins，所以 apisix.plugins 不用添加新加插件 vim apisix/values.yaml apisix: plugins: - 略。。。 customPlugins: enabled: true #luaPath: \"/usr/local/apisix/?.lua\" # -> /usr/local/apisix/apisix/plugins/?.lua luaPath: \"/opt/custom_plugins/?.lua\" plugins: - name: \"limit-rate\" attrs: {} configMap name: \"limit-rate\" mounts: - key: \"limit-rate.lua\" path: \"/usr/local/apisix/apisix/plugins/limit-rate.lua\" 八、wrk 压力测试 https://github.com/wg/wrk/tree/master/scripts yum -y install gcc openssl-devel git git clone https://github.com/wg/wrk.git wrk cd wrk make cp wrk /usr/bin/ -t, --threads 线程 -c, --connections 连接数 -d, --duration 时间 --latency Print latency statistics wrk -t10 -c1000 -d1h --latency http://apisix.ingress.org:30962/index.html Avg 平均值 Stdev 标准方差 Max 最大值 Latency 延迟 Req/Sec 每秒请求数 Latency Distribution 延迟分布 Requests/sec 平均每秒处理完成请求个数 Transfer/sec 平均每秒读取数据量 Running 60m test @ http://apisix.ingress.org:30962/index.html 10 threads and 1000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 48.95ms 78.86ms 2.00s 90.06% Req/Sec 1.69k 334.68 3.68k 68.25% Latency Distribution 50% 23.36ms 75% 31.49ms 90% 126.79ms 99% 379.73ms 60512663 requests in 60.00m, 222.33GB read Socket errors: connect 0, read 2102, write 16, timeout 27480 Non-2xx or 3xx responses: 36 Requests/sec: 16808.64 Transfer/sec: 63.24MB https://www.nginx-cn.net/blog/testing-performance-nginx-ingress-controller-kubernetes/ 九、FAQ https://apisix.apache.org/zh/docs/apisix/FAQ/ 十、Nginx参数优化 https://gist.github.com/denji/8359866 https://www.cloudpanel.io/blog/nginx-performance/ https://nginx.org/en/docs/ events { use epoll; worker_connections 6000; } http { include mime.types; default_type application/octet-stream; access_log /usr/local/nginx/logs/access.log; error_log /usr/local/nginx/logs/error.log; proxy_connect_timeout 900; proxy_read_timeout 900; proxy_send_timeout 900; proxy_cache_path /var/cache/nginx keys_zone=a_cache:10m inactive=10m max_size=10g; gzip on; gzip_static on; gzip_buffers 40 4K; gzip_comp_level 7; gzip_min_length 1k; gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png application/vnd.ms-fontobject font/ttf font/opentype font/x-woff image/svg+xml; gzip_disable \"MSIE [1-6]\\.\"; gzip_vary on; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; client_max_body_size 500M; } location / { proxy_http_version 1.1; proxy_set_header Host $host:$server_port; proxy_set_header X-Referer $http_referer; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header Origin \"\"; client_body_buffer_size 128k; client_max_body_size 500M; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 4k; proxy_buffers 32 4k; proxy_busy_buffers_size 64k; } { \"client-max-body-size\": \"500m\", \"default-server-return\": \"https://192.168.0.127/error\", \"keepalive-requests\": \"3000\", \"keepalive-timeout\": \"65s\", \"location-snippets\": \"proxy_set_header Origin \\\"\\\";\\ngzip_static on;\\nproxy_set_header Accept-Encoding gzip;\\n\", \"proxy-connect-timeout\": \"900s\", \"proxy-read-timeout\": \"900s\", \"proxy_send_timeout\": \"900s\", \"worker-connections\": \"3000\" } nginx: workerRlimitNofile: \"204800\" workerConnections: \"204800\" workerProcesses: auto enableCPUAffinity: true keepaliveTimeout: 30 clientMaxBodySize: 500M logs: enableAccessLog: false configurationSnippet: httpStart: | access_log off; open_file_cache_valid 30s; open_file_cache_min_uses 2; open_file_cache_errors on; sendfile on; tcp_nopush on; tcp_nodelay on; gzip on; gzip_static on; gzip_min_length 10240; gzip_comp_level 5; gzip_vary on; gzip_disable msie6; #gzip_proxied expired no-cache no-store private auth; gzip_proxied any; gzip_types text/css text/javascript text/xml text/plain text/x-component application/javascript application/x-javascript application/json application/xml application/rss+xml application/atom+xml application/vnd.ms-fontobject font/truetype font/opentype image/svg+xml; reset_timedout_connection on; keepalive_requests 100000; client_body_buffer_size 128k; client_header_buffer_size 3m; httpSrvLocation: | proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; final config.yaml: |- # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # apisix: # universal configurations events: # Event distribution module configuration module: lua-resty-events # Sets the name of the events module used. # Supported module: lua-resty-worker-events and lua-resty-events node_listen: # APISIX listening port - 9080 enable_heartbeat: true enable_admin: true enable_admin_cors: true enable_debug: false extra_lua_path: /opts/custom_plugins/?.lua; enable_control: true control: ip: 127.0.0.1 port: 9090 enable_dev_mode: false # Sets nginx worker_processes to 1 if set to true enable_reuseport: true # Enable nginx SO_REUSEPORT switch if set to true. enable_ipv6: true # Enable nginx IPv6 resolver enable_http2: true enable_server_tokens: true # Whether the APISIX version number should be shown in Server header # proxy_protocol: # Proxy Protocol configuration # listen_http_port: 9181 # The port with proxy protocol for http, it differs from node_listen and admin_listen. # # This port can only receive http request with proxy protocol, but node_listen & admin_listen # # can only receive http request. If you enable proxy protocol, you must use this port to # # receive http request with proxy protocol # listen_https_port: 9182 # The port with proxy protocol for https # enable_tcp_pp: true # Enable the proxy protocol for tcp proxy, it works for stream_proxy.tcp option # enable_tcp_pp_to_upstream: true # Enables the proxy protocol to the upstream server proxy_cache: # Proxy Caching configuration cache_ttl: 10s # The default caching time if the upstream does not specify the cache time zones: # The parameters of a cache - name: disk_cache_one # The name of the cache, administrator can be specify # which cache to use by name in the admin api memory_size: 50m # The size of shared memory, it's used to store the cache index disk_size: 1G # The size of disk, it's used to store the cache data disk_path: \"/tmp/disk_cache_one\" # The path to store the cache data cache_levels: \"1:2\" # The hierarchy levels of a cache # - name: disk_cache_two # memory_size: 50m # disk_size: 1G # disk_path: \"/tmp/disk_cache_two\" # cache_levels: \"1:2\" router: http: radixtree_host_uri # radixtree_uri: match route by uri(base on radixtree) # radixtree_host_uri: match route by host + uri(base on radixtree) # radixtree_uri_with_parameter: match route by uri with parameters ssl: 'radixtree_sni' # radixtree_sni: match route by SNI(base on radixtree) proxy_mode: http stream_proxy: # TCP/UDP proxy tcp: # TCP proxy port list - 9100 udp: # UDP proxy port list - 9200 # dns_resolver: # # - 127.0.0.1 # # - 172.20.0.10 # # - 114.114.114.114 # # - 223.5.5.5 # # - 1.1.1.1 # # - 8.8.8.8 # dns_resolver_valid: 30 resolver_timeout: 5 ssl: enable: false listen: - port: 9443 enable_http3: false ssl_protocols: \"TLSv1.2 TLSv1.3\" ssl_ciphers: \"ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA256:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA\" nginx_config: # config for render the template to genarate nginx.conf error_log: \"/dev/stderr\" error_log_level: \"warn\" # warn,error worker_processes: \"10\" enable_cpu_affinity: true worker_rlimit_nofile: 8192 # the number of files a worker process can open, should be larger than worker_connections worker_shutdown_timeout: 240s max_pending_timers: 16384 max_running_timers: 4096 event: worker_connections: 1024 http: lua_shared_dict: # Nginx Lua shared memory zone. Size units are m or k. lrucache-lock: 100m prometheus-metrics: 200m worker-events: 100m enable_access_log: true access_log: \"/dev/stdout\" access_log_format: '$remote_addr - $remote_user [$time_local] $http_host \\\"$request\\\" $status $body_bytes_sent $request_time \\\"$http_referer\\\" \\\"$http_user_agent\\\" $upstream_addr $upstream_status $upstream_response_time \\\"$upstream_scheme://$upstream_host$upstream_uri\\\"' access_log_format_escape: default keepalive_timeout: \"60s\" client_max_body_size: 500M client_header_timeout: 60s # timeout for reading client request header, then 408 (Request Time-out) error is returned to the client client_body_timeout: 60s # timeout for reading client request body, then 408 (Request Time-out) error is returned to the client send_timeout: 30s # timeout for transmitting a response to the client.then the connection is closed underscores_in_headers: \"on\" # default enables the use of underscores in client request header fields real_ip_header: \"X-Real-IP\" # http://nginx.org/en/docs/http/ngx_http_realip_module.html#real_ip_header real_ip_from: # http://nginx.org/en/docs/http/ngx_http_realip_module.html#set_real_ip_from - 127.0.0.1 - 'unix:' upstream: keepalive: 320 keepalive_requests: 1000 keepalive_timeout: 60s http_configuration_snippet: | access_log on; proxy_connect_timeout 900; proxy_read_timeout 900; proxy_send_timeout 900; proxy_buffers 16 32k; proxy_buffer_size 64k; open_file_cache_valid 30s; open_file_cache_min_uses 2; open_file_cache_errors on; sendfile on; tcp_nopush on; tcp_nodelay on; gzip on; gzip_static on; gzip_min_length 1k; gzip_buffers 16 8k; gzip_comp_level 7; gzip_vary on; gzip_disable msie6; gzip_proxied any; gzip_types text/css text/javascript text/xml text/plain text/x-component application/javascript pplication/x-javascript application/json application/xml application/rss+xml application/atom+xml application/vnd.ms-fontobject font/truetype font/opentype image/svg+xml; reset_timedout_connection on; keepalive_requests 1000; client_body_buffer_size 128k; client_header_buffer_size 3m; http_server_location_configuration_snippet: | proxy_connect_timeout 900; proxy_read_timeout 900; proxy_send_timeout 900; proxy_buffers 16 32k; proxy_buffer_size 64k; set $limit_rate 0; set $limit_rate_after 0; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_intercept_errors on; error_page 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 421 422 423 424 425 426 428 429 431 451 500 501 502 503 504 505 506 507 508 510 511 = https://10.15.200.50:58043/notebook/error; plugins: # plugin list - cors - proxy-rewrite - limit-rate - limit-conn - limit-count - limit-req - gzip - redirect - response-rewrite - prometheus - limit-rate plugin_attr: prometheus: export_addr: ip: 0.0.0.0 port: 9091 export_uri: /apisix/prometheus/metrics metric_prefix: apisix_ deployment: role: traditional role_traditional: config_provider: etcd admin: allow_admin: # http://nginx.org/en/docs/http/ngx_http_access_module.html#allow - 127.0.0.1/24 - 0.0.0.0/0 # - \"::/64\" admin_listen: ip: 0.0.0.0 port: 9180 # Default token when use API to call for Admin API. # *NOTE*: Highly recommended to modify this value to protect APISIX's Admin API. # Disabling this configuration item means that the Admin API does not # require any authentication. admin_key: # admin: can everything for configuration data - name: \"admin\" key: edd1c9f034335f136f87ad84b625c8f1 role: admin # viewer: only can view configuration data - name: \"viewer\" key: 4054f7cf07e344346cd3f287985e76a2 role: viewer etcd: host: # it's possible to define multiple etcd hosts addresses of the same etcd cluster. - \"http://192.168.0.127:2379\" # multiple etcd address timeout: 30 watch_timeout: 50 resync_delay: 5 health_check_timeout: 10 startup_retry: 2 prefix: \"/apisix\" # configuration prefix in etcd timeout: 30 # 30 seconds 十一、default error return httpSrv: | location = /default-server-return { proxy_pass https://192.168.0.127/error; } location /error-forward/ { proxy_pass https://10.15.200.50:58043; } httpSrvLocation: | proxy_intercept_errors on; error_page 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 421 422 423 424 425 426 428 429 431 451 500 501 502 503 504 505 506 507 508 510 511 /default-server-return; https://github.com/apache/apisix/issues/7987 configurationSnippet: # Based on # - https://blog.adriaan.io/one-nginx-error-page-to-rule-them-all.html # - https://gist.github.com/lextoumbourou/d6221deb818da4f342ea httpStart: | more_clear_headers Server; map $status $status_text { 400 'Bad Request'; 401 'Unauthorized'; 402 'Payment Required'; 403 'Forbidden'; 404 'Not Found'; 405 'Method Not Allowed'; 406 'Not Acceptable'; 407 'Proxy Authentication Required'; 408 'Request Timeout'; 409 'Conflict'; 410 'Gone'; 411 'Length Required'; 412 'Precondition Failed'; 413 'Payload Too Large'; 414 'URI Too Long'; 415 'Unsupported Media Type'; 416 'Range Not Satisfiable'; 417 'Expectation Failed'; 418 'I\\'m a teapot'; 421 'Misdirected Request'; 422 'Unprocessable Entity'; 423 'Locked'; 424 'Failed Dependency'; 425 'Too Early'; 426 'Upgrade Required'; 428 'Precondition Required'; 429 'Too Many Requests'; 431 'Request Header Fields Too Large'; 451 'Unavailable For Legal Reasons'; 500 'Internal Server Error'; 501 'Not Implemented'; 502 'Bad Gateway'; 503 'Service Unavailable'; 504 'Gateway Timeout'; 505 'HTTP Version Not Supported'; 506 'Variant Also Negotiates'; 507 'Insufficient Storage'; 508 'Loop Detected'; 510 'Not Extended'; 511 'Network Authentication Required'; default 'Something is wrong'; } map $http_accept $extension { default html; ~*application/json json; } httpSrv: | error_page 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 421 422 423 424 425 426 428 429 431 451 500 501 502 503 504 505 506 507 508 510 511 @error_$extension; location @error_json { types { } default_type \"application/json; charset=utf-8\"; echo '{\"error_msg\": \"$status_text\"}'; } location @error_html { types { } default_type \"text/html; charset=utf-8\"; echo '$status $status_text$status $status_text'; } httpSrvLocation: | proxy_intercept_errors on; error_page 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 421 422 423 424 425 426 428 429 431 451 500 501 502 503 504 505 506 507 508 510 511 = https://192.168.0.127/error; "},"notes/docker/kubebuilder.html":{"url":"notes/docker/kubebuilder.html","title":"kubebuilder","keywords":"","body":"kubebuilder 一、环境准备 1、容器环境 docker run -itd --name go_dev -w /go/src -v /go_src:/go/src/ golang:1.17.6 bash docker exec -it go_dev bash 2、安装kubebuilder curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) chmod +x kubebuilder && mv kubebuilder /usr/local/bin/ 3、设置 goproxy go env -w GO111MODULE=on && go env -w GOPROXY=https://goproxy.cn,direct 4、编译环境（可选） FROM golang:1.17.6 ARG http_proxy=http://192.168.0.1:1080 ENV http_proxy=$http_proxy ENV https_proxy=$http_proxy WORKDIR /go/src RUN curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH) && \\ chmod +x kubebuilder && mv kubebuilder /usr/local/bin/ && \\ go env -w GO111MODULE=on && go env -w GOPROXY=https://goproxy.cn,direct docker build -t crd_dev_golang1.17.6:1.0.0 . ## 开发环境 docker run -itd --name crd_dev -w /go/src -v /crd_rtc:/go/src/ 192.168.0.90:3000/amd64/crd_dev_golang1.17.6:1.0.0 docker exec -it crd_dev bash ## 运行 make manifests make install make run ## 创建实例 kubectl apply -f rt.yaml 二、开发 1、初始化项目 go mod init rtcontainer kubebuilder init --domain rtcontainer.io kubebuilder create api --group rt --version v1 --kind RTContainer 2、添加类型 api/v1/rtcontainer_types.go type RootAgent struct { IP string `json:\"ip,omitempty\"` Port string `json:\"port,omitempty\"` NFS string `json:\"nfs,omitempty\"` } type RTContainerSpec struct { RootAgent []RootAgent `json:\"rootagent,omitempty\"` Action string `json:\"action,omitempty\"` Image string `json:\"image,omitempty\"` Name string `json:\"name,omitempty\"` Replicas *int32 `json:\"replicas,omitempty\"` } 3、编写controller delete ？ // +kubebuilder:rbac:groups=crd.crd.RTContainer.io,resources=RTContainers,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=crd.crd.RTContainer.io,resources=RTContainers/status,verbs=get;update;patch // +kubebuilder:rbac:groups=crd.crd.RTContainer.io,resources=RTContainers/finalizers,verbs=update add appsv1 \"k8s.io/api/apps/v1\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" rtv1 \"rtcontainer/api/v1\" func (r *RTContainerReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = log.FromContext(ctx) // TODO(user): your logic here RTContainer := &rtv1.RTContainer{} if err := r.Client.Get(ctx, req.NamespacedName, RTContainer); err != nil { if errors.IsNotFound(err) { log.Log.Info(\"RTContainer resource not found, skipping reconcile\") return ctrl.Result{}, nil } return ctrl.Result{}, nil } //资源标记为删除 if RTContainer.DeletionTimestamp != nil { log.Log.Info(\"RTContainer in deleting\", \"name\", req.String()) return ctrl.Result{}, nil } if err := r.CreateDeployRTContainer(ctx, RTContainer); err != nil { log.Log.Error(err, \"failed to Create rtcontainer\", \"name\", req.String()) return ctrl.Result{}, nil } return ctrl.Result{}, nil } func (r *RTContainerReconciler) CreateDeployRTContainer(ctx context.Context, obj *rtv1.RTContainer) error { logger := log.FromContext(ctx) RTContainer := obj.DeepCopy() name := types.NamespacedName{ Namespace: RTContainer.Namespace, Name: RTContainer.Name, } // 构造owner owner := []metav1.OwnerReference{ { APIVersion: RTContainer.APIVersion, Kind: RTContainer.Kind, Name: RTContainer.Name, Controller: pointer.BoolPtr(true), BlockOwnerDeletion: pointer.BoolPtr(true), UID: RTContainer.UID, }, } labels := map[string]string{\"app\": obj.Name} selector := &metav1.LabelSelector{MatchLabels: labels} meta := metav1.ObjectMeta{ Name: RTContainer.Name, Namespace: RTContainer.Namespace, Labels: labels, OwnerReferences: owner, } // 获取对应deployment, 如不存在则创建 deploy := &appsv1.Deployment{} if err := r.Get(ctx, name, deploy); err != nil { if !errors.IsNotFound(err) { return err } deploy = &appsv1.Deployment{ ObjectMeta: meta, Spec: appsv1.DeploymentSpec{ Replicas: RTContainer.Spec.Replicas, Selector: selector, Template: corev1.PodTemplateSpec{ ObjectMeta: metav1.ObjectMeta{ Labels: labels, }, Spec: corev1.PodSpec{ Containers: newContainers(RTContainer), }, }, }, } if err := r.Create(ctx, deploy); err != nil { return err } logger.Info(\"Create deployment successfully.\", \"name\", name.String()) } return nil } func newContainers(app *rtv1.RTContainer) []corev1.Container { return []corev1.Container{ { Name: app.Name, Image: app.Spec.Image, ImagePullPolicy: corev1.PullIfNotPresent, }, } } 4、编译CRD，更新字段 make manifests cat config/crd/bases/rt.rtcontainer.io_rtcontainers.yaml 5、安装kustomize curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash mv kustomize bin/ 6、安装、运行 make install make run 7、运行实例 apiVersion: rt.rtcontainer.io/v1 kind: RTContainer metadata: name: rt1 spec: rootagent: - ip: \"192.168.2.2\" port: \"8888\" nfs: \"/mount/fatfs_C/nfs_root\" action: \"Create\" image: vm1:1.0 replicas: 1 "},"notes/docker/containerd.html":{"url":"notes/docker/containerd.html","title":"Containerd","keywords":"","body":"Containerd 一、Containerd TODO 二、ctr 1、拉取 ctr i pull docker.io/library/ubuntu:latest ctr -n k8s.io i pull --plain-http=true 192.168.0.31:30002/library/ubuntu_armv8_edge:1.1 https_proxy=http://192.168.0.169:1080 http_proxy=http://192.168.0.169:1080 ctr -n k8s.io i pull k8s.gcr.io/pause:3.2 ctr --plain-text XXX 2、查看 ctr c ls ctr -n k8s.io i ls -q 3、运行 ctr run -t docker.io/library/ubuntu:latest test bash ctr run -d docker.io/library/ubuntu:latest test bash 4、删除 ctr c rm test 5、导入导出镜像 bzip2 -cd aaa_v1.0.0_image_amd64.tar.bz2 | k3s ctr -n k8s.io i import - docker save aaa:v1.0.0 | bzip2 > aaa_v1.0.0_image_amd64.tar.bz2 ctr containers export /path/to/exported-fs.tar ctr -n default i export /dev/stdout docker.io/library/myimage:latest |ctr -n k8s.io i import - 6、推送镜像 ctr -n k8s.io i push -k 192.168.0.127:5000/test:1.0.0 三、crictl 会默认使用containerd的配置文件 /etc/containerd/config.toml crictl -D pull 127.0.0.1:5000/test:1.0.0 四、仓库信任配置 vim /etc/containerd/config.toml ... [plugins.\"io.containerd.grpc.v1.cri\".registry] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"easzlab.io.local:5000\".tls] insecure_skip_verify = true [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"easzlab.io.local:5000\"] endpoint = [\"http://easzlab.io.local:5000\"] ... systemctl restart containerd 其他 1、解压OCI镜像 ctr i export - docker.io/library/vm1:1.0.0 | tar xvf - find blobs/sha256/* -size +4k |xargs -n 1 tar -xvf "},"notes/docker/buildah.html":{"url":"notes/docker/buildah.html","title":"Buildah","keywords":"","body":"Buildah 一、编译部署 1、拉取代码 git clone https://gitlabwh.uniontech.com/wuhan/container/buildah 2、安装依赖 apt install -y pkg-config libgpgme-dev libseccomp-dev libdevmapper-dev go get github.com/sbinet/go-python 3、构建 cd buildah git checkout v1.18.0 make 4、复制配置文件 mkdir /etc/containers/ cp docs/samples/registries.conf /etc/containers/ cp tests/policy.json /etc/containers/ cp ./vendor/github.com/containers/storage/storage.conf /etc/containers/ 5、部署网络插件 git clone https://ghproxy.com/https://github.com/containernetworking/plugins cd ./plugins; ./build_linux.sh mkdir -p /opt/cni/bin install -v ./bin/* /opt/cni/bin 6、安装buildah make install 或 cp bin/buildah /usr/local/bin/buildah chmod 755 /usr/local/bin/buildah 二、使用 1、根据Dockerfile构建镜像 buildah bud -t test 2、推送镜像 buildah push --tls-verify=false --creds admin:123 192.168.0.127:3000/test/test:latest 3、挂载已有镜像并修改、重新生成镜像 buildah from localhost/test buildah mount $mycontainer buildah commit $mycontainer containers-storage:myecho2 4、查看容器 1）在构建容器镜像是的零时容器 2）mount后的容器 buildah containers 5、修改镜像 buildah copy myecho-working-container-2 newecho /usr/local/bin buildah config --entrypoint \"/bin/sh -c /usr/local/bin/newecho\" myecho-working-container-2 buildah run myecho-working-container-2 -- sh -c '/usr/local/bin/newecho' buildah commit myecho-working-container-2 containers-storage:mynewecho 6、基于scratch构建 buildah from scratch 三、其他 install -D -m0755 bin/buildah $(DESTDIR)/$(BINDIR)/buildah $(MAKE) -C docs install #install -D -m0755 bin/buildah //usr/local/bin/buildah #make -C docs install "},"notes/docker/buildkit.html":{"url":"notes/docker/buildkit.html","title":"Buildkit","keywords":"","body":"buildkit & nerdctl & here-document 一、部署 docker buildx/buildkit wget https://github.com/docker/buildx/releases/download/v0.25.0/buildx-v0.25.0.linux-amd64 mkdir -p ~/.docker/cli-plugins cp buildx-v0.25.0.linux-amd64 ~/.docker/cli-plugins/docker-buildx chmod +x ~/.docker/cli-plugins/docker-buildx 1、下载 curl -LO https://github.com/moby/buildkit/releases/download/v0.11.6/buildkit-v0.11.6.linux-amd64.tar.gz 2、安装 tar -zxvf buildkit-v0.11.6.linux-amd64.tar.gz cp bin/* /usr/bin/ 3、启动 a、用参数启动 buildkitd & buildkitd --oci-worker=false --containerd-worker=true & 使用 --oci-worker=false --containerd-worker=true 参数,可以让buildkitd服务使用containerd后端 b、使用配置文件启动 buildkitd.toml mkdir /etc/buildkit/ cat > /etc/buildkit/buildkitd.toml 配置使用containerd后端，禁用oic后端 配置命名空间default 配置平台amd64 配置垃圾回收空间限制 buildkitd --config /etc/buildkit/buildkitd.toml & c、用 systemd 启动 cat > /usr/lib/systemd/system/buildkitd.service cat > /usr/lib/systemd/system/buildkit.socket systemctl daemon-reload && systemctl restart buildkitd && systemctl enable buildkitd 二、使用 1、基本构建 buildctl build --frontend=dockerfile.v0 --local context=. --local dockerfile=. --output type=image,name=tmp:1.0.0 buildctl build --frontend=dockerfile.v0 --local context=. --local dockerfile=. --output type=image,name=tmp:1.0.0 frontend：使用dockerfile作为前端，也可以使用gateway.v0（未测试）。 local context： 指向当前目录，这是Dockerfile执行构建时的路径上下文,比如在从目录中拷贝文件到镜像里 local dockerfile：指向当前目录，表示Dockerfile在此目录 output 的 name： 表示构建的镜像名称 2、构建并推送 buildctl build --frontend dockerfile.v0 --opt target=foo --opt build-arg:foo=bar --local context=. --local dockerfile=. --output type=image,name=docker.io/username/image,push=true 3、导出到本地目录 buildctl build --frontend=dockerfile.v0 --local context=. --local dockerfile=. -o type=local,dest=./a 4、导出到本地tar包 buildctl build --frontend=dockerfile.v0 --local context=. --local dockerfile=. -o type=docker,dest=./a.tar 5、导出到docker buildctl build --frontend=dockerfile.v0 --local context=. --local dockerfile=. -o type=docker,name=tmp:1.0.0 | docker load 6、导出为oci tar 包 buildctl build --frontend=dockerfile.v0 --local context=. --local dockerfile=. -o type=oci,name=./a.tar 7、构建多平台镜像 buildctl build --frontend=dockerfile.v0 --opt platform=linux/amd64,linux/arm64 --local context=. --local dockerfile=. --output type=image,name=tmp:1.0.0 8、查看缓存 buildctl du -v 9、清理缓存 buildctl prune 三、结合nerdctl 1、下载 curl -LO https://github.com/containerd/nerdctl/releases/download/v1.5.0/nerdctl-1.5.0-linux-amd64.tar.gz tar -zxvf nerdctl-1.5.0-linux-amd64.tar.gz 2、自动补全 echo \"source /etc/profile source /etc/profile 3、配置文件 mkdir /etc/nerdctl cat > /etc/nerdctl/nerdctl.toml 4、基础使用 nerdctl ns ls nerdctl image ls nerdctl pull nginx:alpine nerdctl system info nerdctl system prune -h nerdctl build -t hl:1.0.0 . nerdctl login -u admin -p 123456 --insecure-registry 192.168.0.127:5000 nerdctl -n k8s.io push --insecure-registry 192.168.0.127:5000/tmp/test:1.0.0 nerdctl -n k8s.io --address /run/containerd/containerd.sock commit 008dcf8b52b3 192.168.0.127:5000/test:1.0.0 nerdctl -n k8s.io inspect --format '{{.State.Status}}' ID nerdctl tag 192.168.0.127:5000/test:1.0.0 192.168.0.127:5000/test:2.0.0 docker save 192.168.0.127:5000/test:1.0.0 | nerdctl load nerdctl images --format \"{{.Repository}}:{{.Tag}}\" nerdctl inspect --format '{{.RepoTags}}' image_name nerdctl inspect --format '{{.RepoTags}}' ubuntu 四、here-document here-documents # syntax=docker/dockerfile:1 FROM debian RUN /hl/yani EOT # syntax=docker/dockerfile:1 FROM debian RUN cat > demo.txt 123 > asdb > EOF # syntax=docker/dockerfile:1 FROM alpine COPY 五、构建 1、拉取代码 git clone https://github.com/moby/buildkit.git 2、安装 buildx 插件(buildkit) wget https://github.com/docker/buildx/releases/download/v0.23.0/buildx-v0.23.0.linux-amd64 chmod a+x buildx-v0.23.0.linux-amd64 mkdir -p ~/.docker/cli-plugins mv buildx-v0.23.0.linux-amd64 ~/.docker/cli-plugins/docker-buildx 3、编译 make 4、多架构构建 cat > /etc/buildkitd.toml docker buildx ls docker buildx create \\ --driver docker-container \\ --platform linux/arm64,linux/amd64 \\ --use \\ --bootstrap \\ --config /etc/buildkitd.toml \\ --name multi docker buildx ls NAME/NODE DRIVER/ENDPOINT STATUS BUILDKIT PLATFORMS multi * docker-container multi0 unix:///var/run/docker.sock running v0.12.4 linux/arm64*, linux/amd64*, linux/amd64/v2, linux/amd64/v3, linux/amd64/v4, linux/386 default docker default default running v0.11.6+0a15675913b7 linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/amd64/v4, linux/386 切换 docker buildx use 启动 docker buildx inspect --bootstrap multi 删除 docker buildx rm multi 构建 docker buildx build --platform linux/arm64,linux/amd64 --build-arg=\"REPO\" -f Dockerfile.local --output=./dist . 使用配置 Makefile .PHONY: cross cross: $(BUILDX_CMD) bake binaries-cross docker-bake.hcl target \"binaries-cross\" { inherits = [\"binaries\"] output = [bindir(\"cross\")] platforms = [ \"linux/amd64\", \"linux/arm64\" ] } "},"notes/docker/docker.html":{"url":"notes/docker/docker.html","title":"docker 相关","keywords":"","body":"docker 相关 一、常用 1、在线安装docker aliyun 安装docker-ce tsinghua 安装docker-ce curl -sSL https://get.docker.io | bash curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun docker --version docker info install buildx mkdir -p ~/.docker/cli-plugins curl -sL https://github.com/docker/buildx/releases/download/v0.21.2/buildx-v0.21.2.linux-amd64 -o ~/.docker/cli-plugins/docker-buildx chmod +x ~/.docker/cli-plugins/docker-buildx docker buildx version 2、二进制安装 wget https://download.docker.com/linux/static/stable/aarch64/docker-20.10.8.tgz #wget https://download.docker.com/linux/static/stable/x86_64/docker-20.10.8.tgz tar -zxvf docker-20.10.8.tgz cp docker/* /usr/bin/ dockerd & cat /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target docker.socket Wants=network-online.target Requires=docker.socket [Service] Type=notify # the default is not to use systemd for cgroups because the delegate issues still # exists and systemd currently does not support the cgroup feature set required # for containers run by docker ExecStart=/usr/bin/dockerd -H fd:// ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=1048576 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity # Uncomment TasksMax if your systemd version supports it. # Only systemd 226 and above support this version. #TasksMax=infinity TimeoutStartSec=0 # set delegate yes so that systemd does not reset the cgroups of docker containers Delegate=yes # kill only the docker process, not all processes in the cgroup KillMode=process # restart the docker process if it exits prematurely Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target cat /etc/systemd/system/docker.socket [Unit] Description=Docker Socket for the API PartOf=docker.service [Socket] ListenStream=/var/run/docker.sock SocketMode=0660 SocketUser=root SocketGroup=docker [Install] WantedBy=sockets.target 3、导入、导出镜像 docker save registry:latest > registry.tar.gz docker save -o registry.tar.gz registry:latest docker load docker export | docker import - [:标签] docker-squash https://github.com/jwilder/docker-squash docker save | sudo docker-squash -t newtag | docker load 4、启动容器 docker run -itd --name mariadb --restart=always -v /opt/mysql:/etc/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=qwe mariadb docker run --name mariadb -e MYSQL_ROOT_PASSWORD=123456 -p 3306:3306 -v /tmp/my.cnf:/etc/mysql/my.cnf -d mariadb 5、使用dockerpy >>> import docker >>> client = docker.DockerClient(base_url='unix://var/run/docker.sock') 6、docker update docker update --restart=always wiki docker update --cpu-shares 512 -m 300M abebf7571666 hopeful_morse docker update --kernel-memory 80M test 7、查看容器ip地址、id docker inspect -f '\\{\\{.NetworkSettings.IPAddress\\}\\}' wiki docker inspect -f '\\{\\{.Id\\}\\}' registry docker inspect --format '\\{\\{.Id\\}\\}' registry 8、查看全部容器id、占用空间 docker ps -qa docker ps -as 9、保存镜像 docker commit docker commit -a \"user\" -m \"commit info\" [CONTAINER] [imageName]:[imageTag] docker login --username=[userName] --password=[pwd] [registryURL] docker tag [imageID] [remoteURL]:[imageTag] docker push [remoteURL]:[imageTag] docker pull [remoteURL]:[imageTag] docker diff docker commit --change='CMD [\"gunicorn\", \"-b\", \"0.0.0.0:80\", \"httpbin:app\", \"-k\", \"gevent\"]' c24d49252bdc httpbin:test 10、--restart no – 默认值，如果容器挂掉不自动重启 on-failure – 当容器以非 0 码退出时重启容器,同时可接受一个可选的最大重启次数参数 (e.g. on-failure:10) always – 不管退出码是多少都要重启 11、资源限制 # 限制内存最大使用 -m 1024m --memory-swap=1024m # 限制容器使用CPU --cpuset-cpus=\"0,1\" 12、一个容器连接到另一个容器 docker run -i -t --name sonar -d -link mmysql:db tpires/sonar-server sonar 13、构建自己的镜像 docker build -t docker build -t xx/gitlab . 14、查看容器端口 docker port registry 15、查看容器进程 docker top registry 16、监控容器资源使用情况 docker stats docker stats --no-stream 17、批量删除名字包含\"none\"的镜像 docker rmi $(docker images | grep \"none\" | awk '{print $3}') 18、查看可用命令 docker help 19、login docker login --username=yourhubusername --email=youremail@company.com 20、删除已安装docker yum list installed | grep docker yum remove -y docker.x86_64 yum remove -y docker-client.x86_64 yum remove -y docker-common.x86_64 21、配置国内docker源 vim /etc/docker/daemon.json {\"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\"] } systemctl restart docker 22、使用 --volumes-from 备份 docker run --rm --volumes-from gitlab -v /backup1:/backup2 ubuntu tar cvf /backup2/gitlab-etc.tar /etc/gitlab 23、清理 $ cat /usr/bin/prune_docker.sh #!/bin/bash docker container prune -f # 删除所有退出状态的容器 docker volume prune -f # 删除未被使用的数据卷 docker image prune -f # 删除 dangling 或所有未被使用的镜像 $ crontab -l 0 0 * * * /usr/bin/prune_docker.sh >> /var/log/prune_docker.log 2>&1 24、docker 代理 cat /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\"HTTP_PROXY=http://proxy.server:port\" Environment=\"HTTPS_PROXY=http://proxy.server:port\" Environment=\"NO_PROXY=localhost,127.0.0.1\" systemctl daemon-reload systemctl restart docker 25、使用本地仓库 vim /etc/docker/daemon.json { \"insecure-registries\": [\"192.168.0.11:30002\"] } systemctl daemon-reload systemctl restart docker 26、azk8s.cn 支持镜像转换列表 global proxy in China format example dockerhub (docker.io) dockerhub.azk8s.cn dockerhub.azk8s.cn//: dockerhub.azk8s.cn/microsoft/azure-cli:2.0.61 dockerhub.azk8s.cn/library/nginx:1.15 gcr.io gcr.azk8s.cn gcr.azk8s.cn//: gcr.azk8s.cn/google_containers/hyperkube-amd64:v1.13.5 quay.io quay.azk8s.cn quay.azk8s.cn//: quay.azk8s.cn/deis/go-dev:v1.10.0 27、docker in docker docker run -itd --privileged=true -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/bin/docker --name centos centos 28、不裁剪输出 docker ps -a --no-trunc 29、创建docker网络并使用 备注：subnet指定一个网段， -o选项可以解决使用ifconfig命令看不到自己创建的网桥名字的问题 docker network create docker01 --subnet=10.10.10.0/24 -o com.docker.network.bridge.name=docker01 docker run -itd --net docker01 --ip 10.10.10.51 镜像名 30、多阶段构建 FROM gcc AS mybuildstage COPY hello.c . RUN gcc -o hello hello.c FROM ubuntu COPY --from=mybuildstage hello . CMD [\"./hello\"] 31、修改 cgroupdriver vim /etc/systemd/system/docker.service.d/docker-options.conf ... --exec-opt native.cgroupdriver=systemd ... --exec-opt native.cgroupdriver=cgroupfs ... 32、build 不交互，ubuntu、debian export DEBIAN_FRONTEND=noninteractive 33、以非root用户运行docker sudo groupadd docker sudo usermod –aG docker $USER 32、qt in docker xhost + > /dev/null 2>&1 #!/bin/bash xhost + > /dev/null 2>&1 create_data_volume(){ for i in $@; do name=$(echo $i|cut -d: -f1) version=$(echo $i|cut -d: -f2) if [ -z \"$(docker ps -a -f NAME=$name |grep -v CONTAINER)\" ]; then echo \"Creating $name...\" docker create --name $name $name:$version else echo \"$name is existed, skipped.\" fi done } create_data_volume poky:1.0.0 if [ ! -z \"$(docker ps -a -f NAME=qt5.6.3 |grep -v CONTAINER)\" ]; then echo \"Starting qt5.6.3...\" docker restart -t 1 qt5.6.3 else echo \"Runing qt5.6.3...\" docker run -itd --net=host \\ -v $HOME/.Xauthority:$HOME/.Xauthority \\ -v /tmp/.X11-unix:/tmp/.X11-unix \\ -e DISPLAY=$DISPLAY \\ --volumes-from poky:ro \\ -v /opt/qtworkspace:/opt/workspace \\ --name qt5.6.3 qt5.6.3:1.0.0 fi 33、最小化安装应用 apt install --no-install-recommends --no-install-suggests rm -rf /var/lib/apt/lists/* apk add --no-cache rm -rf /var/cache/apk/* pip install --no-cache-dir docker build --no-cache 34、使用cache构建镜像 docker build –cache-from mongo:3.2 -t mongo:3.2.1 . 35、docker远程访问 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H fd:// --containerd=/run/containerd/containerd.sock systemctl daemon-reload service docker restart curl http://localhost:2375/version tcp://192.168.0.127:2376 36、k8s http api server kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8080 37、docker 目录迁移 方法一 systemctl stop docker mv /var/lib/docker /data/ #cp -arv /var/lib/docker /data/ ln -s /data/docker /var/lib/docker systemctl start docker 方法二 vim /lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --graph=/data/docker/ vim /etc/docker/daemon.json { \"live-restore\": true, \"graph\": [ \"/data/docker/\" ] } vim /etc/docker/daemon.json { \"data-root\": \"/data/docker/\", } 38、设备空间不足 1、磁盘用完 docker info du -d1 -h /var/lib/docker/containers | sort -h cat /dev/null > /var/lib/docker/containers/container_id/container_log_name 2、容器默认大小限制 CentOS7 使用docker容器默认的创建大小为10GB vim /etc/docker/daemon.json { \"live-restore\": true, \"storage-opt\": [ \"dm.basesize=20G\" ] } systemctl stop docker rm -rf /var/lib/docker vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd and change it to: ExecStart=/usr/bin/dockerd --storage-opt dm.basesize=20G systemctl daemon-reload systemctl start docker 3、inode节点数满了 文件存储在磁盘上，磁盘的最小存储单位叫做扇区（Sector）。每个扇区存储 512字节（0.5KB），操作系统读取硬盘的时候，不会一个扇区的读取，效率太低，而是一次性读取多个扇区，即一次性读取一个块（block）。这种由多个扇区组成的块，是文件系统存取的最小单位。块的大小，最常见的是4KB，即连续八个sector组成一个块。文件数据存储在块中，还必须找个一个地方存储文件的元数据，比如文件的创建者、文件的创建日期 、文件的大小等等。这种存储元数据的区域就叫做索引节点（inode）。每个文件都有对应的inode，里面包含了文件名以外的所有文件信息。 inode也会消耗磁盘空间，所以磁盘格式化的时候，操作系统自动将磁盘分为两个区域。一个是数据区，存放文件数据；另一个是inode区（inode table）存放inode所包含的信息。每个inode节点的大小，一般是128字节或256字节。inode节点的总数在格式化时就给定，一般是每1KB或每2KB就设置一个inode节点。 # 报错信息 No space left on device # 查看系统的inode节点使用情况 df -i # 尝试重新挂载 mount -o remount -o noatime,nodiratime,inode64,nobarrier /dev/vda1 39、docker 容器文件损坏 容器文件损坏，经常导致容器无法操作。正常的docker命令已经无法操控这台容器，无法关闭、重启、删除。 # 操作容器遇到类似的错误 b'devicemapper: Error running deviceCreate (CreateSnapDeviceRaw) dm_task_run failed' systemctl stop docker 删除容器文件 rm -rf /var/lib/docker/containers 重新整理容器元数据 thin_check /var/lib/docker/devicemapper/devicemapper/metadata thin_check --clear-needs-check-flag /var/lib/docker/devicemapper/devicemapper/metadata systemctl start docker 40、docker 容器优雅的重启 不停止服务器上面的容器，重启dockerd服务。 从docker-ce1.12开始，可以配置live-restore参数，以便在守护进程变得不可用时容器保持运行。 vim /etc/docker/daemon.yaml { \"live-restore\": true } # 在守护进程停机期间保持容器存活 dockerd --live-restore # 只能使用reload重载 # 相当于发送SIGHUP信号量给dockerd守护进程 systemctl reload docker # 但是对应网络的设置需要restart才能生效 systemctl restart docker 41、docker 容器无法删除 Error response from daemon: Conflict, cannot remove the default name of the container # 删除容器文件 rm -rf /var/lib/docker/containers/f8e8c3...65720 # 重启服务 systemctl restart docker.service 42、docker 容器中文异常 # 临时解决 docker exec -it some-mysql env LANG=C.UTF-8 /bin/bash # 永久解决 docker run --name some-mysql \\ -e MYSQL_ROOT_PASSWORD=my-secret-pw \\ -d mysql:tag --character-set-server=utf8mb4 \\ --collation-server=utf8mb4_unicode_ci 43、docker容器总线错误 Bus error (core dumped) 原因是在 docker 运行的时候，shm 分区设置太小导致 share memory 不够。不设置 --shm-size 参数时，docker 给容器默认分配的 shm 大小为 64M，导致程序启动时不足。具体原因还是因为安装 pytorch 包导致了，多进程跑任务的时候，docker 容器分配的共享内存太小，导致 torch 要在 tmpfs 上面放模型数据用于子线程的 共享不足，就出现报错了。 # 问题原因 df -TH Filesystem Type Size Used Avail Use% Mounted on overlay overlay 2.0T 221G 1.4T 3% / tmpfs tmpfs 68M 0 68M 0% /dev shm tmpfs 68M 41k 68M 1% /dev/shm # 启动docker的时候加上--shm-size参数(单位为b,k,m或g) docker run -it --rm --shm-size=200m pytorch/pytorch:latest # 在docker-compose添加对应配置 shm_size: '2gb' 44、docker配置默认网段 cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://XXXX\"], \"default-address-pools\":[{\"base\":\"172.17.0.0/12\", \"size\":24}], \"experimental\": true, \"default-runtime\": \"nvidia\", \"live-restore\": true, \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } docker network inspect app | grep Subnet 45、docker定时任务 # Crontab定时任务 0 */6 * * * \\ docker exec -t sh -c \\ 'exec mysqldump --all-databases -uroot -ppassword ......' 46、docker删除镜像报错 # 查询依赖 - image_id表示镜像名称 docker image inspect --format='{{.RepoTags}} {{.Id}} {{.Parent}}' $(docker image ls -q --filter since=) # 根据TAG删除镜像 docker rmi -f c565xxxxc87f # 删除悬空镜像 docker rmi $(docker images --filter \"dangling=true\" -q --no-trunc) 47、scratch镜像 https://docs.docker.com/develop/develop-images/baseimages/#create-a-simple-parent-image-using-scratch FROM scratch ADD hello / CMD [\"/hello\"] apt-get install build-essential gcc -o hello -static hello.c musl-gcc -o hello -static hello.c 48、通过镜像提取dockerfile alias dfimage=\"docker run -v /var/run/docker.sock:/var/run/docker.sock --rm alpine/dfimage\" dfimage -sV=1.36 mariadb:10.4 49、distroless镜像 distroless rules_docker static 2.34MB base 20.3MB cc 22.6MB latest nonroot debug debug-nonroot docker pull gcr.io/distroless/base docker pull gcr.dockerproxy.com/distroless/static:nonroot docker pull gcr.dockerproxy.com/distroless/base docker pull gcr.dockerproxy.com/distroless/base-debian10 docker pull gcr.dockerproxy.com/distroless/base-debian11 docker pull gcr.dockerproxy.com/distroless/base-debian9 docker pull gcr.dockerproxy.com/distroless/cc docker pull gcr.dockerproxy.com/distroless/cc-debian10 docker pull gcr.dockerproxy.com/distroless/cc-debian11 docker pull gcr.dockerproxy.com/distroless/cc-debian9 docker pull gcr.dockerproxy.com/distroless/dotnet docker pull gcr.dockerproxy.com/distroless/dotnet-debian10 docker pull gcr.dockerproxy.com/distroless/dotnet-debian9 docker pull gcr.dockerproxy.com/distroless/java docker pull gcr.dockerproxy.com/distroless/java-debian10 docker pull gcr.dockerproxy.com/distroless/java-debian11 docker pull gcr.dockerproxy.com/distroless/java-debian9 docker pull gcr.dockerproxy.com/distroless/nodejs docker pull gcr.dockerproxy.com/distroless/nodejs-debian10 docker pull gcr.dockerproxy.com/distroless/nodejs-debian11 docker pull gcr.dockerproxy.com/distroless/nodejs-debian9 docker pull gcr.dockerproxy.com/distroless/python2.7 docker pull gcr.dockerproxy.com/distroless/python2.7-debian10 docker pull gcr.dockerproxy.com/distroless/python2.7-debian9 docker pull gcr.dockerproxy.com/distroless/python3 docker pull gcr.dockerproxy.com/distroless/python3-debian10 docker pull gcr.dockerproxy.com/distroless/python3-debian11 docker pull gcr.dockerproxy.com/distroless/python3-debian9 docker pull gcr.dockerproxy.com/distroless/static docker pull gcr.dockerproxy.com/distroless/static-debian10 docker pull gcr.dockerproxy.com/distroless/static-debian11 docker pull gcr.dockerproxy.com/distroless/static-debian9 50、清除未使用容器 docker system prune 51、docker/moby 官方工具 https://github.com/moby/moby/tree/master/contrib check-config.sh 检查内核是否支持容器相关功能 download-frozen-image-v2.sh 通过sh拉取镜像 52、拉取arm镜像 docker pull arm64v8/alpine:3.18 53、daemon.json 常用配置 { \"insecure-registries\": [\"192.168.0.10:3000\"], \"registry-mirrors\": [\"http://192.168.0.10:3000\",\"http://hub-mirror.c.163.com\",\"https://registry.docker-cn.com\",\"https://docker.mirrors.ustc.edu.cn\"] } 54、Dozzle 日志查看器 https://github.com/amir20/dozzle docker run --name dozzle -d --volume=/var/run/docker.sock:/var/run/docker.sock:ro -p 8888:8080 amir20/dozzle:latest 55、清理镜像 nerdctl image prune -f：删除悬挂的镜像，也就是没有tag的none镜像。 nerdctl image prune -a -f：删除所有未使用的镜像。 nerdctl rmi $(nerdctl images -f \"dangling=true\" -q)：删除悬挂的镜像。 docker system prune：删除悬挂的镜像、停止的容器、未使用的网络、以及构建缓存。 docker system prune -a：删除所有未使用的资源。 docker rmi $(docker images -f \"dangling=true\" -q) 56、debug kubectl debug -it aaaa --image=busybox:1.28 -n kube-system -- bash 57、secret kubectl get secret kubeconfig-admin -n kubesphere-system -o jsonpath='{.data.token}'|base64 --decode > token kubectl get secret kubeconfig-admin -n kubesphere-system -o jsonpath='{.data.ca\\.crt}'|base64 --decode > ca.crt kubectl --server=https://127.0.0.1:6443 --kubeconfig=./ca.crt get po kubectl config view --raw > kubeconfig.yaml cat kubeconfig.yaml | base64 kubectl config view --raw|base64 kubectl apply -f - 58、docker build proxy docker build --build-arg HTTP_PROXY=http://192.168.0.127:1080 --build-arg HTTPS_PROXY=http://192.168.0.127:1080 . 二、linux实现docker资源隔离 Linux 提供的主要的 NameSpace Mount NameSpace- 用于隔离文件系统的挂载点 UTS NameSpace- 用于隔离 HostName 和 DomianName IPC NameSpace- 用于隔离进程间通信 PID NameSpace- 用于隔离进程 ID Network NameSpace- 用于隔离网络 User NameSpace- 用于隔离用户和用户组 UID/GID 三、理解Docker容器和镜像 Image Definition 镜像（Image）就是一堆只读层（read-only layer）的统一视角，也许这个定义有些难以理解，下面的这张图能够帮助理解镜像的定义。 从左边看到了多个只读层，它们重叠在一起。除了最下面一层，其它层都会有一个指针指向下一层。这些层是Docker内部的实现细节，并且能够在主机（运行Docker的机器）的文件系统上访问到。统一文件系统（union file system）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角，这样就隐藏了多层的存在，在用户的角度看来，只存在一个文件系统。就可以在图片的右边看到这个视角的形式。可以在主机文件系统上找到有关这些层的文件。需要注意的是，在一个运行中的容器内部，这些层是不可见的。在我的主机上，我发现它们存在于/var/lib/docker/aufs目录下。 sudo tree -L 1 /var/lib/docker//var/lib/docker/├── aufs├── containers├── graph├── init├── linkgraph.db├── repositories-aufs├── tmp├── trust└── volumes7 directories, 2 files Container Definition 容器（container）的定义和镜像（image）几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 可能会发现，容器的定义并没有提及容器是否在运行，没错，这是故意的。要点：容器 = 镜像 + 读写层。并且容器的定义并没有提及是否要运行容器。 Running Container Definition 一个运行态容器（running container）被定义为一个可读写的统一文件系统加上隔离的进程空间和包含其中的进程。下面这张图片展示了一个运行中的容器。 正是文件系统隔离技术使得Docker成为了一个前途无量的技术。一个容器中的进程可能会对文件进行修改、删除、创建，这些改变都将作用于可读写层（read-write layer）。下面这张图展示了这个行为。 可以通过运行以下命令来验证上面所说的： docker run ubuntu touch happiness.txt 即便是这个ubuntu容器不再运行，依旧能够在主机的文件系统上找到这个新文件。 find / -name happiness.txt/var/lib/docker/aufs/diff/860a7b...889/happiness.txt Image Layer Definition 为了将零星的数据整合起来，提出了镜像层（image layer）这个概念。下面的这张图描述了一个镜像层，通过图片能够发现一个层并不仅仅包含文件系统的改变，它还能包含了其他重要信息。 元数据（metadata）就是关于这个层的额外信息，它不仅能够让Docker获取运行和构建时的信息，还包括父层的层次信息。需要注意，只读层和读写层都包含元数据。 除此之外，每一层都包括了一个指向父层的指针。如果一个层没有这个指针，说明它处于最底层。 Metadata Location 我发现在我自己的主机上，镜像层（image layer）的元数据被保存在名为”json”的文件中，比如说： /var/lib/docker/graph/e809f156dc985.../json e809f156dc985...就是这层的id 一个容器的元数据好像是被分成了很多文件，但或多或少能够在/var/lib/docker/containers/目录下找到，就是一个可读层的id。这个目录下的文件大多是运行时的数据，比如说网络，日志等等。 全局理解（Tying It All Together) 现在，让结合上面提到的实现细节来理解Docker的命令。 docker create docker create 命令为指定的镜像（image）添加了一个可读写层，构成了一个新的容器。注意，这个容器并没有运行。 docker start Docker start命令为容器文件系统创建了一个进程隔离空间。注意，每一个容器只能够有一个进程隔离空间。 docker run 看到这个命令，通常会有一个疑问：docker start 和 docker run命令有什么区别。 从图片可以看出，docker run 命令先是利用镜像创建了一个容器，然后运行这个容器。这个命令非常的方便，并且隐藏了两个命令的细节，但从另一方面来看，这容易让用户产生误解。 docker ps docker ps 命令会列出所有运行中的容器。这隐藏了非运行态容器的存在，如果想要找出这些容器，我们需要使用下面这个命令。 docker ps –a docker ps –a命令会列出所有的容器，不管是运行的，还是停止的。 docker images docker images命令会列出了所有顶层（top-level）镜像。实际上，在这里我们没有办法区分一个镜像和一个只读层，所以我们提出了top-level镜像。只有创建容器时使用的镜像或者是直接pull下来的镜像能被称为顶层（top-level）镜像，并且每一个顶层镜像下面都隐藏了多个镜像层。 docker images –a docker images –a命令列出了所有的镜像，也可以说是列出了所有的可读层。如果想要查看某一个image-id下的所有层，可以使用docker history来查看。 docker stop docker stop命令会向运行中的容器发送一个SIGTERM的信号，然后停止所有的进程。 docker kill docker kill 命令向所有运行在容器中的进程发送了一个不友好的SIGKILL信号。 docker pause docker stop和docker kill命令会发送UNIX的信号给运行中的进程，docker pause命令则不一样，它利用了cgroups的特性将运行中的进程空间暂停。具体的内部原理可以在这里找到：https://www.kernel.org/doc/Doc ... m.txt，但是这种方式的不足之处在于发送一个SIGTSTP信号对于进程来说不够简单易懂，以至于不能够让所有进程暂停。 docker rm docker rm命令会移除构成容器的可读写层。注意，这个命令只能对非运行态容器执行。 docker rmi docker rmi 命令会移除构成镜像的一个只读层。只能够使用docker rmi来移除最顶层（top level layer）（也可以说是镜像），也可以使用-f参数来强制删除中间的只读层。 docker commit docker commit命令将容器的可读写层转换为一个只读层，这样就把一个容器转换成了不可变的镜像。 docker build docker build命令非常有趣，它会反复的执行多个命令。 从上图可以看到，build命令根据Dockerfile文件中的FROM指令获取到镜像，然后重复地1）run（create和start）、2）修改、3）commit。在循环中的每一步都会生成一个新的层，因此许多新的层会被创建。 docker exec docker exec 命令会在运行中的容器执行一个新进程。 docker inspect or docker inspect命令会提取出容器或者镜像最顶层的元数据。 docker save docker save命令会创建一个镜像的压缩文件，这个文件能够在另外一个主机的Docker上使用。和export命令不同，这个命令为每一个层都保存了它们的元数据。这个命令只能对镜像生效。 docker export docker export命令创建一个tar文件，并且移除了元数据和不必要的层，将多个层整合成了一个层，只保存了当前统一视角看到的内容（译者注：expoxt后的容器再import到Docker中，通过docker images –tree命令只能看到一个镜像；而save后的镜像则不同，它能够看到这个镜像的历史镜像）。 docker history docker history命令递归地输出指定镜像的历史镜像。 四、编写dockerfile的最佳实践 1、创建短暂的容器 你定义的图像Dockerfile应该生成尽可能短暂的容器。通过“短暂”，我们的意思是容器可以被停止和销毁，然后重建并用绝对最小的设置和配置替换。 2、了解构建上下文 发出docker build命令时，当前工作目录称为构建上下文。默认情况下，假定Dockerfile位于此处，但您可以使用文件flag（-f）指定其他位置。无论Dockerfile实际存在的位置如何，当前目录中的所有文件和目录的递归内容都将作为构建上下文发送到Docker守护程序。 3、通过stdin管道Dockerfile Docker能够通过stdin与本地或远程构建上下文管道Dockerfile来构建映像。 通过stdin管道Dockerfile对于执行一次性构建非常有用，无需将Dockerfile写入磁盘，或者在生成Dockerfile的情况下，并且之后不应该持久化。 4、排除.dockerignore 要排除与构建无关的文件（不重构源存储库），请使用.dockerignore文件。此文件支持与.gitignore文件类似的排除模式。 5、使用多阶段构建 多阶段构建允许您大幅减小最终image的大小，而无需减少中间层和文件的数量。 由于image是在构建过程的最后阶段构建的，因此可以通过利用构建缓存来最小化image层。 6、避免安装不必要的包 为了降低复杂性，依赖性，文件大小和构建时间，请避免安装额外的或不必要的软件包，只是因为它们可能“很好”。 7、解耦应用程序 每个容器应该只关注一个问题。将应用程序分离到多个容器中可以更容易地水平扩展和重用容器。 8、最小化层数 只有说明RUN，COPY，ADD创建图层。其他指令创建临时中间图像，并不增加构建的大小。 在可能的情况下，使用多阶段构建，并仅将所需的工件复制到最终图像中。这允许您在中间构建阶段中包含工具和调试信息，而不会增加最终图像的大小。 9、对多行参数进行排序 只要有可能，通过按字母顺序排序多行参数来缓解以后的更改。这有助于避免重复包并使列表更容易更新。这也使PR更容易阅读和审查。 10、利用构建缓存 构建映像时，Docker会逐步Dockerfile执行您的指令， 按指定的顺序执行每个指令。在检查每条指令时，Docker会在其缓存中查找可以重用的现有image，而不是创建新的（重复）image。 五、在docker中运行windows dockur/windows dockerhub_dockurr/windows 1、提前准备windows ios 必须使用custom.iso mkdir /storage cp *.ios /storage/custom.iso 2、启动容器 docker run -itd \\ -p 8006:8006 \\ -p 3389:3389 \\ --device=/dev/kvm \\ --cap-add NET_ADMIN \\ --stop-timeout 120 \\ -e VERSION=\"custom.iso\" \\ -e RAM_SIZE=\"10G\" \\ -e CPU_CORES=\"10\" \\ -e DISK_SIZE=\"100G\" \\ -v /storage:/storage \\ --name win11 \\ dockurr/windows 3、通过浏览器安装 http://{IP}:8006 六、在docker中运行macos macOS Containers 是一个可以在 macOS 中运行 macOS 的 Docker 容器 https://meta.appinn.net/t/topic/47790 https://github.com/macOScontainers/homebrew-formula # Install packages brew install --cask macfuse brew install docker docker-buildx macOScontainers/formula/bindfs macOScontainers/formula/containerd macOScontainers/formula/dockerd macOScontainers/formula/rund # Start services sudo brew services start containerd sudo brew services start dockerd # Set up BuildKit mkdir -p ~/.docker/cli-plugins ln -sfn /opt/homebrew/opt/docker-buildx/bin/docker-buildx ~/.docker/cli-plugins/docker-buildx echo | docker login ghcr.io -u --password-stdin docker run --rm -it ghcr.io/macoscontainers/macos-jail/ventura:latest echo \"Hello from macOS container ^_^\" "},"notes/docker/multiarch.html":{"url":"notes/docker/multiarch.html","title":"容器交叉编译","keywords":"","body":"容器交叉编译 一、gcc-linaro gcc-linaro-7.5.0 1、dockerfile From ubuntu:20.04 ENV PATH=$PATH:/root/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu/bin ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu/lib WORKDIR /root RUN sed -i s@/archive.ubuntu.com/@/mirrors.ustc.edu.cn/@g /etc/apt/sources.list && \\ apt update && \\ DEBIAN_FRONTEND=noninteractive apt install -y dracut git make libncurses-dev libelf-dev bison flex libssl-dev bc && \\ git clone http://192.168.0.90/kdpf/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu.git && \\ alias make='make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu-' 2、make_kernel #!/bin/bash rm -rf out_put/* docker run -it --rm -v $PWD/out:/out gcc-linaro:1.0.0 /bin/sh -c \"sh docker build -t gcc-linaro:1.0.0 . 二、multiarch https://github.com/multiarch/qemu-user-static 一、运行环境 #docker run --rm --privileged multiarch/qemu-user-static --reset -p yes docker run --rm --privileged multiarch/qemu-user-static --reset docker cp $(docker create --rm multiarch/qemu-user-static):/usr/bin/qemu-aarch64-static ./ docker run -it --rm -v /root/qemu-aarch64-static:/usr/bin/qemu-aarch64-static ubuntu bash 二、多阶段构建 vim Dockerfile FROM multiarch/qemu-user-static as qemu FROM arm64v8/ubuntu:20.04 COPY --from=qemu /usr/bin/qemu-aarch64-static /usr/bin RUN echo \"while true;do echo 'hello';done\" > /a.sh CMD [ \"bash\", \"/a.sh\" ] docker build . -t test:1.0.0 -f Dockerfile 三、其他 1、构建参数 --squash，默认false。设置该选项，将新构建出的多个层压缩为一个新层，但是将无法在多个镜像之间共享新层；设置该选项，实际上是创建了新image，同时保留原有image。 --force-rm，默认false。设置该选项，总是删除掉中间环节的容器 --rm，默认--rm=true，即整个构建过程成功后删除中间环节的容器 --compress，默认false。设置该选项，将使用 gzip 压缩构建的上下文 --no-cache，默认false。设置该选项，将不使用Build Cache构建镜像 1)、去除没有用的层（实验性） FROM multiarch/qemu-user-static as qemu FROM arm64v8/ubuntu:20.04 COPY --from=qemu /usr/bin/qemu-aarch64-static /usr/bin RUN echo \"while true;do echo 'hello';done\" > /a.sh && rm -rf /usr/bin/qemu-aarch64-static CMD [ \"bash\", \"/a.sh\" ] 2)、运行 docker 实验功能 vim /etc/docker/daemon.json { \"experimental\": true } systemctl daemon-reload systemctl restart docker 3)、构建 docker build --squash . -t test:1.0.0 -f Dockerfile 2、.dockerignore 文件中指定在传递给 docker 引擎 时需要忽略掉的文件或文件夹 comment #代表根目录（上下文环境目录中）中以abc开头的任意直接子目录或者直接子文件将被忽略 #如/abc abc.txt /abc* #代表根目录（上下文环境目录中）中任意直接子目录中以abc开头的任意直接子目录或者直接子文件将被忽略 #如 /file/abc /file/abc.txt */abc* #代表根目录（上下文环境目录中）中任意两级目录下以abc开头的任意直接子目录或者直接子文件将被忽略 #如 /file1/file2/abc /file1/file2/abc.txt */*/abc* #排除根目录中的文件和目录，其名称是单字符扩展名temp。例如，/tempa与/tempb被排除在外。 temp? #Docker还支持一个**匹配任意数量目录（包括零）的特殊通配符字符串 **/abc* #以!（感叹号）开头的行可用于对排除项进行例外处理,比如原本包含了README.md这个文件的过滤，但是加了如下一行后 #就不会再过滤README.md，依然会将其提交到守护进程。 !README.md #异常规则的放置位置会影响行为 *.md !README*.md README-secret.md #README-secret.md 仍然会被忽略 *.md README-secret.md !README*.md #README-secret.md 不会被忽略 您甚至可以使用该.dockerignore文件来排除Dockerfile和.dockerignore文件。这些文件仍然发送到守护程序，因为它需要它们来完成它的工作。但是ADD和COPY命令不会将它们复制到图像中。 3、从 docker image 拷贝文件 docker run -v $PWD:/opt/mount --rm --entrypoint cp multiarch/qemu-user-static /usr/bin/qemu-aarch64-static /opt/mount/ docker cp $(docker create --rm multiarch/qemu-user-static):/usr/bin/qemu-aarch64-static ./ 4、在 x86 运行 arm 容器 apt install qemu-user-static docker run -it --rm -v /usr/bin/qemu-aarch64-static:/usr/bin/qemu-aarch64-static arm64v8/ubuntu:20.04 uname -m "},"notes/docker/kubeedge.html":{"url":"notes/docker/kubeedge.html","title":"kubeedge","keywords":"","body":"kubeedge 一、要求 云端环境： OS: Ubuntu Server 20.04.1 LTS 64bit Kubernetes: v1.19.8 网络插件：calico v3.16.3 Cloudcore: kubeedge/cloudcore:v1.6.1 边缘环境： OS: Ubuntu Server 18.04.5 LTS 64bit EdgeCore: v1.19.3-kubeedge-v1.6.1 docker: version: 20.10.7 cgroupDriver: systemd 二、安装k8s 略 三、安装golang wget -O /usr/local/go1.16.4.linux-amd64.tar.gz https://golang.org/dl/go1.16.4.linux-amd64.tar.gz tar -C /usr/local -zxvf /usr/local/go1.16.4.linux-amd64.tar.gz echo \"export PATH=$PATH:/usr/local/go/bin\" |tee >> /etc/profile source /etc/profile git clone -b v1.8.2 --deph 1 https://github.com/kubeedge/kubeedge.git curl -LO https://github.com/kubeedge/kubeedge/releases/download/v1.8.2/kubeedge-v1.8.2-linux-amd64.tar.gz curl -LO https://github.com/kubeedge/kubeedge/releases/download/v1.8.2/keadm-v1.8.2-linux-amd64.tar.gz curl -LO https://github.com/kubeedge/kubeedge/releases/download/v1.8.2/edgesite-v1.8.2-linux-amd64.tar.gz keadm init --kube-config=$KUBECONFIG --advertise-address=10.0.0.19 keadm init --kube-config=./config --advertise-address=10.0.0.19 export https_proxy=http://192.168.0.184:1080 export http_proxy=http://192.168.0.184:1080 export HTTPS_PROXY=http://192.168.0.184:1080 export HTTP_PROXY=http://192.168.0.184:1080 keadm init --kube-config=/root/.kube/config --advertise-address=10.0.0.119 --kubeedge-version=1.8.2 ./keadm init --advertise-address=\"10.0.0.119\" Kubernetes version verification passed, KubeEdge installation will start... W1109 10:42:45.450270 168278 warnings.go:70] apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition W1109 10:46:17.975017 168278 warnings.go:70] apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition W1109 10:46:18.762340 168278 warnings.go:70] apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition W1109 10:49:36.682854 168278 warnings.go:70] apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition kubeedge-v1.8.1-linux-amd64.tar.gz checksum: checksum_kubeedge-v1.8.1-linux-amd64.tar.gz.txt content: [Run as service] start to download service file for cloudcore [Run as service] success to download service file for cloudcore kubeedge-v1.8.1-linux-amd64/ kubeedge-v1.8.1-linux-amd64/edge/ kubeedge-v1.8.1-linux-amd64/edge/edgecore kubeedge-v1.8.1-linux-amd64/cloud/ kubeedge-v1.8.1-linux-amd64/cloud/csidriver/ kubeedge-v1.8.1-linux-amd64/cloud/csidriver/csidriver kubeedge-v1.8.1-linux-amd64/cloud/admission/ kubeedge-v1.8.1-linux-amd64/cloud/admission/admission kubeedge-v1.8.1-linux-amd64/cloud/cloudcore/ kubeedge-v1.8.1-linux-amd64/cloud/cloudcore/cloudcore kubeedge-v1.8.1-linux-amd64/version KubeEdge cloudcore is running, For logs visit: /var/log/kubeedge/cloudcore.log CloudCore started cp ./kubeedge/build/tools/cloudcore.service /etc/systemd/system/cloudcore.service cp ./kubeedge-v1.8.2-linux-amd64/cloud/cloudcore/cloudcore /etc/kubeedge/cloudcore systemctl daemon-reload systemctl enable cloudcore systemctl start cloudcore cloudcore curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun cp docker/* /usr/bin/ cp containerd.service /lib/systemd/system/ cp docker.service /lib/systemd/system/ cp docker.socket /lib/systemd/system/ echo \"docker:x:998:\" >> /etc/group systemctl daemon-reload systemctl enable docker systemctl start docker mkdir -p /etc/kubeedge cp -r cloudcore/crds /etc/kubeedge/ cp cloudcore/cloudcore /etc/kubeedge/ cp cloudcore/kubeedge-v1.8.2-linux-amd64.tar.gz /etc/kubeedge/ cp cloudcore/keadm /usr/bin cp cloudcore/cloudcore.service /etc/systemd/system/ mkdir/root/.kube/ cp /etc/rancher/k3s/k3s.yaml /root/.kube/config sed -i \"s/127.0.0.1/10.0.0.122/g\" /root/.kube/config keadm init --kube-config=/root/.kube/config --advertise-address=10.0.0.122 --kubeedge-version=1.8.2 systemctl daemon-reload systemctl enable cloudcore systemctl start cloudcore keadm gettoken edge curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun #apt-add-repository ppa:mosquitto-dev/mosquitto-ppa && apt install -y mosquitto apt-get install -y --allow-change-held-packages --allow-downgrades mosquitto dpkg -i edgecore/mosquitto/*.deb mkdir -p /etc/kubeedge cp edgecore/keadm /usr/local/bin/ cp edgecore/edgecore /etc/kubeedge/ cp edgecore/edgecore.service /etc/kubeedge/ cp edgecore/kubeedge-v1.8.2-linux-amd64.tar.gz /etc/kubeedge/ keadm join --cloudcore-ipport=10.0.0.122:10000 --edgenode-name=node2 --kubeedge-version=1.8.2 --token=2db9d4278daa18a1708e74899d2e85ba202d00f7ec72d3b7eea3056275b4281a.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzcwNjQ3MTB9.gsndb_AAlp809SustU1nk0aQZIDrjRVzB09ZeMDuRkc kubeedge-v1.8.2-linux-amd64.tar.gz checksum: checksum_kubeedge-v1.8.2-linux-amd64.tar.gz.txt content: [Run as service] start to download service file for edgecore [Run as service] success to download service file for edgecore kubeedge-v1.8.2-linux-amd64/ kubeedge-v1.8.2-linux-amd64/edge/ kubeedge-v1.8.2-linux-amd64/edge/edgecore kubeedge-v1.8.2-linux-amd64/cloud/ kubeedge-v1.8.2-linux-amd64/cloud/csidriver/ kubeedge-v1.8.2-linux-amd64/cloud/csidriver/csidriver kubeedge-v1.8.2-linux-amd64/cloud/admission/ kubeedge-v1.8.2-linux-amd64/cloud/admission/admission kubeedge-v1.8.2-linux-amd64/cloud/cloudcore/ kubeedge-v1.8.2-linux-amd64/cloud/cloudcore/cloudcore kubeedge-v1.8.2-linux-amd64/version KubeEdge edgecore is running, For logs visit: journalctl -u edgecore.service -b 开启使用kubectl logs master执行 cp kubeedge/build/tools/certgen.sh /etc/kubeedge/ cd /etc/kubeedge/ /etc/kubeedge/certgen.sh stream export CLOUDCOREIPS=\"10.0.0.119\" iptables -t nat -A OUTPUT -p tcp --dport 10350 -j DNAT --to $CLOUDCOREIPS:10003 vi /etc/kubeedge/config/edgecore.yaml edgeStream: enable: true handshakeTimeout: 30 readDeadline: 15 server: 10.0.0.119:10004 tlsTunnelCAFile: /etc/kubeedge/ca/rootCA.crt tlsTunnelCertFile: /etc/kubeedge/certs/server.crt tlsTunnelPrivateKeyFile: /etc/kubeedge/certs/server.key writeDeadline: 15 vi /etc/kubeedge/edgecore.service Environment=\"CHECK_EDGECORE_ENVIRONMENT=false\" 四、example 云边协同计数器demo 在中心云部署Web管理应用和控制后台应用。通过中心云部署应用到边缘计算节点，包含计数应用和伪计数设备。 通过浏览器登录Web，启动计数，控制后台应用将请求转发到相应边缘计算节点的计数应用中，计数应用收到请求调用伪计数设备进行计数并将结果输出到计数应用的容器日志中。 Web管理应用也可点击停止计数，即将停止请求发送到边缘计算平台的计数应用中，计数应用停止调用伪计数设备，停止计数。 vim /etc/sysctl.conf net.ipv4.ip_forward=1 systemctl restart network sysctl net.ipv4.ip_forward cd examples/kubeedge-counter-demo/crds #创建device model kubectl create -f kubeedge-counter-model.yaml #创建model vim kubeedge-counter-instance.yaml - key: 'kubernetes.io/hostname' values: - edge0 # 边缘节点名称 kubectl create -f examples/kubeedge-counter-demo/crds/kubeedge-counter-instance.yaml cloud cd samples/kubeedge-counter-demo/web-controller-app vim main.go beego.Run(\":8089\") make all make docker vim kubeedge-web-controller-app.yaml nodeName: node1 kubectl create -f examples/kubeedge-counter-demo/crds/kubeedge-web-controller-app.yaml edge cd examples/kubeedge-counter-demo/counter-mapper vim Makefile GOARCH=amd64 go build -o pi-counter-app main.go make all make docker kubectl apply -f examples/kubeedge-counter-demo/crds/kubeedge-pi-counter-app.yaml docker save kubeedge/kubeedge-pi-counter:v1.0.0 > kubeedge-pi-counter_v1.0.0_images_amd64.tar scp kubeedge-pi-counter_v1.0.0_images_amd64.tar node2:/root/ docker load -i kubeedge-pi-counter_v1.0.0_images_amd64.tar docker logs -f counter-container-id http://192.168.0.127:8089 ctr -n=k8s.io c ls 五、重新编译，跳过校验检查 vim kubeedge/keadm/cmd/keadm/app/cmd/util/common.go if _, err = os.Stat(filePath); err == nil { fmt.Printf(\"Expected or Default KubeEdge version %v is already downloaded and will checksum for it. \\n\", version) /* if success, _ := checkSum(filename, checksumFilename, version, options.TarballPath); !success { fmt.Printf(\"%v in your path checksum failed and do you want to delete this file and try to download again? \\n\", filename) for { confirm, err := askForconfirm() if err != nil { fmt.Println(err.Error()) continue } if confirm { cmdStr := fmt.Sprintf(\"cd %s && rm -f %s\", options.TarballPath, filename) if err := NewCommand(cmdStr).Exec(); err != nil { return err } fmt.Printf(\"%v have been deleted and will try to download again\\n\", filename) if err := retryDownload(filename, checksumFilename, version, options.TarballPath); err != nil { return err } } else { fmt.Println(\"failed to checksum and will continue to install.\") } break } } else { fmt.Println(\"Expected or Default KubeEdge version\", version, \"is already downloaded\") }*/ } else if !os.IsNotExist(err) { return err } else { if err := retryDownload(filename, checksumFilename, version, options.TarballPath); err != nil { return err } } vim kubeedge/keadm/cmd/keadm/app/cmd/util/rpminstaller.go // \"yum -y install epel-release\", // \"yum -y install mosquitto\", make all WHAT=keadm go build -o /opt/_output/local/bin/edgesite-agent -gcflags= -ldflags CGO_ENABLED=1 GO111MODULE=off go build -v -o /usr/local/bin/edgecore -ldflags=\"${GO_LDFLAGS} -w -s -extldflags -static\" \\ ./cloudcore --version KubeEdge v1.9.1-1+aedc2e42be2203 vim hack/lib/golang.sh goldflags=\"${GOLDFLAGS=-s -w -extldflags -static -buildid=} $(kubeedge::version::ldflags)\" git describe --tags "},"notes/docker/k3s_deploy.html":{"url":"notes/docker/k3s_deploy.html","title":"k3s 安装与使用","keywords":"","body":"k3s 安装与使用 一、安装 1、安装 curl -sfL https://get.k3s.io | sh - cat /var/lib/rancher/k3s/server/node-token curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=XXX sh - 国内安装： server: curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh - node: curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh - 2、 k3s 安装配置 helm curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash cp /etc/rancher/k3s/k3s.yaml $HOME/.kube/config 或 export KUBECONFIG=/etc/rancher/k3s/k3s.yaml helm ls helm repo add aliyuncs https://apphub.aliyuncs.com helm repo add bitnami https://charts.bitnami.com/bitnami # vim /root/.config/helm 3、修改k3s使用本地仓库 vim /etc/hosts 192.168.0.242 local.com scp -r 192.168.0.242:/etc/docker/certs.d/local.com /etc/docker/certs.d/ vim /etc/rancher/k3s/registries.yaml mirrors: local.com: endpoint: - \"https://local.com\" configs: \"local.com\": auth: username: admin password: qwe tls: cert_file: /etc/docker/certs.d/local.com/local.com.cert key_file: /etc/docker/certs.d/local.com/local.com.key ca_file: /etc/docker/certs.d/local.com/ca.crt 4、k3s 可配置环境变量 Environment Variable Description INSTALL_K3S_SKIP_DOWNLOAD 如果设置为 \"true \"将不会下载 K3s 的哈希值或二进制。 INSTALL_K3S_SYMLINK 默认情况下，如果路径中不存在命令，将为 kubectl、crictl 和 ctr 二进制文件创建符号链接。如果设置为'skip'将不会创建符号链接，而'force'将覆盖。 INSTALL_K3S_SKIP_ENABLE 如果设置为 \"true\"，将不启用或启动 K3s 服务。 INSTALL_K3S_SKIP_START 如果设置为 \"true \"将不会启动 K3s 服务。 INSTALL_K3S_VERSION 从 Github 下载 K3s 的版本。如果没有指定，将尝试从\"stable\"频道下载。 INSTALL_K3S_BIN_DIR 安装 K3s 二进制文件、链接和卸载脚本的目录，或者使用/usr/local/bin作为默认目录。 INSTALL_K3S_BIN_DIR_READ_ONLY 如果设置为 true 将不会把文件写入INSTALL_K3S_BIN_DIR，强制设置INSTALL_K3S_SKIP_DOWNLOAD=true。 INSTALL_K3S_SYSTEMD_DIR 安装 systemd 服务和环境文件的目录，或者使用/etc/systemd/system作为默认目录。 INSTALL_K3S_EXEC 带有标志的命令，用于在服务中启动 K3s。如果未指定命令，并且设置了K3S_URL，它将默认为“agent”。如果未设置K3S_URL，它将默认为“server”。要获得帮助，请参考此示例。 INSTALL_K3S_NAME 要创建的 systemd 服务名称，如果以服务器方式运行 k3s，则默认为'k3s'；如果以 agent 方式运行 k3s，则默认为'k3s-agent'。如果指定了服务名，则服务名将以'k3s-'为前缀。 INSTALL_K3S_TYPE 要创建的 systemd 服务类型，如果没有指定，将从 K3s exec 命令中默认。 INSTALL_K3S_CHANNEL_URL 用于获取 K3s 下载网址的频道 URL。默认为https://update.k3s.io/v1-release/channels 。 INSTALL_K3S_CHANNEL 用于获取 K3s 下载 URL 的通道。默认值为 \"stable\"。选项包括：stable, latest, testing。 二、集群安装 1、下载 install 脚本 curl -sfL https://get.k3s.io -o install.sh 2、准备 k3s、离线镜像 略 3、server 安装 vim install.sh ... INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC=\"--docker\" cp k3s /usr/local/bin/ mkdir -p /var/lib/rancher/k3s/agent/images/ cp k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/ ... ./install.sh 4、agent 安装 从 server 获取 token cat /var/lib/rancher/k3s/server/node-token K10bc4ac5219e3d43898a2fd70eba5160ef3c38cd6485eeb3c60c6ac1aa80a14f63::server:d1a22f5300bf09bcab32632de490309a vim install.sh ... export K3S_URL=\"https://node1:6443\" export K3S_TOKEN=\"K10bc4ac5219e3d43898a2fd70eba5160ef3c38cd6485eeb3c60c6ac1aa80a14f63::server:d1a22f5300bf09bcab32632de490309a\" INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC=\"--docker\" cp k3s /usr/local/bin/ mkdir -p /var/lib/rancher/k3s/agent/images/ cp k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/ ... ./install.sh 三、使用 1、k3s 二进制文件启动 k3s server & # Kubeconfig is written to /etc/rancher/k3s/k3s.yaml k3s kubectl get node # On a different node run the below. NODE_TOKEN comes from /var/lib/rancher/k3s/server/node-token # on your server k3s agent --server https://myserver:6443 --token ${NODE_TOKEN} 2、常用命令 crictl: CRI 的 CLI 工具 ctr: containerd 本身的 CLI 工具 k3s kubectl get nodes k3s kubectl get all -A crictl ps crictl images crictl pull rancher/rancher-agent:v2.4.8 # 清理未使用镜像 crictl rmi --prune ctr images ls ctr c ls ctr image import busybox.tar # 镜像标记tag ctr -n k8s.io i tag hub.dream.io/hello:latest hub.dream.io/hello:second # 删除镜像 ctr -n k8s.io i rm hub.dream.io/hello:latest # 拉取镜像 ctr -n k8s.io i pull -k hub.dream.io/hello:latest # 推送镜像 ctr -n k8s.io i push -k hub.dream.io/hello:latest # 导出镜像 ctr -n k8s.io i export hello.tar hub.dream.io/hello:latest # 导入镜像 ctr -n k8s.io i import hello.tar # 不支持 build,commit 镜像 # 查看容器相关操作 ctr c # 运行容器 ctr -n k8s.io run --null-io --net-host -d \\ --env PASSWORD=$drone_password \\ --mount type=bind,src=/etc,dst=/host-etc,options=rbind:rw \\ --mount type=bind,src=/root/.kube,dst=/root/.kube,options=rbind:rw \\ $image sysreport bash /sysreport/run.sh 1. --null-io: 将容器内标准输出重定向到/dev/null 2. --net-host: 主机网络 3. -d: 当task执行后就进行下一步shell命令,如没有选项,则会等待用户输入,并定向到容器内 # K3s worker 节点的角色默认为none kubectl label node ${node} node-role.kubernetes.io/worker=worker 四、其他 1、ctr 访问 k3s crictl 资源 ctr -a \"/run/k3s/containerd/containerd.sock\" -namespace k8s.io i ls 2、使用 kubectl / helm 从外部访问集群 将 /etc/rancher/k3s/k3s.yaml 复制到集群外部的计算机上的~/.kube/config。然后用 K3s 服务器的 IP 或名称替换 \"localhost\"。 export KUBECONFIG=/etc/rancher/k3s/k3s.yaml kubectl get pods --all-namespaces helm ls --all-namespaces kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pods --all-namespaces helm --kubeconfig /etc/rancher/k3s/k3s.yaml ls --all-namespaces 3、curl 请求 kubernetes api kubectl get secrets kubectl describe secrets default-token-l28jq curl --insecure https://192.168.163.121:6443/api --header \"Authorization: bearer $token\" 4、k3s-killall.sh 要停止所有的 k3s 容器并重置容器的状态，可以使用k3s-killall.sh脚本。 killall 脚本清理容器、k3s 目录和网络组件，同时也删除了 iptables 链和所有相关规则。集群数据不会被删除。 5、k3s-uninstall.sh 全部删除完，包括配置和数据。 6、设置私有仓库 1）k3s cat >> /etc/rancher/k3s/registries.yaml 2)、docker cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn/\"], \"insecure-registries\": [\"192.168.0.90:3000\"] } 3)、containerd containerd config default > /etc/containerd/config.toml cat /etc/containerd/config.toml [plugins.cri.registry.mirrors] [plugins.cri.registry.mirrors.\"192.168.0.10:3000\"] endpoint = [\"http://192.168.0.10:3000\"] systemctl restart containerd.service 五、k3s证书有效期 https://kingsd.top/2022/04/07/k3s-custom-ca/ 1、创建100年有效期CA证书 mkdir -p /var/lib/rancher/k3s/server/tls cd /var/lib/rancher/k3s/server/tls openssl genrsa -out client-ca.key 2048 openssl genrsa -out server-ca.key 2048 openssl genrsa -out request-header-ca.key 2048 openssl req -x509 -new -nodes -key client-ca.key -sha256 -days 36500 -out client-ca.crt -addext keyUsage=critical,digitalSignature,keyEncipherment,keyCertSign -subj '/CN=k3s-client-ca' openssl req -x509 -new -nodes -key server-ca.key -sha256 -days 36500 -out server-ca.crt -addext keyUsage=critical,digitalSignature,keyEncipherment,keyCertSign -subj '/CN=k3s-server-ca' openssl req -x509 -new -nodes -key request-header-ca.key -sha256 -days 36500 -out request-header-ca.crt -addext keyUsage=critical,digitalSignature,keyEncipherment,keyCertSign -subj '/CN=k3s-request-header-ca' 2、使用自签名CA证书启动k3s 因为已经在 /var/lib/rancher/k3s/server/tls 创建了CA证书，所以当启动k3s时，将使用已创建的CA颁发证书。 直接安装k3s curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=v1.21 sh - 3、查看证书有效期 cd /var/lib/rancher/k3s/server/tls for i in `ls /var/lib/rancher/k3s/server/tls/*.crt`; do echo $i; openssl x509 -enddate -noout -in $i; done /var/lib/rancher/k3s/server/tls/client-admin.crt notAfter=Apr 7 02:50:31 2023 GMT /var/lib/rancher/k3s/server/tls/client-auth-proxy.crt notAfter=Apr 7 02:50:31 2023 GMT /var/lib/rancher/k3s/server/tls/client-ca.crt notAfter=Mar 14 02:49:24 2122 GMT /var/lib/rancher/k3s/server/tls/client-controller.crt notAfter=Apr 7 02:50:31 2023 GMT /var/lib/rancher/k3s/server/tls/client-k3s-cloud-controller.crt notAfter=Apr 7 02:50:31 2023 GMT /var/lib/rancher/k3s/server/tls/client-k3s-controller.crt notAfter=Apr 7 02:50:31 2023 GMT /var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt notAfter=Apr 7 02:50:31 2023 GMT /var/lib/rancher/k3s/server/tls/client-kube-proxy.crt notAfter=Apr 7 02:50:31 2023 GMT /var/lib/rancher/k3s/server/tls/client-scheduler.crt notAfter=Apr 7 02:50:31 2023 GMT /var/lib/rancher/k3s/server/tls/request-header-ca.crt notAfter=Mar 14 02:49:24 2122 GMT /var/lib/rancher/k3s/server/tls/server-ca.crt notAfter=Mar 14 02:49:24 2122 GMT /var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt notAfter=Apr 7 02:50:31 2023 GMT kubectl get secret -n kube-system k3s-serving -o jsonpath='{.data.tls\\.crt}' | base64 -d | openssl x509 -noout -text | grep Not Not Before: Apr 7 02:49:24 2022 GMT Not After : Apr 7 02:50:31 2023 GMT 从结果可以看到，CA证书的有效期已经变成100年，其他客户端证书的有效期依然是1年 4、模拟证书过期，轮换证书 将服务器时间调整为50年后 ~# timedatectl set-ntp no ~# date -s 20720407 Thu Apr 7 00:00:00 CST 2072 ~# date Thu Apr 7 00:00:00 CST 2072 重启k3s，触发证书轮换 service k3s restart 证书轮换后，再次 查询证书的有效期 for i in `ls /var/lib/rancher/k3s/server/tls/*.crt`; do echo $i; openssl x509 -enddate -noout -in $i; done /var/lib/rancher/k3s/server/tls/client-admin.crt notAfter=Apr 6 16:00:41 2073 GMT /var/lib/rancher/k3s/server/tls/client-auth-proxy.crt notAfter=Apr 6 16:00:41 2073 GMT /var/lib/rancher/k3s/server/tls/client-ca.crt notAfter=Mar 14 02:49:24 2122 GMT /var/lib/rancher/k3s/server/tls/client-controller.crt notAfter=Apr 6 16:00:41 2073 GMT /var/lib/rancher/k3s/server/tls/client-k3s-cloud-controller.crt notAfter=Apr 6 16:00:41 2073 GMT /var/lib/rancher/k3s/server/tls/client-k3s-controller.crt notAfter=Apr 6 16:00:41 2073 GMT /var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt notAfter=Apr 6 16:00:41 2073 GMT /var/lib/rancher/k3s/server/tls/client-kube-proxy.crt notAfter=Apr 6 16:00:41 2073 GMT /var/lib/rancher/k3s/server/tls/client-scheduler.crt notAfter=Apr 6 16:00:41 2073 GMT /var/lib/rancher/k3s/server/tls/request-header-ca.crt notAfter=Mar 14 02:49:24 2122 GMT /var/lib/rancher/k3s/server/tls/server-ca.crt notAfter=Mar 14 02:49:24 2122 GMT /var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt notAfter=Apr 6 16:00:41 2073 GMT kubectl get secret -n kube-system k3s-serving -o jsonpath='{.data.tls\\.crt}' | base64 -d | openssl x509 -noout -text | grep Not Not Before: Apr 7 02:49:24 2022 GMT Not After : Apr 6 16:01:11 2073 GMT kubectl get nodes NAME STATUS ROLES AGE VERSION dev-1 Ready control-plane,master 50y v1.21.11+k3s1 k3s 服务重启后，会自动轮换k3s证书。从以上结果可以看到k3s的证书已经更新到2073年，已经突破了k3s默认生成CA的10年有限期，证明自签名的CA已经生效，客户端证书是由自签名CA颁发 5、后记 通过使用自签名CA的形式来延长CA证书的有效期，其他客户端证书有效期依然为1年，因此还需要在证书已经过期或剩余的时间不足90天时重启k3s来触发客户端证书轮换。 六、FAQ error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: \"cgroupfs\" is different from docker cgroup driver: \"systemd\" \"Failed to run kubelet\" err=\"failed to run Kubelet: misconfiguration: kubelet cgroup driver: \\\"cgroupfs\\\" is different from docker cgroup driver: \\\"systemd\\\"\" vim /etc/systemd/system/docker.service.d/docker-options.conf ... --exec-opt native.cgroupdriver=cgroupfs ... 或 vim /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] } {\"exec-opts\": [\"native.cgroupdriver=systemd\"]} "},"notes/docker/k3s_make.html":{"url":"notes/docker/k3s_make.html","title":"k3s 源码编译","keywords":"","body":"k3s 源码编译 一、编译 1、安装 golang wget https://golang.google.cn/dl/go1.15.2.linux-amd64.tar.gz tar -C /usr/local -xzf go1.15.2.linux-amd64.tar.gz vim /etc/profile export PATH=$PATH:/usr/local/go/bin 2、获取源码（v1.19.2+k3s1） git clone -b v1.19.2+k3s1 --depth=1 https://github.com/rancher/k3s.git 注： --depth=1 用于指定克隆深度，为1即表示只克隆最近一次commit. -b ${branch} clone 某个分支 3、修改代理 cd ./k3s/ vim ./Dockerfile.dapper ARG http_proxy=http://192.168.0.110:1080 ARG https_proxy=http://192.168.0.110:1080 ... trivy --download-db-only --severity HIGH ... curl -x 192.168.0.110:1080 ... vim ./scripts/download curl -x 192.168.0.110:1080 4、下载生成依赖 mkdir -p build/data && ./scripts/download && go generate 5、编译 SKIP_VALIDATE=true make 1、SKIP_VALIDATE=true，因为修改了源文件，不跳过检查会报 dirty 错误 2、如果修改了go.mod 文件，需要在make前先执行 go mod vendor && go mod tidy 6、查看编译结果 编译成功会在当前目录生成 dist 文件夹 k3s 为编译后的二进制文件 k3s-airgap-images-amd64.tar为k3s相关镜像 k3s-images.txt 为k3s相关镜像列表 ls ./dist/artifacts/ k3s k3s-airgap-images-amd64.tar k3s-images.txt cat k3s-images.txt docker.io/rancher/coredns-coredns:1.6.9 docker.io/rancher/klipper-helm:v0.3.0 docker.io/rancher/klipper-lb:v0.1.2 docker.io/rancher/library-busybox:1.31.1 docker.io/rancher/library-traefik:1.7.19 docker.io/rancher/local-path-provisioner:v0.0.14 docker.io/rancher/metrics-server:v0.3.6 docker.io/rancher/pause:3.1 二、运行验证 将编译成功后的 dist/artifacts/ 文件拷贝至相应节点，控制节点和计算节点 1、运行 k3s 控制节点（控制节点执行） cd dist/artifacts/ ./k3s server & 2、查看集群状态（控制节点执行） ./k3s kubectl get nodes 3、运行 k3s 计算节点（计算节点执行） k3s agent --server https://myserver:6443 --token ${NODE_TOKEN} myserver 为控制节点地址 token 为控制节点，cat /var/lib/rancher/k3s/server/node-token 中的内容 4、再次查看集群状态（控制节点执行） ./k3s kubectl get nodes 5、将 k3s 二进制执行文件放入环境变量中 cp dist/artifacts/k3s /usr/bin/ 6、参考链接 k3s build 三、离线安装 1、拷贝 k3s 二进制文件到系统命令中 cp ./dist/artifacts/k3s-arm64 /usr/local/bin/k3s 2、拷贝镜像到相应文件夹，使用离线镜像 mkdir -p /var/lib/rancher/k3s/agent/images/ cp dist/artifacts/k3s-airgap-images-arm64.tar /var/lib/rancher/k3s/agent/images/ 3、执行安装脚本，install.sh 在相应版本的 k3s 源码中（server） https://github.com/rancher/k3s.git INSTALL_K3S_SKIP_DOWNLOAD=true ./install.sh 4、node 节点安装 cat /var/lib/rancher/k3s/server/node-token K3S_TOKEN=${k3s_token} K3S_URL=https://serverIP:6443 ./install.sh 5、验证 kubectl get nodes 6、卸载 k3s-uninstall.sh k3s-agent-uninstall.sh "},"notes/docker/rancher.html":{"url":"notes/docker/rancher.html","title":"rancher 相关","keywords":"","body":"rancher 相关 一、编译rancher dashborad 1、依赖安装 wget https://nodejs.org/dist/v16.14.2/node-v16.14.2-linux-x64.tar.xz tar -vxf node-v16.14.2-linux-x64.tar.xz cp -r node-v16.14.2-linux-x64 /usr/local/ cd node-v16.14.2-linux-x64/ ./bin/node -v v16.14.2 cd /usr/local/node-v16.14.2-linux-x64/ ln -s /usr/local/node-v16.14.2-linux-x64/bin/npm /usr/bin/ ln -s /usr/local/node-v16.14.2-linux-x64/bin/node /usr/bin/ npm config set registry=https://registry.npm.taobao.org npm install -g cnpm --registry=https://registry.npm.taobao.org npm install --global yarn yarn config set registry https://registry.npm.taobao.org 可能存在preset-env报错问题 npm uninstall @babel/preset-env npm install @babel/preset-env@7.12.13 2、下载运行 依赖rancher server的运行，需提前部署 git clone https://github.com/rancher/dashboard.git cd dashboard/ yarn install API=https://192.168.0.31:30000 yarn dev 3、容器开发 docker run -it --net=host --name dev -v /git/dashboard:/src rancher/dashboard:dev bash 4、代码修改 vim pkg/settings/setting.go UIIndex = NewSetting(\"ui-index\", \"/usr/share/rancher/ui/index.html\") UIDashboardIndex = NewSetting(\"ui-dashboard-index\", \"https://releases.rancher.com/dashboard/latest/index.html\") 5、静态编译 编译完成将生成文件，拷贝到http://192.168.0.31:81/dashboard/v2.6.3-beta1.tar.gz中 ./scripts/build-embedded ls dist 6、编译rancher clone rancher 源码进行相应容器镜像编译 vim package/Dockerfile curl -sL http://192.168.0.31:81/dashboard/v2.6.3-beta1.tar.gz | tar xvzf - --strip-components=2 && \\ 7、重置dashboard密码 https://github.com/rancher/rancher/issues/34920 kubectl -n cattle-system exec $(kubectl -n cattle-system get pods -l app=rancher | grep '1/1' | head -1 | awk '{ print $1 }') -- reset-password 二、部署运行rancher 1、通过k8s部署 kubectl create namespace cattle-system kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.crds.yaml helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.7.1 helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org \\ --set bootstrapPassword=admin helm install rancher . --namespace cattle-system helm uninstall rancher --namespace cattle-system 修改rancher为NodePort vim rancher/templates/service.yaml apiVersion: v1 kind: Service metadata: name: {{ template \"rancher.fullname\" . }} labels: {{ include \"rancher.labels\" . | indent 4 }} spec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: 80 nodePort: 30090 - name: https-internal port: 443 protocol: TCP targetPort: 443 nodePort: 30493 selector: app: {{ template \"rancher.fullname\" . }} 2、通过容器部署 docker run -d --restart=unless-stopped \\ --net=\"host\" \\ --restart always \\ -v /etc/docker/certs.d/local.com:/container/certs \\ -e SSL_CERT_DIR=\"/container/certs\" \\ --name rancher \\ rancher/rancher:latest 三、编译rancher-ui 1、基础环境 npm config set registry=https://registry.npm.taobao.org yarn config set registry https://registry.npm.taobao.org npm install -g cnpm --registry=https://registry.npm.taobao.org yum -y install gcc+ gcc-c++ 2、更新nodejs npm install -g npm@latest npm install -g node-gyp 3、编译rancher-ui git clone 'https://github.com/rancher/ui' cd 'ui' npm install yarn upgrade ./scripts/update-dependencies 4、运行 yarn start RANCHER=\"https://rancher-server\" yarn start RANCHER=\"https://192.168.0.10\" yarn start 四、通过 helm 安装 rancher 1、创建命名空间 kubectl create namespace cattle-system 2、创建秘钥 kubectl -n cattle-system create secret tls tls-rancher-ingress --cert=/etc/docker/certs.d/local.com/local.com.cert --key=/etc/docker/certs.d/local.com/local.com.key 3、查看秘钥信息 kubectl -n cattle-system get secret 4、添加 rancher 仓库 helm repo add rancher-latest https://releases.rancher.com/server-charts/latest latest：最新版，建议在尝试新功能时使用。 stable：稳定版，建议生产环境中使用。 alpha：预览版，未来版本的实验性预览。 5、拉取 rancher chart 包 helm pull rancher-latest/rancher --untar 6、修改配置 vim rancher/value.yaml hostname: dashboard ingress: tls: source: secret 7、安装 helm -n cattle-system install rancher . 8、查看安装状态 kubectl -n cattle-system rollout status deploy/rancher 五、FAQ 1、如果harbor中应用更新相同版本，rancher会有cache，版本不会更新 docker exec -itu0 rancher bash rm -rf /var/lib/rancher/management-state/catalog-cache "},"notes/docker/debootstrap.html":{"url":"notes/docker/debootstrap.html","title":"容器基础镜像构建","keywords":"","body":"容器基础镜像构建 构建基础的 rootfs —> 配置基础系统参数 —> 部署用户自定义软件 —> 清理系统 —> 打包为容器镜像 —> 测试镜像 —> 发布仓库 https://docs.docker.com/develop/develop-images/baseimages/ 一、Ubuntu 1、安装 debootstrap apt install -y debootstrap 2、创建 rootfs 存放位置 mkdir -p /opt/diros cd /opt/diros 3、构建基础 Ubuntu 20.10 groovy 的 rootfs debootstrap --verbose --arch=amd64 groovy /opt/diros http://mirrors.aliyun.com/ubuntu #debootstrap --verbose --arch=amd64 bionic /opt/diros https://mirrors.tuna.tsinghua.edu.cn/ubuntu 4、配置基础系统参数 chroot /opt/diros/ /bin/bash apt update apt upgrade apt -y install vim locales dpkg-reconfigure locales cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime tee /etc/lsb-release rm -rf /tmp/* apt clean exit 5、打包并创建 docker 镜像 tar -C /opt/diros/ -cv . | docker import - diros #tar --numeric-owner --exclude=/proc --exclude=/sys --exclude=/diros.tar -cvf diros.tar / tar --numeric-owner --exclude=/proc --exclude=/sys -cvf diros.tar / cat diros.tar | docker import - diros tar --numeric-owner --exclude=/proc --exclude=/sys -C / -cv . | docker import - diros 6、测试 docker run diros cat /etc/lsb-release 二、Centos 1、下载 moby git clone https://github.com/moby/moby #git checkout 20.10 cd moby/contrib/ 2、构建 ./mkimage-yum.sh centos docker images 三、FROM scratch mkdir /opt/tmp cd /opt/tmp vim Dockerfile FROM scratch ADD hello / CMD [\"/hello\"] wget https://raw.githubusercontent.com/docker-library/hello-world/master/hello.c docker run --rm -it -v $PWD:/build ubuntu:16.04 container# apt-get update && apt-get install -y build-essential container# cd /build container# gcc -o hello -static -nostartfiles hello.c docker build --tag hello . docker run --rm hello 四、多阶段编译 vim Dockerfile FROM ubuntu:16.04 AS buildstage RUN apt update && apt install -y build-essential wget && \\ wget https://raw.githubusercontent.com/docker-library/hello-world/master/hello.c && \\ gcc -o hello -static -nostartfiles hello.c FROM scratch COPY --from=buildstage hello / CMD [\"/hello\"] docker build --tag hello . 五、buildah https://github.com/containers/buildah/blob/master/docs/buildah-push.md apt install -y buildah yum install -y buildah buildah images vim Dockerfile FROM ubuntu:16.04 AS buildstage RUN apt update && apt install -y build-essential wget && \\ wget https://raw.githubusercontent.com/docker-library/hello-world/master/hello.c && \\ gcc -o hello -static -nostartfiles hello.c FROM scratch COPY --from=buildstage hello / CMD [\"/hello\"] buildah bud -t test:latest ./ buildah push cbd1566b3bbb dir:./b buildah push cbd1566b3bbb docker-archive:./b.tar "},"notes/docker/overlay2.html":{"url":"notes/docker/overlay2.html","title":"overlay2","keywords":"","body":"overlay2 一、查看镜像 id docker images |grep 3.9-alpine3.12 python 3.9-alpine3.12 7254f7459375 3 months ago 44.2MB 二、查看镜像层 cat /var/lib/docker/image/overlay2/imagedb/content/sha256/7254f7459375ca357ad951c2a97d548455dc9bcc0064589c519ff7dc37708d7d|python3 -m json.tool ... \"rootfs\": { \"type\": \"layers\", \"diff_ids\": [ \"sha256:f4666769fca7a1db532e3de298ca87f7e3124f74d17e1937d1127cb17058fead\", \"sha256:f2bc6754fc86dfbd965dfb93127dc473c36bc44cf21f87a779dcd0f94d924f3d\", \"sha256:e25ee6f7192be198a37c00741fce057aaa0eef58b0fb77a9fc2a23dcd84a5302\", \"sha256:0d2c65b855c43c1c83cf24e866e70e232f3ba451aa6aa159adc1e4dc3d51f96a\", \"sha256:debde983e4fe3ad328c6aa3f37cf9ee30d268b8266e37da87417133d0cc252ad\" ] } ... docker inspect python:3.9-alpine3.12 ... \"RootFS\": { \"Type\": \"layers\", \"Layers\": [ \"sha256:f4666769fca7a1db532e3de298ca87f7e3124f74d17e1937d1127cb17058fead\", \"sha256:f2bc6754fc86dfbd965dfb93127dc473c36bc44cf21f87a779dcd0f94d924f3d\", \"sha256:e25ee6f7192be198a37c00741fce057aaa0eef58b0fb77a9fc2a23dcd84a5302\", \"sha256:0d2c65b855c43c1c83cf24e866e70e232f3ba451aa6aa159adc1e4dc3d51f96a\", \"sha256:debde983e4fe3ad328c6aa3f37cf9ee30d268b8266e37da87417133d0cc252ad\" ] }, ... 三、查看元数据信息及 rootfs 1、第一层 ls /var/lib/docker/image/overlay2/layerdb/sha256/f4666769fca7a1db532e3de298ca87f7e3124f74d17e1937d1127cb17058fead/ cache-id diff size tar-split.json.gz cat cache-id a9410091d624483c6ea184a139807000b39e9ee2d4e6fda29ecbfb7d7a914856 ls /var/lib/docker/overlay2/a9410091d624483c6ea184a139807000b39e9ee2d4e6fda29ecbfb7d7a914856 committed diff link ls /var/lib/docker/overlay2/a9410091d624483c6ea184a139807000b39e9ee2d4e6fda29ecbfb7d7a914856/diff/ bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var cat link ROYFFPZS3DDRNW5YDCZ4RC6TKS ls /var/lib/docker/overlay2/l/ROYFFPZS3DDRNW5YDCZ4RC6TKS/ bin dev etc home lib media mnt opt proc root run sbin srv sys tmp usr var 2、第二层 echo -n \"sha256:f4666769fca7a1db532e3de298ca87f7e3124f74d17e1937d1127cb17058fead sha256:f2bc6754fc86dfbd965dfb93127dc473c36bc44cf21f87a779dcd0f94d924f3d\"|sha256sum 25370907defafe1eacf55dd1df121dd16890773e4a3d0637629de56ed6c69198 ls /var/lib/docker/image/overlay2/layerdb/sha256/25370907defafe1eacf55dd1df121dd16890773e4a3d0637629de56ed6c69198/ cache-id diff parent size tar-split.json.gz 3、第三层 echo -n \"sha256:25370907defafe1eacf55dd1df121dd16890773e4a3d0637629de56ed6c69198 sha256:e25ee6f7192be198a37c00741fce057aaa0eef58b0fb77a9fc2a23dcd84a5302\"|sha256sum 2d87c3c42dcce905c4f0aafd9657cc0247a9a79c461f0e8abf81a2c8523dacfb 4、第四层 echo -n \"sha256:2d87c3c42dcce905c4f0aafd9657cc0247a9a79c461f0e8abf81a2c8523dacfb sha256:0d2c65b855c43c1c83cf24e866e70e232f3ba451aa6aa159adc1e4dc3d51f96a\"|sha256sum 49c9ddedef77ac89647c786c3bd53c7c6bb9db897a0465087e1847afeb5c9eba 5、第五层 echo -n \"sha256:49c9ddedef77ac89647c786c3bd53c7c6bb9db897a0465087e1847afeb5c9eba sha256:debde983e4fe3ad328c6aa3f37cf9ee30d268b8266e37da87417133d0cc252ad\"|sha256sum 66604bb690d7ccdba67800376f20617e4821baf9c716534a399288bd9845be12 三、查看挂载信息 docker run -it --rm python:3.9-alpine3.12 sh mount rootfs on / type rootfs (rw) overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/WEJADLTSDEUAABOXXY6UZFIO4R:/var/lib/docker/overlay2/l/5Q5UEBNKJEEDE6YZ265HDXGXXZ:/var/lib/docker/overlay2/l/GRABQF7VZXYCVN52TF2YEHMO27:/var/lib/docker/overlay2/l/EQ2OTBRW4K7MN6Z2R4QKIIOFHR:/var/lib/docker/overlay2/l/4X5YCB5KOIFOQZEK5P6GAZJY7E:/var/lib/docker/overlay2/l/ROYFFPZS3DDRNW5YDCZ4RC6TKS,upperdir=/var/lib/docker/overlay2/6442daf60a7587396f17c727e570c54538e5731bdfd50af4cb2ac2e59a0e2382/diff,workdir=/var/lib/docker/overlay2/6442daf60a7587396f17c727e570c54538e5731bdfd50af4cb2ac2e59a0e2382/work) proc on /proc type proc (rw,nosuid,nodev,noexec,relatime) tmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755) devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666) sysfs on /sys type sysfs (ro,nosuid,nodev,noexec,relatime) tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,relatime,mode=755) cgroup on /sys/fs/cgroup/systemd type cgroup (ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/devices type cgroup (ro,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/perf_event type cgroup (ro,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (ro,nosuid,nodev,noexec,relatime,net_prio,net_cls) cgroup on /sys/fs/cgroup/cpuset type cgroup (ro,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (ro,nosuid,nodev,noexec,relatime,cpuacct,cpu) cgroup on /sys/fs/cgroup/freezer type cgroup (ro,nosuid,nodev,noexec,relatime,freezer) cgroup on /sys/fs/cgroup/hugetlb type cgroup (ro,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/pids type cgroup (ro,nosuid,nodev,noexec,relatime,pids) cgroup on /sys/fs/cgroup/memory type cgroup (ro,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/blkio type cgroup (ro,nosuid,nodev,noexec,relatime,blkio) mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime) shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k) /dev/mapper/centos-root on /etc/resolv.conf type xfs (rw,relatime,attr2,inode64,noquota) /dev/mapper/centos-root on /etc/hostname type xfs (rw,relatime,attr2,inode64,noquota) /dev/mapper/centos-root on /etc/hosts type xfs (rw,relatime,attr2,inode64,noquota) devpts on /dev/console type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666) proc on /proc/bus type proc (ro,relatime) proc on /proc/fs type proc (ro,relatime) proc on /proc/irq type proc (ro,relatime) proc on /proc/sys type proc (ro,relatime) proc on /proc/sysrq-trigger type proc (ro,relatime) tmpfs on /proc/acpi type tmpfs (ro,relatime) tmpfs on /proc/kcore type tmpfs (rw,nosuid,size=65536k,mode=755) tmpfs on /proc/keys type tmpfs (rw,nosuid,size=65536k,mode=755) tmpfs on /proc/timer_list type tmpfs (rw,nosuid,size=65536k,mode=755) tmpfs on /proc/timer_stats type tmpfs (rw,nosuid,size=65536k,mode=755) tmpfs on /proc/sched_debug type tmpfs (rw,nosuid,size=65536k,mode=755) tmpfs on /proc/scsi type tmpfs (ro,relatime) tmpfs on /sys/firmware type tmpfs (ro,relatime) "},"notes/docker/k8s.html":{"url":"notes/docker/k8s.html","title":"k8s 相关","keywords":"","body":"k8s 相关 一、相关概念 k8s components ​ Kubernetes 是一个开源容器编排引擎，用于容器化应用的自动化部署、扩展、管理、自动扩缩容、维护等功能。k8s 最核心的两个设计理念容错性和易扩展性，容错性保证 k8s 系统稳定性和安全性的基础，易扩展性是保证 k8s 对变更友好，可以快速迭代增加新功能的基础。 功能： 快速部署应用（自动化容器部署和复制） 快速扩展应用（随时扩展或收缩容器规模） 无缝对接新应用功能（很容易升级应用到新版本） 提供容器弹性，如果容器失效就替换它 将容器组织成组，并提供容器间的负载均衡 节省资源，优化硬件资源的使用 特点： 可移植：支持公有云、私有云、混合云、多重云（muti-cloud） 可扩展：模块化、插件化、可挂载、可组合 自动化：自动部署、自动重启、自动复制、自动伸缩/扩展 优势： 容器编排 轻量级 开源 弹性伸缩 负载均衡 控制平面组件： 名称 说明 API Server API服务是控制平面的访问入口 Etcd 保存集群数据的键值对存储服务 Scheduler 调度Pod在工作节点上运行 kube-controllermanager 管理云平台无关的控制器 cloud-controllermanager 管理所有与云平台相关的控制器，这是为了把云平台相关的代码与k8s自身进行分离 节点平面组件： 名称 说明 kubelet 确保Pod中的容器运行正常 kube-proxy 用来实现服务的网络代理 container runtime 负责运行容器的软件，如Docker 核心思想 资源注册 发现框架 Pod k8s 基本调度单元，表示由一个或多个容器组成的分组。Pod 里面的容器共享一些资源，包括存储、网络和每个容器的运行配置。每个 Pod 都有唯一的 IP 地址，其中的容器共享同一个 IP 地址和端口范围，相互之间可以通过 localhost 访问。Pod 还可以使它的容器访问其定义的共享的存储卷（volume），通过这种方式，可以实现 Pod 容器之间的数据共享。 副本集（ReplicaSet） 保证在任何时候都有给定数量的Pod副本。当创建副本集时需要指定一个创建Pod的模板，同时确定Pod的选择器，并给出期望的Pod副本数量。在运行时，副本集根据指定的Pod选择器来监控当前Pod副本的数量，如果副本数量小于期望值，则根据Pod模板来创建新的Pod。一般用标签作为Pod的选择器。目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。 它通常用来保证给定数量的、完全相同的 Pod 的可用性。建议使用 Deployment 而不是直接使用 ReplicaSet，除非需要自定义更新业务流程或根本不需要更新。这实际上意味着，可能永远不需要操作 ReplicaSet 对象，而是使用 Deployment，并在 spec 部分定义应用。 部署（Deployment） 部署在副本集上提供对Pod的更新功能，每个部署都有对应的副本集。当部署中创建的Pod模板发生变化时，这些Pod都需要被更新。在更新时，k8s会创建一个新的副本集来管理新的Pod，当更新完后，旧的副本集会被删除。部署可以采用不同的策略来更新这些Pod，最常用的策略是滚动更新，它可以保证在更新过程中服务不间断。在进行滚动更新时，k8s首先会创建新的副本集中的Pod，再删除已有副本集中的Pod。这个过程是交替进行的 ，在没有足够数量的新Pod运行之前，不会删除已有的Pod。同样的，在没有足够数量的已有Pod被删除之前，不会创建新的Pod。控制器为 Pods 和 ReplicaSets 提供声明式的更新能力。负责描述 Deployment 中的目标状态，而 Deployment 控制器以受控速率更改实际状态， 使其变为期望状态。可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment， 并通过新的 Deployment 收养其资源。 有状态集（StatefulSet） 副本集中的 Pod 没有标识符，不能进行区分，它唯一保证的是所包含的 Pod 总数。有些应用对运行的实例有启动顺序和唯一性的要求，典型的例子是通过多个实例组成集群的服务，如 Cassandra 和 RabbitMQ 等。这些应用要求实例有固定且唯一的网络标识，才能正确的建立集群，对于这样的应用，应该使用有状态集来管理。有状态集中的 Pod 都有唯一不变的标识符，这个标识符由有状态集的名称和 Pod 的序号组成。如果有状态集的名称是 cassandra，同时期望的 Pod 副本数量是 3，那么 Pod 的名称分别是 cassandra-0、cassandra-1 和 cassandra-2。有状态集需要一个对应的服务来提供 Pod 的 DNS 名称。有状态集在创建和扩展时有特殊的限制，如果一个有状态集期望的 Pod 副本数量是 N，那么有状态集会从 0 开始依次创建这些 Pod，在第 1 个 Pod 正常运行之前，不会创建第 2 个Pod；在删除 Pod 时，则是从第 N 个 Pod 开始反向依次删除。是用来管理有状态应用的工作负载 API 对象。StatefulSet 用来管理某 Pod 集合的部署和扩缩， 并为这些 Pod 提供持久存储和持久标识符。StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的， 但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。如果希望使用存储卷为工作负载提供持久存储，可以使用 StatefulSet 作为解决方案的一部分。 尽管 StatefulSet 中的单个 Pod 仍可能出现故障，但持久的 Pod 标识符使得将现有卷与替换已失败 Pod 的新 Pod 相匹配变得更加容易。弊端：当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。 为了实现 StatefulSet 中的 Pod 可以有序地且体面地终止，可以在删除之前将 StatefulSet 缩放为0，在默认 Pod 管理策略(OrderedReady) 时使用滚动更新，可能进入需要人工干预才能修复的损坏状态。 守护程序集（DaemonSet） 守护程序集确保在全部或部分的集群节点上运行 Pod，每个节点上最多运行一个 Pod 副本。如果应用需要执行的任务与节点相关，则应该使用守护程序集，比如收集日志和节点性能指标数据的应用，都应该使用守护程序集来部署。确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。DaemonSet 与 Deployments 非常类似， 它们都能创建 Pod，并且 Pod 中的进程都不希望被终止（例如，Web 服务器、存储服务器）。建议为无状态的服务使用 Deployments，比如前端服务。对这些服务而言，对副本的数量进行扩缩容、平滑升级，比精确控制 Pod 运行在某个主机上要重要得多。 当需要 Pod 副本总是运行在全部或特定主机上，并需要它们先于其他 Pod 启动时， 应该使用 DaemonSet。 服务（Service） 应用在运行时经常需要为其他应用提供服务。在使用控制器管理应用的 Pod 时，Pod 的 IP 地址会随着 Pod 的创建和删除而发生变化。因此，我们不能使用 Pod 的 IP 地址作为访问服务的方式。Kubernetes 中的服务（Service）作为一组 Pod 的抽象，定义了如何访问这些 Pod，它实现了服务发现和负载均衡两项重要的功能。在创建服务时需通过选择器来选择服务所对应的 Pod，应用服务的消费者使用服务的地址来访问 Pod，实际的访问请求会被分发到服务所对应的某个 Pod 上。当 Pod 发生变化时，服务会自动更新所对应的 Pod 列表。 名称 类型 说明 集群IP地址 ClusterIP 服务发布为集群内部的IP地址，只能在集群内部访问 节点端口 NodePort 服务发布在每个节点上的固定端口 负载均衡服务 LoadBalancer 服务发布为可供外部访问的负载均衡服务 外部名称 ExternalName 服务映射成外部名称 存储卷： 容器运行时对内部文件所做的修改是瞬时的，当容器停止之后，相关的修改会丢失。Kubernetes 提供了两个对象，即持久卷和持久卷要求，持久卷可以由管理员手动创建，或者由存储服务动态创建，用户则以持久卷要求的形式来声明所需要使用的持久卷的大小和访问模式；持久卷要求可以与手动创建的持久卷进行绑定，如果需要动态创建，则由存储服务完成创建之后，再进行绑定。一旦完成绑定，持久卷可以用类似卷的方式来访问。 任务和定时任务： 当需要执行一次性任务时，我们可以使用任务对象在创建任务时指定需要运行的 Pod 模板。当需要定期执行任务时，则可以使用定时任务对象，在创建时，需要指定定时任务的 cron 表达式。 配置表（ConfigMap）： 当它运行在 Kubernetes 上时，应该使用配置表（ConfigMap）来管理配置。配置表可以看成是包含名值对的哈希表。 容器运行时（Container Runtime）： 支持多个容器运行环境: Docker、 containerd、cri-o、 rktlet 以及任何实现 Kubernetes CRI (容器运行环境接口)。 Master节点 ControllerManager 集群内各种资源controller的核心管理者 针对每一种具体的资源，都有相应的Controller 保证其下管理的每个Controller所对应的的资源始终处于“期望状态” 作为基本的运行单元，Pod缺少响应的管理功能。如果一个Pod在运行中出现错误，并不会自动创建新的Pod来替代它。如果需要管理Pod，则应该利用k8s提供不同类别的控制器，包括副本集、部署、有状态集合守护进程集。在主节点上运行控制器组件，从逻辑上讲，每个控制器都是一个单独的进程，但是为了减低复杂性，它们都会被编译到同一个可执行文件，并在一个进程中运行。 这些控制器包括： 节点控制器（Node Controller）：负责在节点出现故障时进行通知和响应。 副本控制器（Replication Controller）：负责为系统中的每个副本控制器对象维护正确数量的Pod。 端点控制器（Endpoint Controller）：填充端点对象（即加入Service与Pod）。 服务账户和令牌控制器（Service Account & Token Controllers）：为新的命名空间创建默认账户和API访问令牌。 API Server 集群控制的唯一入口，是提供 Kubernetes 集群控制 RESTful API 的核心组件 集群内各个组件之间数据交互和通信的中枢 提供集群控制的安全机制（身份认证、授权以及admission control） Scheduler 通过API Server的Watch接口监听新建Pod副本信息，并通过调度算法为该Pod选择一个最合适的Node 支持自定义调度算法provider 默认调度算法内置预选策略和优选策略，决策考量资源需求、服务质量、软硬件约束、亲缘性、数据局部性等指标参数 Etcd Kubernetes集群的主数据库，存储着所有资源对象以及状态 默认与Master组件部署在一个Node上 Etcd的数据变更都是通过API Server进行 Node节点 kubernets集群中真正的工作负载节点 Kubelet 位于集群中每个Node上的非容器形式的服务进程组件，Master和Node之间的桥梁 处理Master下发到本Node上的Pod创建、启停等任务管理任务；向API Server注册Node信息 监控本Node上容器和节点资源情况，并定期向Master汇报节点资源占用情况 Kube-proxy Service抽象概念的实现，将到Service的请求按策略（负载均衡）算法分发到后端Pod（Endpoint）上 默认使用iptables mode实现 支持nodeport模式，实现从外部访问集群内的service 容器运行时（Container Runtime）：支持多个容器运行环境: Docker、 containerd、cri-o、 rktlet 以及任何实现 Kubernetes CRI (容器运行环境接口)。 k8s核心组件 组件名 说明 etcd 保存整个集群的状态 apiserver 提供资源操作的唯一入口，并提供认证、授权、访问控制、api注册和发现等机制 controller manager 负责维护集群状态，比如故障检测、自动扩展、滚动更新等 scheduer 负责资源的调度，按照预定的调度策略将pod调度到相应的机器上 kubelet 负责维护容器的生命周期，同时负责volume（CVI）和网络（CNI）的管理，主要负责监视指派到它所在node上的pod，包括增删改监控等 container runtime 负责镜像管理以及pod和容器的真正运行（CRI） kube-proxy 负责为service提供cluster内部的服务发现和负载均衡，负责为pod对象提供代理 其他组件 组件名称 说明 kube-dns 负责为整个集群提供DNS服务 ingress controller 为服务提供外网入口 heapster 提供资源监控 dashboard 提供GUI federation 提供跨可用区的集群 fluentd-elasticsearch 提供集群日志采集、存储与查询 重要概念 Pod 运行于Node节点上，若干相关容器的组合。Pod内包含的容器运行在同一宿主机上，使用相同的网络命名空间、IP地址和端口。Pod是Kubernets最基本的部署调度单元。每个Pod可以由一个或多个业务容器和一个根容器（Pause容器）组成。一个Pod表示某个应用的一个实例。 Pod只提供容器的运行环境并保持容器的运行状态，重启容器不会造成Pod重启。 Pod不会自愈。如果Pod运行的Node故障，或者是调度器本身故障，这个Pod就会被删除。同样的，如果Pod所在Node缺少资源或者Pod处于维护状态，Pod也会被驱逐。Kubernetes使用更高级的称为Controller的抽象层，来管理Pod实例。虽然可以直接使用Pod，但是在Kubernetes中通常是使用Controller来管理Pod的。 Pod中共享的环境包括Linux的namespace、cgroup和其他可能的隔绝环境。 Node Node节点是Kubernetes集群中的工作节点，Node上的负载由Master节点分配，工作负载主要是运行容器应用。 ReplicaSet 是Pod副本的抽象，用于解决Pod的扩容和伸缩 Deployment Deployment表示部署，在内部使用ReplicaSet实现。可以通过Deployment来生成相应的ReplicaSet完成Pod副本的创建。 Replication Controller Replication Controller用来管理Pod的副本，保证集群中存在指定数量的Pod副本。集群中副本的数量大于指定的数量，则会停止指定数量之外的多余容器数量，反之，则会启动少于指定数量个数的容器，保证数量不变。Replication Controller是实现弹性伸缩、动态扩容和滚动升级的核心。 Service Service定义了Pod的逻辑集合和访问该集合的策略，是真实服务的抽象。Service提供了一个统一的服务访问入口以及服务代理和发现机制，关联多个相同Label的Pod，用户不需要了解后台Pod是如何运行。 Label Kubernets中任意API对象都是通过Label进行标识，Label的实质是一系列的Key/Value键值对。其中Key和Value由用户自己指定。Label可以附加在各种资源对象上，如Node、Pod、Service、RC等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上去。Label是Replication Controller和Service运行的基础，两者通过Label来进行关联Node上运行的Pod。 kube-dns 负责为整个集群提供DNS服务 Ingress Conroller 为服务提供外网入口 Heapster 提供资源监控 Dashboard 提供GUI RBAC 基于角色的访问控制 kubernetes有一个很基本的特性就是它的所有资源对象都是模型化的 API 对象，允许执行 CRUD(Create、Read、Update、Delete)操作(也就是我们常说的增、删、改、查操作)，比如下面的这下资源： Pods ConfigMaps Deployments Nodes Secrets Namespaces 上面这些资源对象的可能存在的操作有： create get delete list update edit watch exec 二、网络相关 1、k8s网络特征： 1.每个pod都有一个ip 2.所有pod可以通过ip访问其他pod（不管是否在同一台物理机） 3.Pod内的所有容器共享一个 linux net namespace，pod内的容器，都可以使用localhost来访问pod内的其他容器 4.所有容器可以不用nat转换访问其他容器 5.所有节点都可以不用nat转换与所有容器通讯 6.容器地址和别人看到的地址是同一个地址 2、不同node中pod相互通信需要满足的条件： 1.整个k8s集群中的pod ip不能冲突 2.需要将pod ip和所在node ip关联起来，通过关联可以让pod能互访 1)Node中的docker0的网桥地址不能冲突 2)Pod中的数据在发出时，需要有一个机制知道对方的pod ip在哪个node上 默认docker0网络为172.17.0.0/16的网段，每个容器都在这个子网内获得ip并且将docker0作为网关 2.k8s中，每个node上的docker0都是可以被路由到的，也就是说，在同一个集群内，各个主机都是可以访问其他主机上的pod ip，并不需要在主机上做端口映射。 当启动pod时，同一pod下的所有容器都使用同一个网络命名空间（如：同一个ip），所以必须要使用容器网络的container模式，如果将所有pod中的容器做成一个链式的结构，中间任何一个容器出问题，都会引起连锁反映，所以在每个pod中引入了一个pause，其他容器都链接到这个容器，由pause来负责端口规划和映射。如下图： ​ k8s采用Container Networking Interface(CNI)规范。 目前Kubernetes 支持的网络方案，比如 Flannel、Calico、Canal、Weave Net 等。 3、Flannel（host-gw、vxlan） Flannel是Overlay网络的一种，也是将源数据包封装在另一种网络包里面进行路由转发和通信，目前已经支持UDP、VXLAN、AWS VPC和GCE路由等数据转发方式。Flannel通过给每台宿主机分配一个子网的方式为容器提供虚拟网络，它基于Linux TUN/TAP，使用UDP封装IP包来创建overlay网络，并借助etcd维护网络的分配情况。 它给每个node上的docker容器分配互不冲突的IP地址； 它能给这些IP地址之间建立一个覆盖网络，通过覆盖网络，将数据包原封不动的传递到目标容器内。 4、Calico（IPIP、BGP） 是一个纯3层的数据中心网络方案。 Calico在每一个计算节点利用Linux Kernel实现了一个高效的vRouter来负责数据转发，而每个vRouter通过BGP协议负责把自己上运行的workload的路由信息像整个Calico网络内传播——小规模部署可以直接互联，大规模下可通过指定的BGP route reflector来完成。 Calico节点组网可以直接利用数据中心的网络结构（无论是L2或者L3），不需要额外的NAT，隧道或者Overlay Network。 Calico基于iptables还提供了丰富而灵活的网络Policy，保证通过各个节点上的ACLs来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。 此外，Calico基于iptables还提供了丰富的网络策略，实现了k8s的Network Policy策略，提供容器间网络可达性限制的功能。 Calico的主要组件如下： Felix：Calico Agent，运行在每台Node上，负责为容器设置网络源（IP地址、路由规则、iptables规则等），保证主机容器网络互通。 etcd：Calico使用的存储后端。 BGP Client(BIRD)：负责把Felix在各Node上设置的路由信息通过BGP协议广播到Calico网络。 BGP Route Reflector(BIRD)：通过一个或者多个BGP Route Reflector来完成大规模集群的分级路由分发。 Calicoctl：Calico命令行管理工具。 三、PreStop PreStop钩子： 在Pod的Deployment.yaml配置中，可以定义一个名为PreStop的特殊钩子。这个钩子可以是一个命令或HTTP请求，它会在Pod的容器被终止前执行。 PreStop钩子的主要目的是允许容器在应用终止前执行一些必要的清理工作，比如保存状态、关闭连接等。 当执行PreStop钩子时，Kubernetes会阻塞删除流程，直到钩子执行完成或超时（默认为30秒）。 SIGTERM和SIGKILL信号： 当Pod接收到删除请求时，Kubernetes会首先向Pod发送一个SIGTERM信号。 容器在接收到SIGTERM(143)信号后，应该开始关闭进程、清理资源，并停止接收新的请求。 如果容器在一段时间内（默认为30秒）没有正常退出，Kubernetes会发送一个SIGKILL(137)信号来强制终止容器。 四、探针 1、探针（健康状态监测） 配置存活、就绪和启动探针 Pod的生命周期 1.探针执行方式 livenessProbe: 容器是否正在运行，不为running，则kubelet会杀死容器，根据重启策略进行相应的处理。 readinessProbe： 容器是否准备好，为请求提供服务。是否处于可用Ready状态，达到ready状态表示pod可以接受请求，如果不健康，从service的后端endpoint列表中把pod隔离出去。 两种探针探测失败的方式不同，一个是重启容器，一个是不提供服务 startupProbe: 容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被禁用，直到此探针成功为止。如果启动探测失败，kubelet将杀死容器，而容器根据重启策略进行重启。 2.诊断的三种方式 ExecAction: 在容器内执行指定命令。如果命令退出时返回码为0则认为诊断成功 TCPSocketAction：对指定端口上的容器的IP地址进行TCP检查，如果端口打开，则诊断认为是成功的 HTTPGetAction：对指定的端口和路径上的容器的IP地址执行HTTP Get请求，如果响应状态码大于等于200且小于400，则诊断被认为是成功的 3.Probe详细配置 initialDelaySeconds: 容器启动后第一次执行探测需要等待多少秒 periodSeconds：执行探测的频率。默认10秒，最小1秒 timeoutSeconds：探测超时时间，默认1秒，最小1秒 successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认1,对于liveness必须是1，最小值1 failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败，默认3，最小1 HTTP probe 中可以给httpGet设置其他配置项 4.httpget其他配置项 host：连接的主机名，默认连接到pod的IP。你可能想在http header中设置\"Host\"而不是使用IP。 scheme：连接使用的schema，默认HTTP。 path: 访问的HTTP server的path。 httpHeaders：自定义请求的header。HTTP运行重复的header。 port：访问的容器的端口名字或者端口号。端口号必须介于1和65535之间。 5.示例 livenessProbe: exec: command: [\"cat\",\"/app/index.html\"] livenessProbe: exec: command: [\"cat\", \"/app/index.html\"] # tcpSocket: # port: http initialDelaySeconds: 30 timeoutSeconds: 5 failureThreshold: 6 readinessProbe: exec: command: [\"cat\", \"/app/index.html\"] # tcpSocket: # port: http initialDelaySeconds: 5 timeoutSeconds: 3 periodSeconds: 5 其他 1、参考链接 https://www.qikqiak.com/k8s-book/ 2、理解pod 在探讨pod和容器的区别之前，我们先谈谈为什么k8s会使用pod这个最小单元，而不是使用docker的容器，k8s既然使用了pod，当然有它的理由。 更利于扩展 k8s不仅仅支持Docker容器，也支持rkt甚至用户自定义容器，为什么会有这么多不同的容器呢，因为容器并不是真正的虚拟机，docker的一些概念和误区总结，此外，Kubernetes不依赖于底层某一种具体的规则去实现容器技术，而是通过CRI这个抽象层操作容器，这样就会需要pod这样一个东西，pod内部再管理多个业务上紧密相关的用户业务容器，就会更有利用业务扩展pod而不是扩展容器。 更容易定义一组容器的状态 如果没有使用pod，而是直接使用一组容器去跑一个业务呢，那么当其中一个或者若干个容器出现问题呢，如何去定义这一组容器的状态呢，通过pod这个概念，这个问题就可以很好的解决，一组业务容器跑在一个k8s的pod中，这个pod中会有一个pause容器，这个容器与其他的业务容器都没有关系，以这个pause容器的状态来代表这个pod的状态， 利于容器间文件共享，以及通信。 pause容器有一个ip地址，和一个存储卷，pod中的其他容器共享pause容器的ip地址和存储，这样就做到了文件共享和互信。 pod和容器的区别 总结，pod是k8s的最小单元，容器包含在pod中，一个pod中有一个pause容器和若干个业务容器，而容器就是单独的一个容器，简而言之，pod是一组容器，而容器单指一个容器。 3、容器与虚拟机对比 虚拟机就像房子，容器就像胶囊公寓。 容器：容器本质是基于镜像的跨环境迁移。 容器两个特性：1、封装（配置、文件路径、权限）2、标准（无论什么容器运行环境，将同样的镜像运行起来，都可以还原当时的那一刻） 容器优势： 环境依赖更少 更轻量（虚拟机镜像大几十G上百G，容器镜像一般不到1G） 好移动（因为轻量所以迁移会更快，虚拟机不适合跨环境迁移） 启动快（没有内核、镜像小）、可以回滚、可以弹性。适合部署无状态的服务，随时重启都可以（弊端：随意重启有状态服务，不知丢失数据是什么） 适合于微服务（变化大、流量大）（进程多、更新快） 利于CI/CD，快速部署、用以轻松建立、维护、扩容和滚动更新应用程序。 容器缺点： 隔离性不佳 共享kernel VM优势： 隔离全面 自带kernel VM缺点： 镜像比较大、笨重 对于跨环境迁移能力不足 物理资源损耗更大 推荐使用容器的场景：部署无状态的服务，同虚拟机互补使用，实现更好的隔离性 如部署有状态的服务，需要对应用有一定的了解（做好health check和容错） 持续集成的场景，可以顺利在开发、测试、生产之间迁移 适合部署跨云、跨域、跨数据中心的混合云场景下的应用部署和弹性伸缩 跨操作系统的场景 应用经常变更和迭代的场景 "},"notes/docker/k8s_yaml.html":{"url":"notes/docker/k8s_yaml.html","title":"k8s 常用 yaml","keywords":"","body":"K8S Yaml test cat Deployment apiVersion: apps/v1 kind: Deployment metadata: name: test-deployment namespace: test-deployment labels: key: val spec: replicas: 1 selector: matchLabels: app: test-deployment template: metadata: labels: app: test-deployment test: \"true\" spec: hostAliases: - hostnames: - test.aa.com - test.bb.com ip: 192.168.0.127 - hostnames: - test.cc.com ip: 192.168.0.128 hostNetwork: true tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" - operator: \"Exists\" effect: \"NoSchedule\" - operator: \"Exists\" effect: \"NoExecute\" schedulerName: default-scheduler nodeSelector: resourceGroup: test affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: \"test\" operator: In values: - \"true\" topologyKey: \"kubernetes.io/hostname\" restartPolicy: Always # Never terminationGracePeriodSeconds: 30 containers: - name: test-deployment image: docker.io/library/nginx:1.21.3 imagePullPolicy: IfNotPresent ports: - containerPort: 22 name: ssh protocol: TCP securityContext: runAsUser: 0 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File env: - name: http_proxy value: http://xxxx:3120 command: - nginx -g daemon off; resources: requests: cpu: 200m memory: 128Mi limits: cpu: \"8\" memory: 20Gi nvidia.com/gpu: \"1\" livenessProbe: httpGet: path: /healthz port: 80 scheme: HTTPS livenessProbe: tcpSocket: port: 53 initialDelaySeconds: 5 periodSeconds: 10 volumeMounts: - name: config-volume mountPath: /etc/kubernetes - name: dnsmasq-config mountPath: /etc/dnsmasq.conf subPath: dnsmasq.conf serviceAccountName: service-account-sa volumes: - name: config-volume configMap: name: config-demo - name: dnsmasq-config configMap: name: dnsmasq-config StatefulSet apiVersion: apps/v1 kind: StatefulSet metadata: annotations: meta.helm.sh/release-name: yani meta.helm.sh/release-namespace: default labels: app.kubernetes.io/instance: yani app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: template helm.sh/chart: template-1.0.0 taskId: yani name: yani namespace: default spec: podManagementPolicy: OrderedReady replicas: 1 selector: matchLabels: app.kubernetes.io/instance: yani app.kubernetes.io/name: template serviceName: yani template: metadata: creationTimestamp: null labels: app.kubernetes.io/instance: yani app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: template helm.sh/chart: template-1.0.0 spec: containers: - command: - /bin/sh - -c - /script/start-script.sh image: docker.io/gradiant/jupyter:6.0.3 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: / port: http scheme: HTTP initialDelaySeconds: 600 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: yani ports: - containerPort: 8888 name: http protocol: TCP readinessProbe: failureThreshold: 5 httpGet: path: / port: http scheme: HTTP initialDelaySeconds: 30 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi securityContext: runAsUser: 1001 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /script name: start-script - mountPath: /home/jovyan name: jupyter-pvc dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: fsGroup: 1001 terminationGracePeriodSeconds: 30 volumes: - configMap: defaultMode: 0777 name: yani-start-script name: start-script - emptyDir: {} name: jupyter-pvc Job apiVersion: batch/v1 kind: Job metadata: name: hello-job spec: activeDeadlineSeconds: 60 # 设置Job的超时时间为60秒 template: spec: containers: - name: hello-job-container image: busybox command: [\"echo\", \"Hello, Kubernetes Job!\"] # 如果你想让容器保持运行一段时间（比如模拟长时间运行的任务）， # 可以使用下面的命令替换上面的echo命令。但请注意，这通常不是Job的预期用途。 # command: [\"sh\", \"-c\", \"echo Hello, Kubernetes Job! && sleep 30\"] restartPolicy: Never backoffLimit: 0 # 设置为0表示不允许重试，根据你的需求调整 CronJob apiVersion: batch/v1beta1 kind: CronJob metadata: name: my-cronjob spec: schedule: \"*/1 * * * *\" # 每分钟的每一秒执行一次（注意：CronJob的精度通常到分钟，这里仅为示例） jobTemplate: spec: template: spec: containers: - name: my-container image: busybox command: [\"echo\", \"$(date) Hello from the Kubernetes cron job\"] restartPolicy: OnFailure successfulJobsHistoryLimit: 3 failedJobsHistoryLimit: 1 kubectl apply -f - schedule: \"0 8-9 \" # 每天的 8:00 AM 和 9:00 AM 执行 schedule: \"0,59 8-10 \" # 每小时的第 0 分钟和第 59 分钟，执行一次 ConfigMap apiVersion: v1 kind: ConfigMap metadata: name: config-demo namespace: kube-system data: scheduler-config.yaml: | apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: d-scheduler - schedulerName: image-locality-scheduler apiVersion: v1 kind: ConfigMap metadata: name: {{ include \"common.names.fullname\" . }}-start-script namespace: {{ .Release.Namespace | quote }} labels: {{- include \"common.labels.standard\" . | nindent 4 }} {{- if .Values.commonLabels }} {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.commonLabels \"context\" $ ) | nindent 4 }} {{- end }} {{- if .Values.commonAnnotations }} annotations: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.commonAnnotations \"context\" $ ) | nindent 4 }} {{- end }} data: start-script.sh: | {{- .Values.script | nindent 4 }} script: | #!/bin/sh echo \"hl\" > /tmp/yani ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-role-crb roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-role-cr subjects: - kind: ServiceAccount name: service-account-sa namespace: kube-system ClusterRole apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-role-cr rules: - apiGroups: [\"\"] resources: - pods - pods/logs - pods/status - pods/binding - bindings - nodes - events - services - namespaces - configmaps - secrets - serviceaccounts - resourcequotas - replicationcontrollers - persistentvolumes - persistentvolumeclaims verbs: - get - list - watch - create - update - patch - delete - apiGroups: [\"apps\"] resources: - replicasets - statefulsets - deployments - daemonsets verbs: - get - list - watch - create - update - patch - delete - apiGroups: [\"storage.k8s.io\"] resources: - storageclasses - volumeattachments - csinodes - csidrivers - csistoragecapacities verbs: - get - list - watch - apiGroups: [\"policy\"] resources: - poddisruptionbudgets verbs: - get - list - watch - apiGroups: [\"k8s.io\", \"events.k8s.io\"] resources: - priorityclasses - events verbs: - get - list - watch - create - update - patch - delete - apiGroups: [\"node\"] resources: - runtimeclasses verbs: - get - list - watch - apiGroups: [\"coordination.k8s.io\"] resources: - leases verbs: - get - list - watch - create - update - patch ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: service-account-sa namespace: kube-system daemonset apiVersion: apps/v1 kind: DaemonSet metadata: name: dnsmasq namespace: kube-system spec: selector: matchLabels: dns: dnsmasq template: metadata: labels: dns: dnsmasq spec: tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" - operator: \"Exists\" effect: \"NoSchedule\" - operator: \"Exists\" effect: \"NoExecute\" containers: - name: dnsmasq image: easzlab.io.local:5000/busybox:1.28 command: [\"/bin/sh\", \"-c\"] args: - | cat /opt/resolv.conf > /etc/resolv.conf tail -f /dev/null volumeMounts: - name: resolv-host mountPath: /etc/resolv.conf - name: resolv-config mountPath: /tmp/resolv.conf subPath: resolv.conf volumes: - name: resolv-host hostPath: path: /etc/resolv.conf type: FileOrCreate - name: resolv-config configMap: name: resolv-config Service apiVersion: v1 kind: Service metadata: annotations: meta.helm.sh/release-name: ingress-apisix meta.helm.sh/release-namespace: ingress-apisix labels: app.kubernetes.io/component: etcd app.kubernetes.io/instance: ingress-apisix app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: etcd app.kubernetes.io/version: 3.5.10 helm.sh/chart: etcd-9.7.3 name: apisix-etcd namespace: ingress-apisix spec: clusterIP: 10.127.15.211 clusterIPs: - 10.127.15.211 internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: client port: 2379 protocol: TCP targetPort: client - name: peer port: 2380 protocol: TCP targetPort: peer selector: app.kubernetes.io/component: etcd app.kubernetes.io/instance: ingress-apisix app.kubernetes.io/name: etcd sessionAffinity: None type: ClusterIP {{- $serviceType := .Values.service.type -}} apiVersion: v1 kind: Service metadata: name: {{ include \"common.names.fullname\" . }} namespace: {{ .Release.Namespace | quote }} labels: {{- include \"common.labels.standard\" . | nindent 4 }} {{- if .Values.commonLabels }} {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.commonLabels \"context\" $ ) | nindent 4 }} {{- end }} {{- if .Values.commonAnnotations }} annotations: {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.commonAnnotations \"context\" $ ) | nindent 4 }} {{- end }} spec: type: {{ .Values.service.type }} sessionAffinity: {{ default \"None\" .Values.service.sessionAffinity }} {{- if and .Values.service.clusterIP (eq .Values.service.type \"ClusterIP\") }} clusterIP: {{ .Values.service.clusterIP }} {{- end }} {{- if and .Values.service.loadBalancerIP (eq .Values.service.type \"LoadBalancer\") }} loadBalancerIP: {{ .Values.service.loadBalancerIP }} {{- end }} {{- if and (eq .Values.service.type \"LoadBalancer\") .Values.service.loadBalancerSourceRanges }} loadBalancerSourceRanges: {{- toYaml .Values.service.loadBalancerSourceRanges | nindent 4 }} {{- end }} {{- if or (eq .Values.service.type \"LoadBalancer\") (eq .Values.service.type \"NodePort\") }} externalTrafficPolicy: {{ .Values.service.externalTrafficPolicy | quote }} {{- end }} ports: {{- range .Values.service.ports }} - name: {{ .name }} port: {{ .port }} targetPort: {{ .targetPort }} {{- if and (or (eq $serviceType \"NodePort\") (eq $serviceType \"LoadBalancer\")) (not (empty .nodePort)) }} nodePort: {{ .nodePort }} {{- end }} {{- end }} selector: {{- include \"common.labels.matchLabels\" . | nindent 4 }} service: type: ClusterIP # clusterIP LoadBalancer NodePort ports: - name: http port: 80 targetPort: http Ingress apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: meta.helm.sh/release-name: yani generation: 1 labels: app.kubernetes.io/instance: yani name: yani namespace: default spec: ingressClassName: nginx rules: - host: hl.test.com http: paths: - backend: service: name: yani port: number: 80 path: / pathType: Prefix {{- if .Values.ingress.enabled -}} {{- $fullName := include \"common.names.fullname\" . -}} {{- $httpPort := .Values.service.port -}} {{- $pathType := .Values.ingress.pathType -}} apiVersion: {{ include \"common.capabilities.ingress.apiVersion\" . }} kind: Ingress metadata: name: {{ .Release.Name }} namespace: {{ .Release.Namespace | quote }} labels: {{- include \"common.labels.standard\" . | nindent 4 }} {{- if .Values.commonLabels }} {{- include \"common.tplvalues.render\" ( dict \"value\" .Values.commonLabels \"context\" $ ) | nindent 4 }} {{- end }} spec: {{- if .Values.ingress.ingressClassName }} ingressClassName: {{ .Values.ingress.ingressClassName | quote }} {{- end }} rules: {{- range .Values.ingress.hosts }} - host: {{ .host }} http: paths: {{- range .paths }} - path: {{ default \"/\" .path }} pathType: {{ default \"Prefix\" $pathType }} backend: service: name: {{ $fullName }} port: number: {{ .port | default $httpPort }} {{- end }} {{- end }} {{- end }} ingress: enabled: true ingressClassName: nginx pathType: Prefix apiVersion: \"networking.k8s.io/v1\" hosts: - host: hl.test.com paths: - path: / port: 80 pod apiVersion: v1 kind: Pod metadata: name: test-pod spec: schedulerName: default-scheduler containers: - name: test-pod image: docker.io/library/nginx:1.21.3 Secret kubectl config view --raw > kubeconfig.yaml cat kubeconfig.yaml | base64 kubectl config view --raw|base64 kubectl apply -f - ServiceMonitor apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: annotations: meta.helm.sh/release-name: ingress-apisix meta.helm.sh/release-namespace: ingress-apisix labels: app.kubernetes.io/instance: ingress-apisix app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: apisix app.kubernetes.io/version: 3.9.1 helm.sh/chart: apisix-2.8.1 release: ai-kube-prometheus-stack name: apisix namespace: ingress-apisix spec: endpoints: - interval: 15s path: /apisix/prometheus/metrics scheme: http targetPort: prometheus namespaceSelector: matchNames: - ingress-apisix selector: matchLabels: app.kubernetes.io/instance: ingress-apisix app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: apisix app.kubernetes.io/service: apisix-prometheus-metrics app.kubernetes.io/version: 3.9.1 helm.sh/chart: apisix-2.8.1 PodMonitor apiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: labels: app: apisix-etcd app.kubernetes.io/instance: ingress-apisix app.kubernetes.io/name: etcd app.kubernetes.io/version: 3.5.10 release: ai-kube-prometheus-stack name: apisix-etcd namespace: monitoring spec: podMetricsEndpoints: - interval: 15s path: /metrics scheme: http port: client namespaceSelector: matchNames: - ingress-apisix selector: matchLabels: app.kubernetes.io/instance: ingress-apisix app.kubernetes.io/name: etcd app.kubernetes.io/component: etcd Namespace apiVersion: v1 kind: Namespace metadata: name: aaa api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND bindings v1 true Binding componentstatuses cs v1 false ComponentStatus configmaps cm v1 true ConfigMap endpoints ep v1 true Endpoints events ev v1 true Event limitranges limits v1 true LimitRange namespaces ns v1 false Namespace nodes no v1 false Node persistentvolumeclaims pvc v1 true PersistentVolumeClaim persistentvolumes pv v1 false PersistentVolume pods po v1 true Pod podtemplates v1 true PodTemplate replicationcontrollers rc v1 true ReplicationController resourcequotas quota v1 true ResourceQuota secrets v1 true Secret serviceaccounts sa v1 true ServiceAccount services svc v1 true Service mutatingwebhookconfigurations admissionregistration.k8s.io/v1 false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io/v1 false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io/v1 false CustomResourceDefinition apiservices apiregistration.k8s.io/v1 false APIService apisixclusterconfigs acc apisix.apache.org/v2 false ApisixClusterConfig apisixconsumers ac apisix.apache.org/v2 true ApisixConsumer apisixglobalrules agr apisix.apache.org/v2 true ApisixGlobalRule apisixpluginconfigs apc apisix.apache.org/v2 true ApisixPluginConfig apisixroutes ar apisix.apache.org/v2 true ApisixRoute apisixtlses atls apisix.apache.org/v2 true ApisixTls apisixupstreams au apisix.apache.org/v2 true ApisixUpstream controllerrevisions apps/v1 true ControllerRevision daemonsets ds apps/v1 true DaemonSet deployments deploy apps/v1 true Deployment replicasets rs apps/v1 true ReplicaSet statefulsets sts apps/v1 true StatefulSet tokenreviews authentication.k8s.io/v1 false TokenReview localsubjectaccessreviews authorization.k8s.io/v1 true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io/v1 false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io/v1 false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io/v1 false SubjectAccessReview horizontalpodautoscalers hpa autoscaling/v2 true HorizontalPodAutoscaler cronjobs cj batch/v1 true CronJob jobs batch/v1 true Job certificatesigningrequests csr certificates.k8s.io/v1 false CertificateSigningRequest leases coordination.k8s.io/v1 true Lease endpointslices discovery.k8s.io/v1 true EndpointSlice events ev events.k8s.io/v1 true Event flowschemas flowcontrol.apiserver.k8s.io/v1beta3 false FlowSchema prioritylevelconfigurations flowcontrol.apiserver.k8s.io/v1beta3 false PriorityLevelConfiguration nodes metrics.k8s.io/v1beta1 false NodeMetrics pods metrics.k8s.io/v1beta1 true PodMetrics alertmanagerconfigs amcfg monitoring.coreos.com/v1alpha1 true AlertmanagerConfig alertmanagers am monitoring.coreos.com/v1 true Alertmanager podmonitors pmon monitoring.coreos.com/v1 true PodMonitor probes prb monitoring.coreos.com/v1 true Probe prometheusagents promagent monitoring.coreos.com/v1alpha1 true PrometheusAgent prometheuses prom monitoring.coreos.com/v1 true Prometheus prometheusrules promrule monitoring.coreos.com/v1 true PrometheusRule scrapeconfigs scfg monitoring.coreos.com/v1alpha1 true ScrapeConfig servicemonitors smon monitoring.coreos.com/v1 true ServiceMonitor thanosrulers ruler monitoring.coreos.com/v1 true ThanosRuler ingressclasses networking.k8s.io/v1 false IngressClass ingresses ing networking.k8s.io/v1 true Ingress networkpolicies netpol networking.k8s.io/v1 true NetworkPolicy runtimeclasses node.k8s.io/v1 false RuntimeClass poddisruptionbudgets pdb policy/v1 true PodDisruptionBudget clusterrolebindings rbac.authorization.k8s.io/v1 false ClusterRoleBinding clusterroles rbac.authorization.k8s.io/v1 false ClusterRole rolebindings rbac.authorization.k8s.io/v1 true RoleBinding roles rbac.authorization.k8s.io/v1 true Role priorityclasses pc scheduling.k8s.io/v1 false PriorityClass csidrivers storage.k8s.io/v1 false CSIDriver csinodes storage.k8s.io/v1 false CSINode csistoragecapacities storage.k8s.io/v1 true CSIStorageCapacity storageclasses sc storage.k8s.io/v1 false StorageClass volumeattachments storage.k8s.io/v1 false VolumeAttachment hostAliases cat Demo apiVersion: apps/v1 kind: DaemonSet metadata: name: apisix-etcd-monitor namespace: ingress-apisix spec: selector: matchLabels: app: apisix-etcd-monitor template: metadata: labels: app: apisix-etcd-monitor spec: hostNetwork: true nodeSelector: monitor: apisix-etcd containers: - name: apisix-etcd-monitor image: easzlab.io.local:5000/alpine:3.10.4 ports: - name: metrics containerPort: 3379 hostPort: 3379 command: - sh - -c - tail -f /dev/null --- apiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: labels: app: apisix-etcd-monitor release: ai-kube-prometheus-stack name: apisix-etcd-monitor namespace: monitoring spec: podMetricsEndpoints: - interval: 15s path: /metrics scheme: http port: metrics namespaceSelector: matchNames: - ingress-apisix selector: matchLabels: app: apisix-etcd-monitor apiVersion: apps/v1 kind: DaemonSet metadata: name: apisix-etcd-monitor namespace: ingress-apisix spec: selector: matchLabels: app: apisix-etcd-monitor template: metadata: labels: app: apisix-etcd-monitor spec: hostNetwork: true nodeSelector: monitor: apisix-etcd containers: - name: apisix-etcd-monitor image: easzlab.io.local:5000/alpine:3.10.4 ports: - name: metrics containerPort: 3379 hostPort: 3379 command: - sh - -c - tail -f /dev/null --- apiVersion: v1 kind: Service metadata: name: apisix-etcd-monitor namespace: ingress-apisix labels: app: apisix-etcd-monitor-svc spec: selector: app: apisix-etcd-monitor ports: - name: metrics protocol: TCP port: 3379 targetPort: metrics --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: release: ai-kube-prometheus-stack name: apisix-etcd-monitor namespace: ingress-apisix spec: endpoints: - interval: 15s path: /metrics scheme: http targetPort: metrics namespaceSelector: matchNames: - ingress-apisix selector: matchLabels: app: apisix-etcd-monitor-svc j2 indent=0 指定转换时不增加额外的缩进。 indent(2)：在生成的 YAML 内容前面添加 2 个空格的缩进，以便在嵌套上下文中正确对齐。 # vars/main.yaml dnsmasq_replica_count: \"{{ groups['dnsmasq'] | length }}\" resources: requests: cpu: 200m memory: 128Mi limits: cpu: 4 memory: 4Gi dnsmasq_global_conf: | no-hosts no-resolv no-poll neg-ttl=300 min-cache-ttl=300 dns-forward-max=10000 cache-size=100000 edns-packet-max=1232 log-facility=/var/log/dnsmasq.log all-servers server=114.114.114.114 server=8.8.8.8 dnsmasq_address_conf: | address=/image.ac.com/127.0.0.1 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" - operator: \"Exists\" effect: \"NoSchedule\" - operator: \"Exists\" effect: \"NoExecute\" affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: \"{{ ingress_label }}\" operator: In values: - \"true\" topologyKey: \"kubernetes.io/hostname\" config: worker-processes: \"auto\" worker-cpu-affinity: \"auto\" max-worker-connections: \"16384\" # value.yaml.j2 image: repository: {{ dnsmasq_image }} pullPolicy: IfNotPresent name: {{ dnsmasq_name }} namespace: {{ dnsmasq_namespace }} replicaCount: {{ dnsmasq_replica_count}} podLabel: {{ dnsmasq_label }} resources: {{ resources | to_nice_yaml(indent=2) }} dnsmasqConfig: | # 字符串 {{ dnsmasq_global_conf | to_nice_yaml | from_yaml | indent(2) }} {{ dnsmasq_address_conf | to_nice_yaml | from_yaml | indent(2) }} config: {{ config | to_nice_yaml(indent=0) | indent(2) }} affinity: {{ affinity | to_nice_yaml(indent=2) }} tolerations: {{ tolerations | to_nice_yaml(indent=0) }} # resolv.conf.j2 {% for item in groups['dnsmasq'] %} nameserver {{ item }} {% endfor %} options timeout:1 attempts:3 rotate nodeSelector: ingress: \"true\" nodeSelector: {{ ingress_label }}: \"true\" tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule operator: Exists - effect: NoExecute operator: Exists # indent=0 指定转换时不增加额外的缩进。 # indent(2)：在生成的 YAML 内容前面添加 2 个空格的缩进，以便在嵌套上下文中正确对齐。 tolerations: {{ tolerations | to_nice_yaml(indent=0) | indent(2) }} ansible # file/chart/templates/test.yaml ... resources: {{- toYaml .Values.resources | nindent 10 }} ... # file/chart/templates/cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: dnsmasq-config namespace: {{ .Values.namespace }} data: dnsmasq.conf: | {{- .Values.dnsmasqConfig | nindent 4 }} # task/main.yaml - name: install import_tasks: helm-install.yml vars: version: \"{{ version }}\" name: \"{{ iname }}\" - name: 轮询等待 svc 运行 wait_for: host: \"{{ groups['ingress'][0] }}\" port: \"{{ ingress_nodePort }}\" delay: 10 timeout: 180 - name: get svc ip shell: \"{{ kubectl }} get svc test -o jsonpath='{.spec.clusterIP}'\" register: test_ip - name: echo stdout shell: cmd: | echo {{ test_ip.stdout }} delegate_to: \"{{ item }}\" loop: \"{{ groups['ingress'] }}\" when: etcd.external | default(false) != true - name: clean import_tasks: clean.yml when: \"'clean' in ansible_run_tags\" tags: - clean "},"notes/docker/k8s_deploy.html":{"url":"notes/docker/k8s_deploy.html","title":"k8s 安装部署","keywords":"","body":"k8s 安装部署 一、v2.13.2 1、v2.13.2 git checkout v2.13.2 2、main.yml vim /root/kubespray/roles/download/defaults/main.yml kube_version: v1.17.7 etcd_version: v3.3.12 gcr_image_repo: \"local.com/k8s/gcr.io\" kube_image_repo: \"local.com/k8s/k8s.gcr.io\" docker_image_repo: \"local.com/k8s\" quay_image_repo: \"local.com/k8s/quay.io\" calico_version: \"v3.13.2\" calico_ctl_version: \"v3.13.2\" calico_cni_version: \"v3.13.2\" calico_policy_version: \"v3.13.2\" calico_typha_version: \"v3.13.2\" typha_enabled: false flannel_version: \"v0.12.0\" cni_version: \"v0.8.6\" kubelet_download_url: \"http://192.168.0.242:88/kubelet\" kubectl_download_url: \"http://192.168.0.242:88/kubectl\" kubeadm_download_url: \"http://192.168.0.242:88/kubeadm\" etcd_download_url: \"http://192.168.0.242:88/etcd-v3.3.12-linux-amd64.tar.gz\" cni_download_url: \"http://192.168.0.242:88/cni-plugins-linux-amd64-v0.8.6.tgz\" calicoctl_download_url: \"http://192.168.0.242:88/calicoctl-linux-amd64\" crictl_download_url: \"http://192.168.0.242:88/crictl-v1.18.0-linux-amd64.tar.gz\" 3、k8s-cluster.yml vim /root/kubespray/inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml kube_version: v1.18.4 kube_image_repo: \"local.com/k8s/k8s.gcr.io\" 4、hosts vim /etc/hosts 192.168.0.242 local.com 5、certs.d mkdir /etc/docker/certs.d/local.com 二、使用kubespray安装(kubespray: v2.11.0, k8s:v1.15.3) 1、查看系统是否支持虚拟化 egrep --color 'vmx|svm' /proc/cpuinfo 2、关闭和禁用防火墙 systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config 3、禁用交换分区 swapoff -a sed -i 's/.*swap.*/#&/' /etc/fstab 4、每个节点分别安装docker docker安装 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun systemctl start docker systemctl enable docker 5、免密 ssh-keygen ssh-copy-id HOSTNAME 6、安装python3 yum install -y epel-release yum makecache fast yum install -y python36 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF setenforce 0 yum install -y kubelet kubeadm kubectl 7、每个节点分别下载kubespray源码 git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray git tag git checkout v2.11.0 8、每个节点分别安装相关依赖 pip3 install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 9、复制配置文件 cp -rfp inventory/sample inventory/mycluster 10、更新配置文件信息 申明数组，输入节点ip declare -a IPS=(10.10.1.2 10.10.1.3 10.10.1.4) 将ip写入脚本 CONFIG_FILE=inventory/mycluster/hosts.yml python3 contrib/inventory_builder/inventory.py ${IPS[@]} 11、检查配置文件是否有问题 cat inventory/mycluster/group_vars/all/all.yml cat inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml kube_version: v1.15.3 12、修改镜像源为国内源 azk8s.cn 支持镜像转换列表（可选） global proxy in China format example dockerhub (docker.io) dockerhub.azk8s.cn dockerhub.azk8s.cn//: dockerhub.azk8s.cn/microsoft/azure-cli:2.0.61 dockerhub.azk8s.cn/library/nginx:1.15 gcr.io gcr.azk8s.cn gcr.azk8s.cn//: gcr.azk8s.cn/google_containers/hyperkube-amd64:v1.13.5 quay.io quay.azk8s.cn quay.azk8s.cn//: quay.azk8s.cn/deis/go-dev:v1.10.0 grc_image_files=( ./roles/download/defaults/main.yml ./inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml ) for file in ${grc_image_files[@]};do sed -i 's#gcr.io/google_containers#registry.cn-hangzhou.aliyuncs.com/kbspray#g' $file sed -i 's#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/kbspray#g' $file sed -i 's#gcr.io/google-containers#registry.cn-hangzhou.aliyuncs.com/kbspray#g' $file sed -i 's#quay.io/coreos#registry.cn-hangzhou.aliyuncs.com/kbspray#g' $file done 以下文件看情况需要翻墙下载（可以略过） cd /tmp/releases https://storage.googleapis.com/kubernetes-release/release/v1.14.3/bin/linux/amd64/kubeadm https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz https://storage.googleapis.com/kubernetes-release/release/v1.14.3/bin/linux/amd64/hyperkube https://github.com/projectcalico/calicoctl/releases/download/v3.4.4/calicoctl-linux-amd64 13、如果需要可以配置 docker 代理（可使用 privoxy），需要下载的容器镜像 配置docker代理 vim /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\"HTTP_PROXY=http://proxy.server:port\" Environment=\"HTTPS_PROXY=http://proxy.server:port\" Environment=\"NO_PROXY=localhost,127.0.0.1\" systemctl daemon-reload systemctl restart docker 需要下载的镜像 gcr.io/google-containers/kube-proxy gcr.io/google-containers/kube-apiserver gcr.io/google-containers/kube-controller-manager gcr.io/google-containers/kube-scheduler coredns/coredns calico/node calico/cni calico/kube-controllers k8s.gcr.io/k8s-dns-node-cache nginx k8s.gcr.io/cluster-proportional-autoscaler-amd64 gcr.io/google_containers/kubernetes-dashboard-amd64 quay.io/coreos/etcd gcr.io/google-containers/pause gcr.io/google_containers/pause-amd64 14、开始部署 https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml 15、获取集群信息 kubectl cluster-info Kubernetes master is running at https://192.168.21.88:6443 coredns is running at https://192.168.21.88:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy kubernetes-dashboard is running at https://192.168.21.88:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 16、创建 dashboard 用户 https://github.com/kubernetes/dashboard/wiki/Creating-sample-user # cat dashboard-admin-user.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system kubectl apply -f dashboard-admin-user.yaml kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') 17、安装nginx测试 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm spec: replicas: 3 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP type: NodePort selector: name: nginx kubectl apply -f nginx.yaml kubectl get service | grep nginx 18、清理环境 ansible-playbook -i inventory/mycluster/hosts.yml reset.yml rm -rf /etc/kubernetes/ rm -rf /var/lib/kubelet rm -rf /var/lib/etcd rm -rf /usr/local/bin/kubectl rm -rf /etc/systemd/system/kubelet.service systemctl stop etcd.service systemctl disable etcd.service docker stop $(docker ps -q) docker rm $(docker ps -qa) systemctl restart docker 19、升级 https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md 升级kube git fetch origin git checkout origin/master ansible-playbook upgrade-cluster.yml -b -i inventory/sample/hosts.ini -e kube_version=v1.6.0 升级docker ansible-playbook -b -i inventory/sample/hosts.ini cluster.yml --tags=docker 升级etcd ansible-playbook -b -i inventory/sample/hosts.ini cluster.yml --tags=etcd 升级kubelet ansible-playbook -b -i inventory/sample/hosts.ini cluster.yml --tags=node --skip-tags=k8s-gen-certs,k8s-gen-tokens 升级网络插件 ansible-playbook -b -i inventory/sample/hosts.ini cluster.yml --tags=network 升级所有的add-ones ansible-playbook -b -i inventory/sample/hosts.ini cluster.yml --tags=apps 只升级helm(假定helm_enabled配置为true) ansible-playbook -b -i inventory/sample/hosts.ini cluster.yml --tags=helm 升级master组件 ansible-playbook -b -i inventory/sample/hosts.ini cluster.yml --tags=master 20、添加、减少节点 ansible-playbook -i inventory/mycluster/hosts.yml scale.yml -b -v \\ --private-key=~/.ssh/private_key ansible-playbook -i inventory/mycluster/hosts.yml remove-node.yml -b -v \\ --private-key=~/.ssh/private_key \\ --extra-vars \"node=nodename,nodename2\" 21、其他 journalctl -u -f kubectl edit service -n kube-system kubernetes-dashboard google_containers #查看部署组件 kubectl get all --namespace=kube-system kubectl logs --namespace=kube-system kubernetes-dashboard-7bf56bf786-4pwpj --follow 1 node(s) had taints that the pod didn't tolerate. 有时候一个pod创建之后一直是pending，没有日志，也没有pull镜像，describe的时候发现里面有一句话： 1 node(s) had taints that the pod didn't tolerate. 直译意思是节点有了污点无法容忍，执行 kubectl get no -o yaml | grep taint -A 5 之后发现该节点是不可调度的。这是因为kubernetes出于安全考虑默认情况下无法在master节点上部署pod，于是用下面方法解决： kubectl taint nodes --all node-role.kubernetes.io/master- kubectl api-resources\\ kubectl explain XXX 三、使用Minikube安装（适合单机部署开发） 安装kubenets版本v1.15 1、查看系统是否支持虚拟化 egrep --color 'vmx|svm' /proc/cpuinfo 2、关闭和禁用防火墙 systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config 3、禁用交换分区 swapoff -a sed -i 's/.*swap.*/#&/' /etc/fstab 4、安装minikube curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && mv minikube /usr/local/bin/ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube install minikube /usr/local/bin 5、安装docker docker安装 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun systemctl start docker systemctl enable docker 6、安装kubelet cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum -y makecache fast yum install -y kubectl systemctl start kubelet systemctl enable kubelet 7、不依赖虚拟机模拟启动 执行下面命令会自动安装kubeadm、kubelet两个软件 在拉取镜像时会访问https://k8s.gcr.io/v2/失败，需要翻墙 --image-repository 使用我同步到aliyun仓库的k8s镜像 minikube start --vm-driver=none --kubernetes-version v1.15.0 -v 0 --image-repository registry.cn-hangzhou.aliyuncs.com/hlyani --registry-mirror=https://registry.docker-cn.com * minikube v1.2.0 on linux (amd64) * using image repository registry.cn-hangzhou.aliyuncs.com/ates-k8s * Creating none VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... * Configuring environment for Kubernetes v1.15.0 on Docker 18.09.6 * Downloading kubeadm v1.15.0 * Downloading kubelet v1.15.0 * Pulling images ... * Launching Kubernetes ... * Configuring local host environment ... ! The 'none' driver provides limited isolation and may reduce system security and reliability. ! For more information, see: - https://github.com/kubernetes/minikube/blob/master/docs/vmdriver-none.md ! kubectl and minikube configuration will be stored in /root ! To use kubectl or minikube commands as your own user, you may ! need to relocate them. For example, to overwrite your own settings: - sudo mv /root/.kube /root/.minikube $HOME - sudo chown -R $USER $HOME/.kube $HOME/.minikube * This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true * Verifying: apiserver proxy etcd scheduler controller dns * Done! kubectl is now configured to use \"minikube\" * For best results, install kubectl: https://kubernetes.io/docs/tasks/tools/install-kubectl/ 8、执行下面命令先拉去镜像（可选） kubeadm config images list kubeadm config images pull --kubernetes-version v1.15.0 --image-repository registry.cn-hangzhou.aliyuncs.com/hlyani docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.cn-hangzhou.aliyuncs.com/hlyani/kube-proxy v1.15.0 d235b23c3570 8 days ago 82.4MB registry.cn-hangzhou.aliyuncs.com/hlyani/kube-apiserver v1.15.0 201c7a840312 8 days ago 207MB registry.cn-hangzhou.aliyuncs.com/hlyani/kube-scheduler v1.15.0 2d3813851e87 8 days ago 81.1MB registry.cn-hangzhou.aliyuncs.com/hlyani/kube-controller-manager v1.15.0 8328bb49b652 8 days ago 159MB registry.cn-hangzhou.aliyuncs.com/hlyani/coredns 1.3.1 eb516548c180 5 months ago 40.3MB registry.cn-hangzhou.aliyuncs.com/hlyani/etcd 3.3.10 2c4adeb21b4f 6 months ago 258MB registry.cn-hangzhou.aliyuncs.com/hlyani/pause 3.1 da86e6ba6ca1 18 months ago 742kB 9、查看init-defaults kubeadm config print init-defaults 10、清除部署（可选） minikube delete [preflight] Running pre-flight checks [preflight] Running pre-flight checks [reset] Stopping the kubelet service [reset] Unmounting mounted directories in \"/var/lib/kubelet\" [reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki] [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf] [reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes] The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually. For example: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar) to reset your system's IPVS tables. The reset process does not clean your kubeconfig files and you must remove them manually. Please, check the contents of the $HOME/.kube/config file. 11、让kubectl使用minikube的配置文件 kubectl config use-context minikube 12、dashboard minikube dashboard --url 13、测试，部署nginx kubectl run hello --image=nginx --port=80 kubectl expose deployment hello --type=NodePort #kubectl expose deployment hello --port=80 --type=LoadBalancer kubectl get pod curl $(minikube service hello --url) 14、其他 minikube有一个强大的子命令叫做addons，它可以在kubernetes中安装插件（addon）。这里所说的插件，就是部署在Kubernetes中的Deploymen或者Daemonset。 Kubernetes在设计上有一个特别有意思的地方，就是它的很多扩展功能，甚至于基础功能也可以部署在Kubernetes中，比如网络插件、DNS插件等。安装这些插件的时候， 就是用kubectl命令，直接在kubernetes中部署。 minikube addons list minikube addons disable XXX minikube addons enable XXX 四、使用kubeadm安装 1、安装kubeadm, kubelet 和 kubect kubeadm: the command to bootstrap the cluster. kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers. kubectl: the command line util to talk to your cluster. cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum -y makecache fast yum install -y kubectl kubeadm kubelet systemctl start kubelet systemctl enable kubelet 2、由于绕过了iptables而导致路由错误的问题。 cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system 3、禁用交换分区和禁用防火墙 swapoff -a sed -i 's/.*swap.*/#&/' /etc/fstab setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config systemctl stop firewalld systemctl disable firewalld 4、安装docker docker安装 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun systemctl start docker systemctl enable docker 5、使用kubeadm开始安装 apiVersion: kubeadm.k8s.io/v1alpha1 kind: MasterConfiguration api: advertiseAddress: \"192.168.1.10\" networking: podSubnet: \"10.0.0.0/24\" kubernetesVersion: \"v1.15.0\" imageRepository: \"registry.cn-hangzhou.aliyuncs.com/hlyani\" kubeadm init --config kubeam.yaml 或 kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/hlyani --apiserver-advertise-address=192.168.1.10 --kubernetes-version=v1.15.0 --pod-network-cidr=10.0.0.0/24 -v 0 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.1.10:6443 --token 4mgxai.ou2w2ff7zwz5ykym \\ --discovery-token-ca-cert-hash sha256:16bf000d7f9ce90f932f13747727e9795eac8babe901f7fae5842347a661d67a 6、安装网络插件 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/62e44c867a2846fefb68bd5f178daf4da3095ccb/Documentation/kube-flannel.yml Error registering network: failed to acquire lease: node \"node1\" pod cidr not assigned https://github.com/coreos/flannel/blob/fd8c28917f338a30b27534512292cd5037696634/Documentation/troubleshooting.md#kubernetes-specific kubectl patch node node1 -p '{\"spec\":{\"podCIDR\":\"10.0.0.0/24\"}}' kubectl apply -f https://docs.projectcalico.org/v3.7/manifests/calico.yaml 7、添加node节点 kubeadm join 192.168.1.10:6443 --token 4mgxai.ou2w2ff7zwz5ykym \\ --discovery-token-ca-cert-hash sha256:16bf000d7f9ce90f932f13747727e9795eac8babe901f7fae5842347a661d67a kubeadm token list openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //' kubeadm join --token : --discovery-token-ca-cert-hash sha256: 8、安装dashboard https://github.com/kubernetes/dashboard https://github.com/kubernetes-retired/heapster git clone https://github.com/kubernetes-retired/heapster.git git tag git checkout v1.5.4 ls d/ grafana.yaml heapster-rbac.yaml heapster.yaml influxdb.yaml kubectl apply -f d/ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml #image: registry.cn-hangzhou.aliyuncs.com/hlyani/kubernetes-dashboard-amd64:v1.10.1 spec: ports: - port: 443 targetPort: 8443 type: NodePort 修改 kubectl edit service -n kube-system kubernetes-dashboard https://github.com/kubernetes/dashboard/wiki/Creating-sample-user # cat dashboard-admin-user.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system kubectl apply -f dashboard-admin-user.yaml kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') 9、失败清理环境 rm -rf /var/lib/etcd/* rm -rf /var/lib/kubelet/* rm -rf /etc/kubernetes/* docker rm -f `docker ps -qa` kill -9 `netstat -ntlp|grep 102|awk '{print $7}'|cut -d'/' -f1` echo \"1\" >/proc/sys/net/bridge/bridge-nf-call-iptables kubeadm reset -f iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X 五、FAQ 1、Warning VolumeResizeFailed 13m volume_expand error expanding volume \"default/data-mariadb-master-0\" of plugin \"kubernetes.io/rbd\": rbd info failed, error: exit status 127 扩容pvc时失败问题 kubcectl get po -A kubectl get pvc kubectl describe pvc data-mariadb-master-0 echo \"deb http://mirrors.aliyun.com/debian/ stretch main non-free contrib deb-src http://mirrors.aliyun.com/debian/ stretch main non-free contrib deb http://mirrors.aliyun.com/debian-security stretch/updates main deb-src http://mirrors.aliyun.com/debian-security stretch/updates main deb http://mirrors.aliyun.com/debian/ stretch-updates main non-free contrib deb-src http://mirrors.aliyun.com/debian/ stretch-updates main non-free contrib deb http://mirrors.aliyun.com/debian/ stretch-backports main non-free contrib deb-src http://mirrors.aliyun.com/debian/ stretch-backports main non-free contrib\" | tee > sources.list kubectl cp sources.list --namespace kube-system kube-controller-manager-node1:/etc/apt/ kubectl exec -it --namespace kube-system kube-controller-manager-node1 sh apt update && apt install -y ceph-common kubectl cp /etc/ceph/ceph.client.admin.keyring --namespace kube-system kube-controller-manager-node1:/etc/ceph/ kubectl cp /etc/ceph/ceph.conf --namespace kube-system kube-controller-manager-node1:/etc/ceph/ rbd ls k8s "},"notes/docker/k8s_use.html":{"url":"notes/docker/k8s_use.html","title":"k8s 使用","keywords":"","body":"K8S 使用 一、常用操作 1、探针 livenessProbe: exec: command: [\"cat\", \"/app/index.html\"] # tcpSocket: # port: http initialDelaySeconds: 30 timeoutSeconds: 5 failureThreshold: 6 readinessProbe: exec: command: [\"cat\", \"/app/index.html\"] # tcpSocket: # port: http initialDelaySeconds: 5 timeoutSeconds: 3 periodSeconds: 5 2、亲和性、标签 In: label的值在某个列表中 NotIn：label的值不在某个列表中 Exists：某个label存在 DoesNotExist：某个label不存在 Gt：label的值大于某个值（字符串比较） Lt：label的值小于某个值（字符串比较） 如果nodeAffinity中nodeSelector有多个选项，节点满足任何一个条件即可； 如果matchExpressions有多个选项，则节点必须同时满足这些选项才能运行pod 。 1.节点亲和性/反亲和性 NodeAffinity: 节点亲和性 requiredDuringSchedulingIgnoredDuringExecution: 硬亲和性，必须部署在指定的节点上，或必须不部署在指定节点上 preferredDuringSchedulingIgnoredDuringExecution: 软亲和性，尽量部署在满足条件的节点上，或者尽量不部署在被匹配的节点上 2.Pod亲和性/反亲和性 podAffinity: Pod 亲和性 podAntiAffinity: Pod 反亲和性 requiredDuringSchedulingIgnoredDuringExecution: 将a应用和b应用部署在一起，或不部署在一起 labelSelector matchExpressions matchLabels namespaceSelector namespaces topologyKey preferredDuringSchedulingIgnoredDuringExecution: 尽量将a应用和b应用部署在一起，或不部署在一起 podAffinityTerm weight 1-100 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: aaa operator: In values: - aaa topologyKey: \"kubernetes.io/hostname\" affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: aaa operator: In values: - bbb topologyKey: kubernetes.io/hostname 3.亲和性 apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 必须满足 labelSelector: - matchExpressions: - key: disktype # 标签 disktype operator: In # 等于 values: - ssd # ssd preferredDuringSchedulingIgnoredDuringExecution: # 尽量满足 - weight: 1 preference: matchExpressions: - key: processor # labels key operator: In # 等于 values: - gpu # gpu containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: meme operator: In values: - bus namespaces: #如果写了namespaces但是留空，匹配所有namespace下的指定label的pod - kube-system topologyKey: kubernetes.io/hostname 4.反亲和性 spec: affinity: podAntiAffinity: # 就这里加 Anti requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: meme operator: In values: - bus topologyKey: kubernetes.io/hostname 5.示例 spec: template: metadata: labels: ddp: worker spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: ddp operator: In values: - worker topologyKey: kubernetes.io/hostname 查看标签 kubectl get no --show-labels kubectl get no -l testtag=error kubectl get pods -l time=2019 --show-labels 设置标签 kubectl label nodes node2 node-role.kubernetes.io/worker= kubectl label no node3 testtag=error kubectl label no node1 testtag=node1 kubectl label node node2 role=worker kubectl label no node1 disktype=ssd spec: nodeSelector: disktype: ssd1 spec: nodeSelector: kubernetes.io/hostname: master.cluster.k8s spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: labelSelector: - matchExpressions: - key: testtag operator: In values: - node1 - node2 删除标签 kubectl label no node3 testtag- 3、集群资源查询 kubectl top kubectl describe node Capacity（容量）: 指的是节点上理论上的最大资源量，即节点上未做任何预留，全部可用于运行Pods的最大资源总量。这通常反映了硬件的极限或管理员设定的上限。 Allocatable（可分配）: 则是指在考虑了系统预留（system reserve）、kubelet预留以及其他系统组件（如kube-proxy、runtime等）所需资源后，真正可用于运行用户Pods的资源量。它是Kubernetes在调度Pod时实际参考的可用资源量，确保系统组件能够正常运行，防止资源被完全耗尽而导致节点不稳定。 requests关注的是保障容器的最低资源需求，而limits则用来限制容器资源使用的上限 内存使用超出限制时可能会被Kubernetes OOM Killer（Out Of Memory killer）终止 m 千分之一 毫核 kubectl proxy http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/ http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespaces//pods/ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/ kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces//pods/ 4、命名空间相关 1.创建、切换命名空间 kubectl get ns kubectl create namespace my-namespace kubectl config set-context $(kubectl config current-context) --namespace= kubectl config set-context kubernetes-admin@cluster.local --namespace=kube-system kubectl config view | grep namespace: 2.namespace 一直 Teminating kubectl delete ns test kubectl edit ns test 删除 finalizers 3.删除命名空间所有内容 kubectl delete pods --all -n test kubectl api-resources --verbs=list --namespaced -o name|xargs -I {} kubectl delete --all {} -n test 5、维护节点 1.设置节点不可调度 kuberctl cordon node2 2.驱逐节点上的pod kubectl drain node2 --delete-local-data --ignore-daemonsets --force 3.维护结束 kubectl uncordon node2 4.删除节点 kubectl cordon node2 kubectl drain node2 --delete-local-data --ignore-daemonsets --forece kubectl delete node node2 --delete-local-data 即使pod使用了emptyDir也删除 --ignore-daemonsets 忽略deamonset控制器的pod，如果不忽略，deamonset控制器控制的pod被删除后可能马上又在此节点上启动起来,会成为死循环； --force 不加force参数只会删除该NODE上由ReplicationController, ReplicaSet, DaemonSet,StatefulSet or Job创建的Pod，加了后还会删除'裸奔的pod'(没有绑定到任何replication controller) 5.重新加入 [root@master] kubeadm token create --print-join-command [root@node2] kubeadm join 172.27.9.131:6443 --token svrip0.lajrfl4jgal0ul6i --discovery-token-ca-cert-hash sha256:5f656ae26b5e7d4641a979cbfdffeb7845cc5962bbfcd1d5435f00a25c02ea50 6、annotation 修改annotation kubectl annotate --overwrite pod test action=StopContainer 7、污点与容忍度 污点（Taint），排斥一类特定的Pod。 容忍度（Toleration），允许调度器调度带有对应污点的Pod，应用于Pod上。 NoExecute 这会影响已在节点上运行的 Pod，具体影响如下： 如果 Pod 不能容忍这类污点，会马上被驱逐。 果 Pod 能够容忍这类污点，但是在容忍度定义中没有指定 tolerationSeconds， 则 Pod 还会一直在这个节点上运行。 如果 Pod 能够容忍这类污点，而且指定了 tolerationSeconds， 则 Pod 还能在这个节点上继续运行这个指定的时间长度。 这段时间过去后，节点生命周期控制器从节点驱除这些 Pod。 NoSchedule 除非具有匹配的容忍度规约，否则新的 Pod 不会被调度到带有污点的节点上。 当前正在节点上运行的 Pod 不会被驱逐。 PreferNoScheduler ​ 控制平面将尝试避免将不能容忍污点的 Pod 调度到的节点上，但不能保证完全避免。 kubectl taint nodes 192.168.0.127 key1=value1:NoExecute --overwrite tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoExecute\" tolerationSeconds: 6000 operator的默认值是Equal 其他值：Exists tolerationSeconds是当pod需要被驱逐时，可以继续在node上运行的时间，可选参数。 删除 kubectl taint nodes 192.168.0.127 key1=value1:NoExecute- 允许在污点上调度 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" - operator: \"Exists\" effect: \"NoSchedule\" - operator: \"Exists\" effect: \"NoExecute\" 8、HostPath 挂载模式 描述 DirectoryOrCreate 如果在给定路径上什么都不存在，将根据需要创建空目录，权限设置为0755，与Kubelet具有相同的组和属主信息。 Directory 在给定路径上必须存在目录。 FileOrCreate 如果在给定路径上什么都不存在，那么将在给定路径根据需要创建空文件，权限设置为0644，具有与Kubelet相同的组和所有权。 File 在给定路径上必须存在文件。 apiVersion: v1 kind: Pod metadata: name: test spec: containers: - image: nginx:1.7.9 name: test volumeMounts: - mountPath: /test name: test-volume volumes: - name: test-volume hostPath: path: /data type: DirectoryOrCreate apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \"/data\" --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hostpath spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 9、获取 kubeconfig 并在 pod 中使用 kubectl apply -f - kubectl apply -f - 10、日志轮转 kubelet --container-log-max-size=50Mi --container-log-max-files=3 --container-log-dir=/path/to/custom/logs /var/lib/kubelet/config.yaml containerLogMaxFiles: 3 containerLogMaxSize: 10Mi containerd [plugins.\"io.containerd.grpc.v1.cri\".containerd] [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] log-driver = \"json-file\" log-opts = [\"max-size=50m\", \"max-file=3\"] 11、配置变化重启pod apiVersion: apps/v1 kind: Deployment metadata: name: {{ include \"app.fullname\" . }} labels: {{- include \"app.labels\" . | nindent 4 }} app.kubernetes.io/component: app spec: replicas: 1 selector: matchLabels: {{- include \"app.selectorLabels\" . | nindent 6 }} app.kubernetes.io/component: app template: metadata: labels: {{- include \"app.labels\" . | nindent 8 }} app.kubernetes.io/component: app annotations: configHash: {{ include (print $.Template.BasePath \"/config.yaml\") . | sha256sum | quote }} # configHash: {{ .Values.sagflow.config | toYaml | sha256sum | quote }} $.Template.BasePath \"/config.yaml\" -> templates/configmap.yaml 12、会话粘滞 spec: sessionAffinity: ClientIP sessionAffinityConfig: clientIP: timeoutSeconds: 3600 # 粘性时间为 1 小时 13、CronJob 并发控制 通过 spec.concurrencyPolicy 字段，控制 CronJob 在新任务触发时是否允许并发运行： Allow（默认值）：允许并发运行多个任务。 Forbid：禁止并发运行，若前一个任务未完成，新任务会被跳过。 Replace：如果上一个任务未完成，会终止上一个任务，并启动新任务。 任务重试策略 通过 spec.jobTemplate.spec.backoffLimit 定义失败任务的重试次数（默认为 6 次）。超过重试次数后，任务标记为失败。 超时控制 通过 spec.jobTemplate.spec.activeDeadlineSeconds 设置任务的最长运行时间（以秒为单位）。如果超过这个时间，任务会被强制终止。 历史保留策略 spec.successfulJobsHistoryLimit：保留成功任务的数量（默认 3）。 spec.failedJobsHistoryLimit：保留失败任务的数量（默认 1）。 spec: schedule: \"*/5 * * * *\" # 每 5 分钟触发一次 successfulJobsHistoryLimit: 0 # 保留所有成功任务 failedJobsHistoryLimit: 0 # 保留所有失败任务 二、常用helm源 helm repo add aliyuncs https://apphub.aliyuncs.com helm repo add bitnami https://charts.bitnami.com/bitnami helm repo add stable https://kubernetes-charts.storage.googleapis.com helm repo add local https://local.com/chartrepo/k8s helm repo add az https://mirror.azure.cn/kubernetes/charts/ helm repo add goharbor https://helm.goharbor.io helm repo add emqx https://repos.emqx.io/charts helm repo add --ca-file /etc/docker/certs.d/local.com/ca.crt --username=admin --password=XXX local https://local.com/chartrepo/k8s 三、常用命令 1.jsonpath kubectl get deploy -l app.kubernetes.io/name=controller -o jsonpath={.items[0].status.availableReplicas} kubectl get nodes --namespace default -o jsonpath=\"{.items[0].status.addresses[0].address}\" 2.强制删除容器 kubectl delete pod PODNAME --force --grace-period=0 helm uninstall --timeout 1s test & kubectl get po -o=name|grep test|xargs kubectl delete --grace-period=0 --force 3.从集群中导出配置 kubectl get daemonset -n kube-system kube-flannel-ds -o yaml > kube-system-kube-flannel-ds.yaml 4.测试 api sever curl -kv https://10.96.0.1:443/version 5.集群组件状态 kubectl get --raw='/readyz?verbose' 6.events kubectl get events --sort-by=.metadata.creationTimestamp --field-selector=involvedObject.kind=Pod,involvedObject.name= kubectl get events --sort-by=.metadata.creationTimestamp --field-selector=involvedObject.kind=Pod,involvedObject.name=nginx-deployment-7bc4686759-m7h4l kubectl get events kubectl get events --field-selector type=Warning kubectl get events --sort-by=.metadata.creationTimestamp kubectl get events --field-selector involvedObject.kind!=Pod kubectl get events --field-selector involvedObject.kind=Node, involvedObject.name= kubectl get events --field-selector type!=Normal 7.http 访问apiserver kubectl proxy --address='0.0.0.0' --accept-hosts='^*$' --port=8080 8.logs kubectl logs kubectl logs --since=6h kubectl logs --tail=50 kubectl logs -f [-c ] kubectl logs -f kubectl logs -c kubectl logs pod.log kubectl logs --previous 9.port-forward kubectl port-forward $POD_NAME 8080:8080 -n default kubectl port-forward svc/test 8080:31000 10.其他命令 docker inspect 5c3af3101afb -f \"{{.HostConfig.Memory}}\" docker stats --format 'table {{.CPUPerc}}\\t{{.MemPerc}}' --no-stream d17 2>/dev/null | tail -1 | sed 's/ //g'| sed 's/%/,/g' kubectl run limit-test --image=busybox --limits \"memory=100Mi\" --command -- /bin/sh -c \"while true; do sleep 2; done\" kubectl run limit-test --image=busybox --requests \"cpu=50m\" --command -- /bin/sh -c \"while true; do sleep 2; done\" kubectl explain Pod.spec.volumes.hostPath kubectl get pod test-pod -o jsonpath='{.spec.containers[*].resources.limits}' kubectl expose pod httpbin --port 80 11.命令补全 kubectl completion bash /root/.bashrc source 12.查看 gpu 使用情况 kubectl describe nodes | grep -E \"Name:|nvidia.com/gpu\" kubectl get nodes -o custom-columns=\"NAME:.metadata.name,GPU_ALLOCATED:.status.allocatable.nvidia\\.com/gpu,GPU_USED:.status.capacity.nvidia\\.com/gpu\" 13.pod 访问 svc 服务的规则如下 ..svc.cluster.local ..svc. vim /var/lib/kubelet/config.yaml ... clusterDomain: cluster.local ... 四、证书过期 kubeadm alpha certs renew all --config=/etc/kubernetes/kubeadm-config.yaml kubeadm alpha certs check-expiration --config=/etc/kubernetes/kubeadm-config.yaml cp -r /etc/kubernetes /etc/kubernetes_bak cd /etc/kubernetes/pki/ rm -rf {apiserver.crt,apiserver-kubelet-client.crt,front-proxy-ca.crt,front-proxy-client.crt,front-proxy-client.key,front-proxy-ca.key,apiserver-kubelet-client.key,apiserver.key} kubeadm init phase certs all --apiserver-advertise-address cd /etc/kubernetes/ rm -rf {admin.conf,controller-manager.conf,kubelet.conf,scheduler.conf} kubeadm init phase kubeconfig all cp /etc/kubernetes/admin.conf $HOME/.kube/config https://github.com/hlyani/kubernetes1.17.3 wget https://raw.githubusercontent.com/hlyani/kubernetes1.17.3/master/update-kubeadm-cert.sh chmod +x update-kubeadm-cert.sh ./update-kubeadm-cert.sh all ./update-kubeadm-cert.sh master 五、修改 service 默认端口范围 默认范围 30000-32767 1、k8s cat /etc/systemd/system/kube-apiserver.service | grep node-port --service-node-port-range=80-32767 \\ 2、k3s k3s server --kube-apiserver-arg --service-node-port-range=80-32767 cat /etc/systemd/system/k3s.service ExecStart=/usr/local/bin/k3s \\ server --kube-apiserver-arg service-node-port-range=80-32767 systemctl daemon-reload systemctl restart docker cat /etc/systemd/system/k3s.service ExecStart=/usr/local/bin/k3s \\ server --kube-apiserver-arg=\"service-node-port-range=80-32767\" systemctl daemon-reload systemctl restart docker 六、Pod Pid限制 vim /var/lib/kubelet/config.yaml podPidsLimit: 1024 /proc/sys/kernel/pid_max # 定义了可以分配给进程的最大进程 ID（PID） sysctl -w kernel.pid_max=65535 /etc/sysctl.conf kernel.pid_max = 65535 sysctl -p 七、FAQ 1、flannel网络已存在 NetworkPlugin cni failed to set up pod \"xxxxx\" network: failed to set bridge addr: \"cni0\" already has an IP address different from10.x.x.x - Error ip link set cni0 down ip link set flannel.1 down ip link delete cni0 ip link delete flannel.1 systemctl restart containerd systemctl restart kubelet k8s cluster ping 10.96.0.1 no route kubectl edit cm kube-proxy -n kube-system ... kind: KubeProxyConfiguration metricsBindAddress: \"\" mode: \"ipvs\" ... kube-proxy Failed to retrieve node info: Unauthorized 报错日志来看是证书验证失败，github上看到了有此问题的解决方法 ，需要删除kube-proxy 依赖的secret 可能是多次运行kubeadm，导致集群里保存的证书和新生成的证书不一致 kubectl delete secret -n kube-system kube-proxy-token-kgrw7 k8s 修改 pod-network-cidr 地址范围 --pod-network-cidr=10.244.0.0/16 -> --pod-network-cidr=192.168.0.0/16 kubectl -n kube-system edit cm kubeadm-config vim /etc/kubernetes/manifests/kube-scheduler.yaml kubectl cluster-info dump | grep -m 1 cluster-cidr 2、k8s证书过期，重新部署出问题，恢复数据和集群 查看源pvc关系 [root@node1]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mariadb-master-0 Bound pvc-16840bfc-4a3b-45de-8250-3c71044c00ce 1000Gi RWO ceph-rbd 359d data-mariadb-slave-0 Bound pvc-3a4b00ba-7fd9-4eb3-93ef-b4ea96648761 160Gi RWO ceph-rbd 359d data-mariadb-slave-1 Bound pvc-12e8300a-3ef3-4ebe-a728-ec325666e675 160Gi RWO ceph-rbd 359d data-mariadb-slave-2 Bound pvc-be01ef0f-51f9-4654-964b-288f74f5d43f 160Gi RWO ceph-rbd 359d 查看每个节点的rbd挂载关系 [root@node1]# mount|grep rbd /dev/rbd0 on /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/k8s-image-kubernetes-dynamic-pvc-d83535a2-774d-11ea-96f9-4a11faeddc0e type ext4 (rw,relatime,seclabel,stripe=1024,data=ordered) /dev/rbd0 on /var/lib/kubelet/pods/6f364828-81bf-4bdc-9145-3d105f140932/volumes/kubernetes.io~rbd/pvc-c9b4b571-4c3c-46bf-8c67-1f1fc204dd6b type ext4 (rw,relatime,seclabel,stripe=1024,data=ordered) /dev/rbd1 on /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/k8s-image-kubernetes-dynamic-pvc-d816cdfb-774d-11ea-96f9-4a11faeddc0e type ext4 (rw,relatime,seclabel,stripe=1024,data=ordered) /dev/rbd1 on /var/lib/kubelet/pods/f1baca67-5ae2-4916-8e67-c9a37a51b94b/volumes/kubernetes.io~rbd/pvc-292d52f7-22b3-4af9-9633-94e09a0f8bef type ext4 (rw,relatime,seclabel,stripe=1024,data=ordered) /dev/rbd8 on /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/k8s-image-kubernetes-dynamic-pvc-fc5a0ab3-220b-11ea-8d64-8e19e7bc2052 type ext4 (rw,relatime,seclabel,stripe=1024,data=ordered) /dev/rbd8 on /var/lib/kubelet/pods/45de1bb4-5863-4dac-93f1-fd801a5a2f4d/volumes/kubernetes.io~rbd/pvc-16840bfc-4a3b-45de-8250-3c71044c00ce type ext4 (rw,relatime,seclabel,stripe=1024,data=ordered) kubectl get pv pvc-XXXXXXXXXXXXXXXXXXXXXXXXX -o yaml imageName: csi-vol-YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY 查看pv pool 里面的rbd rbd ls k8s kubernetes-dynamic-pvc-045ec64a-2092-11ea-8d64-8e19e7bc2052 kubernetes-dynamic-pvc-0eef530b-206b-11ea-8d64-8e19e7bc2052 重新创建应用，删除应用，查看当前应用的pvc，将pvc对应的rbd删除，并将旧的rbd重命名为当前pvc，最后再重新创建应用 remove (rm) rename (mv) copy (cp) rbd rm k8s/kubernetes-dynamic-pvc-d95e5e92-3c7e-11eb-9d5a-6a0e4650a17b rbd mv k8s/kubernetes-dynamic-pvc-fc5a0ab3-220b-11ea-8d64-8e19e7bc2052 k8s/kubernetes-dynamic-pvc-d95e5e92-3c7e-11eb-9d5a-6a0e4650a17b 其他 ceph osd dump | grep full_ratio ceph osd set-full-ratio 0.98 ceph osd set-backfillfull-ratio 0.95 3、记一次 kube-flannel pod启动异常 第一现象，kube-flannel一直启不起来，不断重启 kubectl get po -A 查看pod日志，显示连接不上10.96.0.1:443，即kube-api 服务地址 kubectl logs -n kube-flannel kube-flannel-ds-7bcbn kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 443/TCP 5d7h 尝试去各个节点 连接10.96.0.1:443，均不通 curl -kv https://10.96.0.1:443/version 考虑查看kube-proxy日志 kubectl logs -n kube-system kube-proxy-p8fdr ... Failed to retrieve node info: Unauthorized ... 报错日志来看是证书验证失败 ，需要删除kube-proxy 依赖的secret，让集群重新生成最新的secret kubectl delete secret -n kube-system kube-proxy-token-kgrw7 等待secret重创，删除kube-proxy pod，重启kube-proxy 各个节点请求api-server，正常 curl -kv https://10.96.0.1:443/version 重新部署kube-flannel，查看flannel日志 NetworkPlugin cni failed to set up pod \"xxxxx\" network: failed to set bridge addr: \"cni0\" already has an IP address different from10.x.x.x - Error 删除之前已有网卡，重新服务，等待自动重新创建 ip link set cni0 down ip link set flannel.1 down ip link delete cni0 ip link delete flannel.1 systemctl restart containerd / systemctl restart docker systemctl restart kubelet "},"notes/docker/k8s_hpa.html":{"url":"notes/docker/k8s_hpa.html","title":"k8s hpa","keywords":"","body":"K8S HPA 一、概述 K8S HPA（HorizontalPodAutoscaler），主要适用于 Deployment 或 StatefulSet 对象。 https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ 1、扩缩比例算法 期望副本数 = ceil[当前副本数 * (当前指标 / 期望指标)] currentUtilization = metricsTotal * 100 / requestedTotal usageRatio = currentUtilization / targetUtilization if (1 - tolerance) 当前指标 = sum(所有 Pod 当前指标值) / Pod 数 # 当前指标或平均指标 2、计算过程示例 如果当前指标值为 200m，而期望值为 100m，则副本数将加倍， 因为 200.0 / 100.0 == 2.0 如果当前值为 50m，则副本数将减半， 因为 50.0 / 100.0 == 0.5。如果比率足够接近 1.0（在全局可配置的容差范围内，默认为 0.1）， 则控制平面会跳过扩缩操作。 3、排除计算 Pod 排除以下异常 Pod，其他所有 Pod 参与指标计算： 非就绪状态的 Pod 正在被终止的 Pod 尚未达到指标采集时间的 Pod 未暴露指标的 Pod 未满足健康检查的 Pod 超过指标失效时间的 Pod，可能是指标采集延迟或网络问题导致的。 4、资源指标 资源指标是指 Kubernetes 内置的 CPU 和内存指标。 支持的目标类型： Utilization：按百分比表示的指标利用率（默认目标为 Pod 请求的百分比）。 Value：指标的绝对值（例如，500m 表示 0.5 CPU 核心）。 AverageValue：所有 Pod 指标的平均值（例如，每个 Pod 的平均内存使用量）。 5、多指标联合策略 多指标联合策略示例 可以同时监控多个指标，并根据定义的规则来扩展Pod数量。例如，基于CPU和内存同时进行扩展。 HPA 支持定义多个指标，并根据所有指标计算出最终的扩缩容决策。 联合规则： 如果多个指标有不同的扩缩容建议值 扩容时会选择 Pod 数量最多的值。这是因为扩容是为了确保工作负载不会超载并保持服务的可用性。max 缩容时会选择 Pod 数量最少的值。但必须满足 minReplicas 限制，确保副本数不会小于最小值。这是为了避免 Pod 数量过少，导致服务不稳定。min 为保证服务的可用性，扩容优先于缩容。 6、kube-controller 中配置 spec: containers: - command: - kube-controller-manager - --horizontal-pod-autoscaler-sync-period=15 # HPA 的同步间隔时间（单位：秒）。 # 表示 HPA 控制器多长时间检查一次指标并决定是否调整 Pod 数量。 # 默认值为 15 秒，这里显式设置为 15。 - --horizontal-pod-autoscaler-tolerance=0.1 # HPA 的容差范围（默认值：0.1，即 10%）。 # 当当前指标值与目标值的比率在 (1 ± 容差范围) 内（如 0.9 到 1.1）时，HPA 不会触发扩缩容。 # 只有超出此范围时才会触发扩缩容操作。 - --horizontal-pod-autoscaler-initial-readiness-delay=30 # 初始化延迟时间（单位：秒）。 # 在 Pod 创建时，HPA 会等待这么长时间后才检查 Pod 的就绪状态。 # 避免刚启动的 Pod 被误判为负载不足或负载过高。 # 默认值为 30 秒。 - --horizontal-pod-autoscaler-cpu-initialization-period=300 # CPU 初始化周期（单位：秒）。 # 在 Pod 创建后的指定时间内，HPA 会忽略 CPU 使用率指标，以防止初始化期间的波动影响扩缩容决策。 # 默认值为 300 秒（5 分钟）。 - --horizontal-pod-autoscaler-downscale-stabilization=300 # 缩容稳定化时间（单位：秒）。 # 表示在决定缩容之前，HPA 会等待指定时间，以确保指标值持续下降而不是短期波动。 # 默认值为 300 秒（5 分钟），避免频繁缩容导致的不稳定。 三、keda https://keda.sh/docs/2.16/ 1、示例 kubectl apply -f -2、部署 git clone -b master --depth 1 http://gitlab.hpc.sugon.com/ai/ske-chart.git helm install -n ske keda ske-chart/keda 3、自动扩缩容 1.主要参数解释 1. 最小实例数 spec.minReplicaCount 2. 最大实例数 spec.maxReplicaCount 3. 扩容生效时长 advanced.horizontalPodAutoscalerConfig.behavior.scaleUp.stabilizationWindowSeconds 4. 缩容生效时长 advanced.horizontalPodAutoscalerConfig.behavior.scaleDown.stabilizationWindowSeconds 5. 缩容触发值 spec.triggers.type.activationThreshold 6. 扩容触发值 spec.triggers.type.threshold 特别注意： k8s 默认有10%的容差，设置扩缩容指标时序需考虑 - --horizontal-pod-autoscaler-tolerance=0.1 # HPA 的容差范围（默认值：0.1，即 10%）。 # 当当前指标值与目标值的比率在 (1 ± 容差范围) 内（如 0.9 到 1.1）时，HPA 不会触发扩缩容。 # 只有超出此范围时才会触发扩缩容操作。 2.CPU 利用率 apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: HPA_NAME namespace: default spec: scaleTargetRef: name: DEPLOY_NAME pollingInterval: 30 cooldownPeriod: 300 minReplicaCount: 1 maxReplicaCount: 10 advanced: horizontalPodAutoscalerConfig: behavior: scaleUp: stabilizationWindowSeconds: 30 scaleDown: stabilizationWindowSeconds: 300 triggers: - type: prometheus metricType: \"Value\" metadata: serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090 query: | clamp_min( sum( rate(container_cpu_usage_seconds_total{container!=\"\",namespace=\"default\",pod=~\".*test.*\"}[1m]) ) / sum( kube_pod_container_resource_limits{container!=\"\",namespace=\"default\",pod=~\".*test.*\",resource=\"cpu\"} ) * 100 or vector(0), 0 ) threshold: \"77.27\" activationThreshold: \"66.666\" 3.GPU 利用率 apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: HPA_NAME namespace: default spec: scaleTargetRef: name: DEPLOY_NAME pollingInterval: 30 cooldownPeriod: 300 minReplicaCount: 1 maxReplicaCount: 10 advanced: horizontalPodAutoscalerConfig: behavior: scaleUp: stabilizationWindowSeconds: 30 scaleDown: stabilizationWindowSeconds: 300 triggers: - type: prometheus metricType: \"Value\" metadata: serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090 query: | clamp_min( sum( DCGM_FI_DEV_GPU_UTIL{exported_container!=\"\",exported_namespace=\"default\",exported_pod=~\".*notebook-xxx.*\"} ) / sum( label_replace( kube_pod_container_resource_limits{namespace=\"default\",pod=~\".*notebook-xxxx.*\",resource=\"nvidia_com_gpu\"}, \"exported_pod\", \"$1\", \"pod\", \"(.*)\" ) ) or vector(0), 0 ) threshold: \"77.27\" activationThreshold: \"66.666\" 4.QPM apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: HPA_NAME namespace: default spec: scaleTargetRef: name: DEPLOY_NAME pollingInterval: 30 cooldownPeriod: 300 minReplicaCount: 1 maxReplicaCount: 10 advanced: horizontalPodAutoscalerConfig: behavior: scaleUp: stabilizationWindowSeconds: 30 scaleDown: stabilizationWindowSeconds: 300 triggers: - type: prometheus metricType: \"Value\" metadata: serverAddress: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090 query: | ceil( sum(rate(nginx_ingress_controller_requests{exported_namespace=\"default\",host=~\".*test.*\"}[1m])) * 60 ) or vector(0) threshold: \"77.27\" activationThreshold: \"66.666\" 四、示例 1、部署 deploy kubectl apply -f - 2、配置 hpa kubectl autoscale deployment -n hpa hpa-test --cpu-percent=50 --min=1 --max=10 或 kubectl apply -f - 3、查看 hpa kubectl get hpa -n hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test Deployment/hpa-test 0%/50% 1 10 1 18s 4、增加负载 kubectl run -it load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://hpa-test.hpa.svc.cluster.local; done\" 5、查看 hpa kubectl get hpa -n hpa hpa-test --watch kubectl get hpa -n hpa hpa-test -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test Deployment/hpa-test 0%/50% 1 10 1 10m hpa-test Deployment/hpa-test 92%/50% 1 10 1 10m hpa-test Deployment/hpa-test 249%/50% 1 10 2 11m hpa-test Deployment/hpa-test 250%/50% 1 10 4 11m hpa-test Deployment/hpa-test 250%/50% 1 10 5 11m hpa-test Deployment/hpa-test 166%/50% 1 10 5 11m hpa-test Deployment/hpa-test 69%/50% 1 10 5 12m hpa-test Deployment/hpa-test 56%/50% 1 10 5 12m hpa-test Deployment/hpa-test 58%/50% 1 10 6 12m hpa-test Deployment/hpa-test 53%/50% 1 10 6 12m hpa-test Deployment/hpa-test 47%/50% 1 10 6 13m hpa-test Deployment/hpa-test 47%/50% 1 10 6 13m kubectl get po -n hpa -w NAME READY STATUS RESTARTS AGE hpa-test-6c7675c585-28dsd 0/1 ContainerCreating 0 7s hpa-test-6c7675c585-k8hx7 1/1 Running 0 15m hpa-test-6c7675c585-rqtqj 0/1 ContainerCreating 0 22s hpa-test-6c7675c585-vkf7h 0/1 ContainerCreating 0 7s hpa-test-6c7675c585-lzgb2 0/1 Pending 0 0s hpa-test-6c7675c585-lzgb2 0/1 Pending 0 0s hpa-test-6c7675c585-lzgb2 0/1 ContainerCreating 0 0s hpa-test-6c7675c585-rqtqj 1/1 Running 0 36s hpa-test-6c7675c585-vkf7h 1/1 Running 0 34s hpa-test-6c7675c585-lzgb2 1/1 Running 0 25s hpa-test-6c7675c585-28dsd 1/1 Running 0 43s hpa-test-6c7675c585-gjc58 0/1 Pending 0 0s hpa-test-6c7675c585-gjc58 0/1 Pending 0 0s hpa-test-6c7675c585-gjc58 0/1 ContainerCreating 0 0s hpa-test-6c7675c585-gjc58 1/1 Running 0 5s kubectl get po -n hpa NAME READY STATUS RESTARTS AGE hpa-test-6c7675c585-28dsd 1/1 Running 0 2m hpa-test-6c7675c585-gjc58 1/1 Running 0 44s hpa-test-6c7675c585-k8hx7 1/1 Running 0 17m hpa-test-6c7675c585-lzgb2 1/1 Running 0 105s hpa-test-6c7675c585-rqtqj 1/1 Running 0 2m15s hpa-test-6c7675c585-vkf7h 1/1 Running 0 2m "},"notes/docker/k8s_ceph.html":{"url":"notes/docker/k8s_ceph.html","title":"k8s 使用 ceph 储存","keywords":"","body":"k8s 使用 ceph 储存 一、latest 1、创建pool ceph osd pool create k8s rbd pool init k8s 2、配置 ceph-csi 1、生成 ceph client 认证 ceph auth get-or-create client.k8s mon 'profile rbd' osd 'profile rbd pool=k8s' mgr 'profile rbd pool=k8s' [client.k8s] key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg== 2、生成 ceph-csi configmap ceph mon dump fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8 0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a 1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b 2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c cat csi-config-map.yaml --- apiVersion: v1 kind: ConfigMap data: config.json: |- [ { \"clusterID\": \"b9127830-b0cc-4e34-aa47-9d1a2e9949a8\", \"monitors\": [ \"192.168.1.1:6789\", \"192.168.1.2:6789\", \"192.168.1.3:6789\" ] } ] metadata: name: ceph-csi-config EOF kubectl apply -f csi-config-map.yaml cat csi-kms-config-map.yaml --- apiVersion: v1 kind: ConfigMap data: config.json: |- {} metadata: name: ceph-csi-encryption-kms-config EOF kubectl apply -f csi-kms-config-map.yaml cat ceph-config-map.yaml --- apiVersion: v1 kind: ConfigMap data: ceph.conf: | [global] auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx # keyring is a required key and its value should be empty keyring: | metadata: name: ceph-config EOF kubectl apply -f ceph-config-map.yaml 3、生成 ceph-csi cephx secret cat csi-rbd-secret.yaml --- apiVersion: v1 kind: Secret metadata: name: csi-rbd-secret namespace: default stringData: userID: k8s userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg== EOF kubectl apply -f csi-rbd-secret.yaml 4、配置 ceph-csi 插件 cat csi-provisioner-rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: rbd-csi-provisioner # replace with non-default namespace name namespace: default --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: rbd-external-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"list\", \"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\", \"patch\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims/status\"] verbs: [\"update\", \"patch\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"snapshot.storage.k8s.io\"] resources: [\"volumesnapshots\"] verbs: [\"get\", \"list\", \"patch\"] - apiGroups: [\"snapshot.storage.k8s.io\"] resources: [\"volumesnapshots/status\"] verbs: [\"get\", \"list\", \"patch\"] - apiGroups: [\"snapshot.storage.k8s.io\"] resources: [\"volumesnapshotcontents\"] verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"delete\", \"patch\"] - apiGroups: [\"snapshot.storage.k8s.io\"] resources: [\"volumesnapshotclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"volumeattachments\"] verbs: [\"get\", \"list\", \"watch\", \"update\", \"patch\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"volumeattachments/status\"] verbs: [\"patch\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"csinodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"snapshot.storage.k8s.io\"] resources: [\"volumesnapshotcontents/status\"] verbs: [\"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"serviceaccounts\"] verbs: [\"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: rbd-csi-provisioner-role subjects: - kind: ServiceAccount name: rbd-csi-provisioner # replace with non-default namespace name namespace: default roleRef: kind: ClusterRole name: rbd-external-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: # replace with non-default namespace name namespace: default name: rbd-external-provisioner-cfg rules: - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"delete\"] - apiGroups: [\"coordination.k8s.io\"] resources: [\"leases\"] verbs: [\"get\", \"watch\", \"list\", \"delete\", \"update\", \"create\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: rbd-csi-provisioner-role-cfg # replace with non-default namespace name namespace: default subjects: - kind: ServiceAccount name: rbd-csi-provisioner # replace with non-default namespace name namespace: default roleRef: kind: Role name: rbd-external-provisioner-cfg apiGroup: rbac.authorization.k8s.io EOF cat csi-nodeplugin-rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: rbd-csi-nodeplugin # replace with non-default namespace name namespace: default --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: rbd-csi-nodeplugin rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\"] # allow to read Vault Token and connection options from the Tenants namespace - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"serviceaccounts\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"volumeattachments\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: rbd-csi-nodeplugin subjects: - kind: ServiceAccount name: rbd-csi-nodeplugin # replace with non-default namespace name namespace: default roleRef: kind: ClusterRole name: rbd-csi-nodeplugin apiGroup: rbac.authorization.k8s.io EOF kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml cat csi-rbdplugin-provisioner.yaml --- kind: Service apiVersion: v1 metadata: name: csi-rbdplugin-provisioner # replace with non-default namespace name namespace: default labels: app: csi-metrics spec: selector: app: csi-rbdplugin-provisioner ports: - name: http-metrics port: 8080 protocol: TCP targetPort: 8680 --- kind: Deployment apiVersion: apps/v1 metadata: name: csi-rbdplugin-provisioner # replace with non-default namespace name namespace: default spec: replicas: 3 selector: matchLabels: app: csi-rbdplugin-provisioner template: metadata: labels: app: csi-rbdplugin-provisioner spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - csi-rbdplugin-provisioner topologyKey: \"kubernetes.io/hostname\" serviceAccountName: rbd-csi-provisioner priorityClassName: system-cluster-critical containers: - name: csi-provisioner image: k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0 args: - \"--csi-address=$(ADDRESS)\" - \"--v=5\" - \"--timeout=150s\" - \"--retry-interval-start=500ms\" - \"--leader-election=true\" # set it to true to use topology based provisioning - \"--feature-gates=Topology=false\" # if fstype is not specified in storageclass, ext4 is default - \"--default-fstype=ext4\" - \"--extra-create-metadata=true\" env: - name: ADDRESS value: unix:///csi/csi-provisioner.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /csi - name: csi-snapshotter image: k8s.gcr.io/sig-storage/csi-snapshotter:v5.0.1 args: - \"--csi-address=$(ADDRESS)\" - \"--v=5\" - \"--timeout=150s\" - \"--leader-election=true\" env: - name: ADDRESS value: unix:///csi/csi-provisioner.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /csi - name: csi-attacher image: k8s.gcr.io/sig-storage/csi-attacher:v3.4.0 args: - \"--v=5\" - \"--csi-address=$(ADDRESS)\" - \"--leader-election=true\" - \"--retry-interval-start=500ms\" env: - name: ADDRESS value: /csi/csi-provisioner.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /csi - name: csi-resizer image: k8s.gcr.io/sig-storage/csi-resizer:v1.4.0 args: - \"--csi-address=$(ADDRESS)\" - \"--v=5\" - \"--timeout=150s\" - \"--leader-election\" - \"--retry-interval-start=500ms\" - \"--handle-volume-inuse-error=false\" env: - name: ADDRESS value: unix:///csi/csi-provisioner.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /csi - name: csi-rbdplugin # for stable functionality replace canary with latest release version image: quay.io/cephcsi/cephcsi:canary args: - \"--nodeid=$(NODE_ID)\" - \"--type=rbd\" - \"--controllerserver=true\" - \"--endpoint=$(CSI_ENDPOINT)\" - \"--csi-addons-endpoint=$(CSI_ADDONS_ENDPOINT)\" - \"--v=5\" - \"--drivername=rbd.csi.ceph.com\" - \"--pidlimit=-1\" - \"--rbdhardmaxclonedepth=8\" - \"--rbdsoftmaxclonedepth=4\" - \"--enableprofiling=false\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_ID valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace # - name: KMS_CONFIGMAP_NAME # value: encryptionConfig - name: CSI_ENDPOINT value: unix:///csi/csi-provisioner.sock - name: CSI_ADDONS_ENDPOINT value: unix:///csi/csi-addons.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /csi - mountPath: /dev name: host-dev - mountPath: /sys name: host-sys - mountPath: /lib/modules name: lib-modules readOnly: true - name: ceph-csi-config mountPath: /etc/ceph-csi-config/ - name: ceph-csi-encryption-kms-config mountPath: /etc/ceph-csi-encryption-kms-config/ - name: keys-tmp-dir mountPath: /tmp/csi/keys - name: ceph-config mountPath: /etc/ceph/ - name: csi-rbdplugin-controller # for stable functionality replace canary with latest release version image: quay.io/cephcsi/cephcsi:canary args: - \"--type=controller\" - \"--v=5\" - \"--drivername=rbd.csi.ceph.com\" - \"--drivernamespace=$(DRIVER_NAMESPACE)\" env: - name: DRIVER_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: ceph-csi-config mountPath: /etc/ceph-csi-config/ - name: keys-tmp-dir mountPath: /tmp/csi/keys - name: ceph-config mountPath: /etc/ceph/ - name: liveness-prometheus image: quay.io/cephcsi/cephcsi:canary args: - \"--type=liveness\" - \"--endpoint=$(CSI_ENDPOINT)\" - \"--metricsport=8680\" - \"--metricspath=/metrics\" - \"--polltime=60s\" - \"--timeout=3s\" env: - name: CSI_ENDPOINT value: unix:///csi/csi-provisioner.sock - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - name: socket-dir mountPath: /csi imagePullPolicy: \"IfNotPresent\" volumes: - name: host-dev hostPath: path: /dev - name: host-sys hostPath: path: /sys - name: lib-modules hostPath: path: /lib/modules - name: socket-dir emptyDir: { medium: \"Memory\" } - name: ceph-config configMap: name: ceph-config - name: ceph-csi-config configMap: name: ceph-csi-config - name: ceph-csi-encryption-kms-config configMap: name: ceph-csi-encryption-kms-config - name: keys-tmp-dir emptyDir: { medium: \"Memory\" } EOF cat csi-rbdplugin.yaml --- kind: DaemonSet apiVersion: apps/v1 metadata: name: csi-rbdplugin # replace with non-default namespace name namespace: default spec: selector: matchLabels: app: csi-rbdplugin template: metadata: labels: app: csi-rbdplugin spec: serviceAccountName: rbd-csi-nodeplugin hostNetwork: true hostPID: true priorityClassName: system-node-critical # to use e.g. Rook orchestrated cluster, and mons' FQDN is # resolved through k8s service, set dns policy to cluster first dnsPolicy: ClusterFirstWithHostNet containers: - name: driver-registrar # This is necessary only for systems with SELinux, where # non-privileged sidecar containers cannot access unix domain socket # created by privileged CSI driver container. securityContext: privileged: true image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0 args: - \"--v=5\" - \"--csi-address=/csi/csi.sock\" - \"--kubelet-registration-path=/var/lib/kubelet/plugins/rbd.csi.ceph.com/csi.sock\" env: - name: KUBE_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumeMounts: - name: socket-dir mountPath: /csi - name: registration-dir mountPath: /registration - name: csi-rbdplugin securityContext: privileged: true capabilities: add: [\"SYS_ADMIN\"] allowPrivilegeEscalation: true # for stable functionality replace canary with latest release version image: quay.io/cephcsi/cephcsi:canary args: - \"--nodeid=$(NODE_ID)\" - \"--pluginpath=/var/lib/kubelet/plugins\" - \"--stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/pv/\" - \"--type=rbd\" - \"--nodeserver=true\" - \"--endpoint=$(CSI_ENDPOINT)\" - \"--csi-addons-endpoint=$(CSI_ADDONS_ENDPOINT)\" - \"--v=5\" - \"--drivername=rbd.csi.ceph.com\" - \"--enableprofiling=false\" # If topology based provisioning is desired, configure required # node labels representing the nodes topology domain # and pass the label names below, for CSI to consume and advertise # its equivalent topology domain # - \"--domainlabels=failure-domain/region,failure-domain/zone\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_ID valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace # - name: KMS_CONFIGMAP_NAME # value: encryptionConfig - name: CSI_ENDPOINT value: unix:///csi/csi.sock - name: CSI_ADDONS_ENDPOINT value: unix:///csi/csi-addons.sock imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: socket-dir mountPath: /csi - mountPath: /dev name: host-dev - mountPath: /sys name: host-sys - mountPath: /run/mount name: host-mount - mountPath: /etc/selinux name: etc-selinux readOnly: true - mountPath: /lib/modules name: lib-modules readOnly: true - name: ceph-csi-config mountPath: /etc/ceph-csi-config/ - name: ceph-csi-encryption-kms-config mountPath: /etc/ceph-csi-encryption-kms-config/ - name: plugin-dir mountPath: /var/lib/kubelet/plugins mountPropagation: \"Bidirectional\" - name: mountpoint-dir mountPath: /var/lib/kubelet/pods mountPropagation: \"Bidirectional\" - name: keys-tmp-dir mountPath: /tmp/csi/keys - name: ceph-logdir mountPath: /var/log/ceph - name: ceph-config mountPath: /etc/ceph/ - name: liveness-prometheus securityContext: privileged: true image: quay.io/cephcsi/cephcsi:canary args: - \"--type=liveness\" - \"--endpoint=$(CSI_ENDPOINT)\" - \"--metricsport=8680\" - \"--metricspath=/metrics\" - \"--polltime=60s\" - \"--timeout=3s\" env: - name: CSI_ENDPOINT value: unix:///csi/csi.sock - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - name: socket-dir mountPath: /csi imagePullPolicy: \"IfNotPresent\" volumes: - name: socket-dir hostPath: path: /var/lib/kubelet/plugins/rbd.csi.ceph.com type: DirectoryOrCreate - name: plugin-dir hostPath: path: /var/lib/kubelet/plugins type: Directory - name: mountpoint-dir hostPath: path: /var/lib/kubelet/pods type: DirectoryOrCreate - name: ceph-logdir hostPath: path: /var/log/ceph type: DirectoryOrCreate - name: registration-dir hostPath: path: /var/lib/kubelet/plugins_registry/ type: Directory - name: host-dev hostPath: path: /dev - name: host-sys hostPath: path: /sys - name: etc-selinux hostPath: path: /etc/selinux - name: host-mount hostPath: path: /run/mount - name: lib-modules hostPath: path: /lib/modules - name: ceph-config configMap: name: ceph-config - name: ceph-csi-config configMap: name: ceph-csi-config - name: ceph-csi-encryption-kms-config configMap: name: ceph-csi-encryption-kms-config - name: keys-tmp-dir emptyDir: { medium: \"Memory\" } --- # This is a service to expose the liveness metrics apiVersion: v1 kind: Service metadata: name: csi-metrics-rbdplugin # replace with non-default namespace name namespace: default labels: app: csi-metrics spec: ports: - name: http-metrics port: 8080 protocol: TCP targetPort: 8680 selector: app: csi-rbdplugin EOF wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml kubectl apply -f csi-rbdplugin-provisioner.yaml wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml kubectl apply -f csi-rbdplugin.yaml 3、使用 ceph 块设备 1、创建 storageclass cat csi-rbd-sc.yaml --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: csi-rbd-sc provisioner: rbd.csi.ceph.com parameters: clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8 pool: k8s imageFeatures: layering csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret csi.storage.k8s.io/provisioner-secret-namespace: default csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret csi.storage.k8s.io/controller-expand-secret-namespace: default csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret csi.storage.k8s.io/node-stage-secret-namespace: default reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - discard EOF kubectl apply -f csi-rbd-sc.yaml 2、创建 PesistentVolumeClaim cat raw-block-pvc.yaml --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: raw-block-pvc spec: accessModes: - ReadWriteOnce volumeMode: Block resources: requests: storage: 1Gi storageClassName: csi-rbd-sc EOF kubectl apply -f raw-block-pvc.yaml cat raw-block-pod.yaml --- apiVersion: v1 kind: Pod metadata: name: pod-with-raw-block-volume spec: containers: - name: fc-container image: fedora:26 command: [\"/bin/sh\", \"-c\"] args: [\"tail -f /dev/null\"] volumeDevices: - name: data devicePath: /dev/xvda volumes: - name: data persistentVolumeClaim: claimName: raw-block-pvc EOF kubectl apply -f raw-block-pod.yaml cat pvc.yaml --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: rbd-pvc spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 1Gi storageClassName: csi-rbd-sc EOF kubectl apply -f pvc.yaml 4、使用 demo cat pod.yaml --- apiVersion: v1 kind: Pod metadata: name: csi-rbd-demo-pod spec: containers: - name: web-server image: nginx volumeMounts: - name: mypvc mountPath: /var/lib/www/html volumes: - name: mypvc persistentVolumeClaim: claimName: rbd-pvc readOnly: false EOF kubectl apply -f pod.yaml 二、before 一、介绍 https://medium.com/velotio-perspectives/an-innovators-guide-to-kubernetes-storage-using-ceph-a4b919f4e469 使用两种存储类型来与 k8s 集成 1、Ceph-RBD 2、CephFS PV 与 PVC ​ PersistentVolume (持久卷， 简称 PV)和Persistent VolumeClaim(持久卷声明，简称 PVC)使得K8s集群具备了存储的逻辑抽象能力，使得在配置Pod的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。存储的PV和PVC的这种关系，跟计算的Node和Pod的关系是非常类似的；PV和Node是资源的提供者，根据集群的基础设施变化而变化，由K8s集群管理员配置；而PVC和Pod是资源的使用者，根据业务服务的需求变化而变化，由K8s集群的使用者即服务的管理员来配置。 ​ 当集群用户需要在其pod中使用持久化存储时，他们首先创建PVC清单，指定所需要的最低容量要求和访问模式，然后用户将待久卷声明清单提交给Kubernetes API服务器，Kubernetes将找到可匹配的PV并将其绑定到PVC。PVC可以当作pod中的一个卷来使用，其他用户不能使用相同的PV，除非先通过删除PVC绑定来释放。 PV 的访问模型: accessModes: ReadWriteOnce【简写:RWO】: 单路读写，即仅能有一个节点挂载读写，是最基本的方式，可读可写，但只支持被单个Pod挂载。 ReadOnlyMany【ROX】: 多路只读，可以以只读的方式被多个Pod挂载。 ReadWriteMany【RWX】：多路读写，这种存储可以以读写的方式被多个Pod共享。不是每一种存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是NFS。 卷可以处于以下的某种状态： Available（可用），一块空闲资源还没有被任何声明绑定 Bound（已绑定），卷已经被声明绑定 Released（已释放），声明被删除，但是资源还未被集群重新声明 Failed（失败），该卷的自动回收失败 二、部署 Ceph（略） 三、Ceph-RBD https://github.com/ajaynemade/K8s-Ceph 1、安装 Ceph-RBD client vim Ceph-RBD-Provisioner.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: rbd-provisioner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: rbd-provisioner subjects: - kind: ServiceAccount name: rbd-provisioner namespace: kube-system roleRef: kind: ClusterRole name: rbd-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: Role metadata: name: rbd-provisioner rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: rbd-provisioner roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: rbd-provisioner subjects: - kind: ServiceAccount name: rbd-provisioner namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: rbd-provisioner --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: rbd-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: \"quay.io/external_storage/rbd-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/rbd serviceAccount: rbd-provisioner kubectl create -n kube-system -f Ceph-RBD-Provisioner.yaml output clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created deployment.extensions/rbd-provisioner created 2、检查 RBD volume 状态，等待状态为 Running kubectl get pods -l app=rbd-provisioner -n kube-system NAME READY STATUS RESTARTS AGE rbd-provisioner-857866b5b7-vc4pr 1/1 Running 0 16s 3、等待运行起来过后，配置 admin key 作为认证 ceph auth get-key client.admin AQDyWw9dOUm/FhAA4JCA9PXkPo6+OXpOj9N2ZQ== kubectl create secret generic ceph-secret \\ --type=\"kubernetes.io/rbd\" \\ --from-literal=key='AQDyWw9dOUm/FhAA4JCA9PXkPo6+OXpOj9N2ZQ==' \\ --namespace=kube-system 4、为 k8s 创建一个 ceph pool，并且创建一个 client key ceph --cluster ceph osd pool create k8s 1024 1024 ceph --cluster ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rwx pool=k8s' 5、获取创建的 auth token，并且为 kube pool 创建 k8s secret ceph --cluster ceph auth get-key client.k8s AQDabg9d4MBeIBAAaOhTjqsYpsNa4X10V0qCfw== kubectl create secret generic ceph-secret-k8s \\ --type=\"kubernetes.io/rbd\" \\ --from-literal=key=\"AQDabg9d4MBeIBAAaOhTjqsYpsNa4X10V0qCfw==\" \\ --namespace=kube-system 6、创建存储 class vim Ceph-RBD-StorageClass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: k8s provisioner: ceph.com/rbd # allowVolumeExpansion: true parameters: monitors: 10.0.1.118:6789, 10.0.1.227:6789, 10.0.1.172:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: k8s userId: k8s userSecretName: ceph-secret-k8s userSecretNamespace: kube-system imageFormat: \"2\" imageFeatures: layering kubectl create -f Ceph-RBD-StorageClass.yaml allowVolumeExpansion: true，允许扩容 bug：需要在pod/kube-controller-manager中安装ceph-mon，并拷贝ceph.client.admin.keyring到/etc/ceph/文件夹中。 如下编辑pvc，再重启pod即可实现扩容 kubectl edit pvc data-kafka-0 status: accessModes: ReadWriteOnce storage: 8Gi 7、到此已经配置完。接下来通过创建 PVC 来测试 Ceph-RBD。 vim Ceph-RBD-PVC.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: testclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: k8s kubectl create -f Ceph-RBD-PVC.yaml 8、检查pvc，就会发现它表明它已与存储类创建的pv绑定。 kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE testclaim Bound pvc-c215ad98-95b3-11e9-8b5d-12e154d66096 1Gi RWO fast-rbd 2m 9、检查 persistent volume（pv） kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-c215ad98-95b3-11e9-8b5d-12e154d66096 1Gi RWO Delete Bound default/testclaim fast-rbd 8m 四、CephFS 1、创建一个单独的命名空间 kubectl create ns cephfs 2、使用 ceph admin auth token，创建 k8s secret ceph auth get-key client.admin AQDyWw9dOUm/FhAA4JCA9PXkPo6+OXpOj9N2ZQ== kubectl create secret generic ceph-secret-admin --from-literal=key=\"AQDyWw9dOUm/FhAA4JCA9PXkPo6+OXpOj9N2ZQ==\" -n cephfs 3、创建 cluster role、role bonding、provisioner vim Ceph-FS-Provisioner.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfs roleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cephfs-provisioner namespace: cephfs roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner --- apiVersion: v1 kind: ServiceAccount metadata: name: cephfs-provisioner namespace: cephfs --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cephfs-provisioner namespace: cephfs spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: \"quay.io/external_storage/cephfs-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE value: cephfs command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: cephfs-provisioner kubectl create -n cephfs -f Ceph-FS-Provisioner.yaml 4、创建 storage class vim Ceph-FS-StorageClass.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: cephfs provisioner: ceph.com/cephfs parameters: monitors: 10.0.1.226:6789, 10.0.1.205:6789, 10.0.1.82:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: cephfs claimRoot: /pvc-volumes kubectl create -f Ceph-FS-StorageClass.yaml 5、到此配置完成。等待状态变为 Running kubectl get pods -n cephfs NAME READY STATUS RESTARTS AGE cephfs-provisioner-8d957f95f-s7mdq 1/1 Running 0 1m 6、CephFS提供程序启动后，请尝试创建持久卷声明。 在此步骤中，存储类将负责动态创建持久卷。 vim Ceph-FS-PVC.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: storageClassName: cephfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi kubectl create -f Ceph-FS-PVC.yaml 7、检查 pv 和 pvc kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-a7db18a7-9641-11e9-ab86-12e154d66096 1Gi RWX cephfs 2m kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-a7db18a7-9641-11e9-ab86-12e154d66096 1Gi RWX Delete Bound default/claim1 cephfs 2m 五、测试 CephFS 1、创建 pvc vim cephfs-pvc-test.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs-claim spec: accessModes: - ReadWriteOnce storageClassName: dynamic-cephfs resources: requests: storage: 2Gi kubectl apply -f cephfs-pvc-test.yaml 2、查看 kubectl get pvc kubectl get pv 3、创建 nginx pod 挂载测试 vim nginx-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx-pod1 labels: name: nginx-pod1 spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: ceph-rdb mountPath: /usr/share/nginx/html volumes: - name: ceph-rdb persistentVolumeClaim: claimName: cephfs-claim kubectl apply -f nginx-pod.yaml 4、查看 kubectl get pods -o wide 5、修改访问内容 kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from Ceph RBD!!! > /usr/share/nginx/html/index.html' 6、访问测试 POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '{print $6}') curl http://$POD_ID 7、清理 kubectl delete -f nginx-pod.yaml kubectl delete -f cephfs-pvc-test.yaml 六、测试 Ceph-RBD # 创建pvc测试 cat >ceph-rdb-pvc-test.yamlnginx-pod.yaml /usr/share/nginx/html/index.html' # 访问测试 POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '{print $6}') curl http://$POD_ID # 清理 kubectl delete -f nginx-pod.yaml kubectl delete -f ceph-rdb-pvc-test.yaml "},"notes/docker/chartmuseum.html":{"url":"notes/docker/chartmuseum.html","title":"chartmuseum 相关","keywords":"","body":"chartmuseum 相关 chartmuseum github 一、安装 chartmuseum 1、添加 helm 源 helm repo add stable https://charts.helm.sh/stable 2、拉取 chart 包 helm pull stable/chartmuseum --untar 3、修改配置 vim chartmuseum/value.yaml DISABLE_API: false type: NodePort 4、部署 helm install chartmuseum/ 二、通过 helm 添加使用 helm repo add chartmuseum http://localhost:8080 helm search repo chartmuseum helm install chartmuseum/mychart helm push mychart/ chartmuseum 三、通过 curl 命令，进行 CRUD 操作 1、查看仓库信息 curl http://localhost:8080/index.yaml 2、查看所有软件 curl http://localhost:8080/api/charts 3、查看某个软件的所有版本信息 curl http://localhost:8080/api/charts/nginx 4、查看某个软件的具体版本的信息 curl http://localhost:8080/api/charts/nginx/5.1.5 5、下载软件包 curl -O http://localhost:8080/charts/nginx-5.1.5.tgz 6、上传软件包到仓库 curl --data-binary \"@rancher-2.5.1.tgz\" http://localhost:8080/api/charts curl -X POST -k --data-binary \"@mychart-1.0-0.1.1.tgz\" localhost:8080/api/charts 7、删除仓库中的软件 curl -X DELETE http://localhost:8080/api/charts/nginx/5.1.5 8、更新镜像描述 curl -X PUT -u admin:123 -H 'accept: application/json' -H 'Content-Type: application/json' -d '{\"description\": \"aaaa\"}' http://192.168.0.127:3000/api/v2.0/projects/ai/repositories/detection 9、获取镜像信息 curl -X GET -u admin:123 http://192.168.0.127:3000/api/v2.0/projects/ai/repositories/detection |jq "},"notes/docker/harbor.html":{"url":"notes/docker/harbor.html","title":"harbor 相关","keywords":"","body":"harbor 相关 1、安装 docker curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 2、安装 docker-compose curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 3、下载 harbor https://github.com/goharbor/harbor wget https://github.com/goharbor/harbor/releases/download/v2.0.0-rc2/harbor-offline-installer-v2.0.0-rc2.tgz 4、生成证书 1、生成私钥 openssl genrsa -out ca.key 4096 2、生成证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Chengdu/L=Chengdu/O=example/OU=Personal/CN=local.com\" \\ -key ca.key \\ -out ca.crt 5、生成 server 端证书 1、生私钥 openssl genrsa -out local.com.key 4096 2、生成证书请求文件 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Chengdu/L=Chengdu/O=example/OU=Personal/CN=local.com\" \\ -key local.com.key \\ -out local.com.csr 3、生成 x509 v3 cat > v3.ext 4、使用 v3.ext 文件生成 harbor 主机证书 openssl x509 -req -sha512 -days 3650 \\ -extfile v3.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in local.com.csr \\ -out local.com.crt 5、提供证书给harbor或docker cp local.com.crt /data/cert/ cp local.com.key /data/cert/ 6、转换 .crt 为 .cert， 以给 docker 使用 openssl x509 -inform PEM -in local.com.crt -out local.com.cert 7、创建文件和复制 server 端证书、key 、Ca文件到 harbor 主机上的 docker 的认证文件夹 mkdir -p /etc/docker/certs.d/local.com/ cp local.com.cert /etc/docker/certs.d/local.com/ cp local.com.key /etc/docker/certs.d/local.com/ cp ca.crt /etc/docker/certs.d/local.com/ 8、重启 docker systemctl restart docker 6、准备和配置 harbor配置文件 tar -zxvf harbor-offline-installer-v2.0.0-rc2.tgz cd harbor/ cp harbor.yml.tmpl harbor.yml vim harbor.yml hostname: local.com https: certificate: /etc/docker/certs.d/local.com/local.com.cert private_key: /etc/docker/certs.d/local.com/local.com.key data_volume: /data 将prepare中的镜像goharbor/prepare:v2.0.0 改为 goharbor/prepare:v2.0.0-dev ./prepare 7、部署harbor ./install.sh --with-chartmuseum 8、重装（如需执行） docker-compose down -v docker-compose up -d ./install.sh --with-chartmuseum 9、使用 一、docker image 1、将证书拷贝到需要使用harbor的主机 scp -r 127.0.0.1:/etc/docker/certs.d/local.com/ *.*.*.*:/etc/docker/certs.d/ 2、编辑 /etc/docker/daemon.json { \"insecure-registries\": [\"local.com\"], } 3、重启 docker systemctl restart docker 4、配置 hosts 文件 127.0.0.1 local.com 5、登录 docker login local.com 6、使用 for i in `docker images|egrep -v '9001|none|REPOSITORY'|awk '{print $1\":\"$2}'`;do docker tag $i \"local.com/stx/\"$i;done for i in `docker images|grep local.com|awk '{print $1\":\"$2}'`;do docker push $i;done 二、chart 1、添加仓库 helm repo add --ca-file /etc/docker/certs.d/local.com/ca.crt --username=admin --password=qwe local https://local.com/chartrepo/k8s 2、push helm push --ca-file /etc/docker/certs.d/local.com/ca.crt nginx-6.0.1.tgz local helm push grafana-0.0.2.tgz test --username hl --password xxx 3、pull helm pull local/nginx 4、安装 helm install --ca-file=ca.crt --username=admin --password=Passw0rd --version 6.0.1 local/nginx 10、忘记密码 1、运行harbor-db容器 docker exec -it harbor-db bash 2、进入数据库并选择表 psql -U postgres -d registry 3、查看用户信息 select * from harbor_user; admin | | ce058f765a36af71a7c43d95710d6074 | system admin | admin user | f | | rdk5buqqqi975je9hcv6hv0040agvf3b | t | 2020-07-01 09:48:17.048602 | 2020-07-01 09:48:20.272419 | sha256 4、更新密码（当前harbor版本v2.0.0） 密码修改后为admin/qwe update harbor_user set salt='rdk5buqqqi975je9hcv6hv0040agvf3b', password='ce058f765a36af71a7c43d95710d6074' where user_id = 1; 可删除data/database内所有内容，重新部署，获取harbor_user表的相关字段 "},"notes/docker/helm.html":{"url":"notes/docker/helm.html","title":"helm 相关","keywords":"","body":"helm 相关 一、快速安装 helm release wget https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz tar -zxvf helm-*-linux-amd64.tar.gz cp helm /usr/local/bin/helm chmod 777 /usr/local/bin/helm https://github.com/chartmuseum/helm-push releases wget https://github.com/chartmuseum/helm-push/releases/download/v0.10.2/helm-push_0.10.2_linux_amd64.tar.gz HELM_PUSH=${HOME}/.local/share/helm/plugins mkdir -p ${HELM_PUSH} tar -zxvf helm-push-*.tar.gz -C ${HELM_PUSH} cp registries.yaml /etc/cloud/k8s/ systemctl restart k8s 二、Helm v3 三、Helm 客户端安装（v3.0.0） 1、使用官方脚本安装 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 或 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh chmod 700 get_helm.sh ./get_helm.sh 2、添加常用 Chart 源 #应该都不需要墙，stable是官方的，aliyuncs最快 helm repo add stable https://kubernetes-charts.storage.googleapis.com helm repo add aliyuncs https://apphub.aliyuncs.com helm repo add bitnami https://charts.bitnami.com/bitnami 3、查看 Chart 源 helm repo list 4、查找应用 helm search repo tomcat 5、直接从 Chart 在线安装，需要实现创建动态存储卷等。 helm install my-tomcat aliyuncs/tomcat NAME: my-tomcat LAST DEPLOYED: Thu Dec 5 13:56:04 2019 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** 1. Get the Tomcat URL by running: ** Please ensure an external IP is associated to the my-tomcat service before proceeding ** ** Watch the status using: kubectl get svc --namespace default -w my-tomcat ** export SERVICE_IP=$(kubectl get svc --namespace default my-tomcat --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\") echo URL : http://$SERVICE_IP/ echo Management URL : http://$SERVICE_IP/manager 2. Login with the following credentials echo Username: user echo Password: $(kubectl get secret --namespace default my-tomcat -o jsonpath=\"{.data.tomcat-password}\" | base64 --decode) 6、离线安装 helm pull aliyuncs/tomcat 解压 tar -zxvf tomcat-6.0.5.tgz tomcat/Chart.yaml tomcat/values.yaml tomcat/templates/NOTES.txt tomcat/templates/_helpers.tpl tomcat/templates/deployment.yaml tomcat/templates/ingress.yaml tomcat/templates/pvc.yaml tomcat/templates/secrets.yaml tomcat/templates/svc.yaml tomcat/.helmignore 修改 tomcat/values.yaml，改为 NodePort 方式，并取消动态存储卷配置。 vim tomcat/values.yaml service: type: LoadBalancer # HTTP Port port: 80 改为 service: type: NodePort # HTTP Port port: 80 ########################## persistence: enabled: true 改为 persistence: enabled: false 离线安装 tomcat helm install my-tomcat tomcat NAME: my-tomcat LAST DEPLOYED: Thu Dec 5 14:06:40 2019 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** 1. Get the Tomcat URL by running: export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].nodePort}\" services aa-tomcat) export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT/ 2. Login with the following credentials echo Username: user echo Password: $(kubectl get secret --namespace default aa-tomcat -o jsonpath=\"{.data.tomcat-password}\" | base64 --decode) 四、常用功能 1、创建一个 Chart 模板 helm create test 2、打包 Chart 模板 helm package test 3、查看 Chart 信息 helm show chart test-0.1.0.tgz 4、先移除原先的仓库 helm repo remove stable 5、添加新的仓库地址 helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 6、更新仓库 helm repo update 五、使用 harbor 作为仓库存储 Charts https://github.com/goharbor/harbor-helm 1、添加 harbor repo helm repo add goharbor https://helm.goharbor.io 2、创建命名空间 test（可以跳过） kubectl create namespace test 3、查看当前context（可以跳过） kubectl config current-context 4、设置 context 指定对应的 namespace，不指定使用的是 default（可以跳过） kubectl config set-context --namespace test 这里是因为，helm 3 开始helm 3 的执行权限和kubectl config 的权限是一致的，通过kubectl config的方式来控制helm 3 的执行权限。 5、可以提前拉取以下镜像（harbor-1.2.3） docker pull goharbor/chartmuseum-photon:v0.9.0-v1.9.3 docker pull goharbor/redis-photon:v1.9.3 docker pull goharbor/clair-photon:v2.1.0-v1.9.3 docker pull goharbor/notary-server-photon:v0.6.1-v1.9.3 docker pull goharbor/notary-signer-photon:v0.6.1-v1.9.3 docker pull goharbor/harbor-registryctl:v1.9.3 docker pull goharbor/registry-photon:v2.7.1-patch-2819-2553-v1.9.3 docker pull goharbor/nginx-photon:v1.9.3 docker pull goharbor/harbor-jobservice:v1.9.3 docker pull goharbor/harbor-core:v1.9.3 docker pull goharbor/harbor-portal:v1.9.3 6.1、安装 harbor， 关闭数据卷挂载，并使用 NodePort 方式进行访问。 helm -n test install harbor goharbor/harbor \\ --set persistence.enabled=false \\ --set expose.type=nodePort \\ --set expose.tls.enabled=false \\ --set externalURL=http://192.168.0.11:30002 参数说明： persistence.enabled=false 关闭存储，为了方便操作，真实使用时需要挂在存储 expose.type=nodePort 使用 NodePort 访问 expose.tls.enabled=false 关闭tls externalURL=http://192.168.0.11:30002 设置登录 harbor 的外部链接 6.2、配置 harbor 使用 Ceph 持续存储（根据情况可选） 查看已有的 storageclasses kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 8h 下载 harbor helm pull goharbor/harbor 解压 tar -zxvf harbor-1.2.3.tgz vim harbor/values.yaml expose: type: nodePort tls: enabled: false externalURL: http://192.168.0.11:30002 persistence: persistentVolumeClaim: registry: # 所有 storageClass 都修改为以下内容 storageClass: \"ceph-rbd\" 部署 helm install harbor harbor/ 7、访问界面登录 http://192.168.0.11:30002 默认账号密码 admin/Harbor12345 8、添加仓库 chart_repo 9、创建用户 hl/XXX 10.1、添加 repo 到 helm helm repo add hl http://192.168.0.11:30002/chartrepo/chart_repo 10.2、添加 repo 到 helm，及添加认证信息（根据情况可选） helm repo add hl http://192.168.0.11:30002/chartrepo/chart_repo --username hl_user --password hl_password helm repo add --ca-file /etc/docker/certs.d/local.com/ca.crt --username=admin --password=qwe local https://local.com/chartrepo/k8s 11、安装 helm-push 插件 helm plugin install https://github.com/chartmuseum/helm-push 12、push charts 到 harbor 里面 helm push grafana-0.0.2.tgz test --username hl --password xxx 六、其他 1、helm hub helm hub 2、helm 官网 https://v3.helm.sh/docs/ 3、通过 curl 创建仓库 curl -u \"user:password\" -H \"Content-Type: application/json\" -X POST -d '{\"project_name\": \"test\", \"public\": true}' https://192.168.0.127:5000/api/v2.0/projects -k 4、registry相关操作 1.登录 helm registry login my.harbor.com:5000 --username user --password password --insecure 2.打包、推送 helm package template helm push template-1.0.0.tgz oci://my.harbor.com:5000/test --insecure-skip-tls-verify 3.拉取 helm pull oci://my.harbor.com:5000/test/template --version 1.0.0 5、部署 helm upgrade --install --create-namespace \\ -n hl \\ --set-json 'image={\"repository\":\"gradiant/jupyter\",\"tag\":\"6.0.3\"}' \\ --set replicaCount=1 \\ --set-json 'resource={\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"},\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}' \\ --set-json 'containerPorts=[{\"name\":\"http\",\"port\":80}]' \\ --set-json 'service={\"type\":\"ClusterIP\",\"ports\":[{\"name\":\"http\",\"port\":80,\"targetPort\":\"http\"}]}' \\ --set-json 'ingress={\"enabled\":true,\"hosts\":[{\"host\":\"hl.test.ingress\",\"paths\":[{\"path\":\"/\",\"port\":80}]}]}' \\ hl \\ oci://my.harbor.com:5000/test/template --version 1.0.0 helm pull oci://my.harbor.com:5000/test/gradiant/jupyter --version 6.0.3 helm upgrade --install -n {NAMESPACE} sagflow . helm install -n {NAMESPACE} -f values.yaml jupyter jupyter-6.0.3.tgz "},"notes/docker/docker_net.html":{"url":"notes/docker/docker_net.html","title":"docker 网络相关","keywords":"","body":"docker 网络相关 一、docker 五种网络模式 在docker run --net选项指定容器网络模式，docker有以下4种网络模式： docker使用linux的namespace技术来进行资源隔离，如pid namespace隔离进程，mount namespace隔离文件系统，network namespace提供一份独立的网络环境，包括网卡、路由、iptable规则等都与其他的network namespace隔离。一个docker容器一般会分配一个独立的network namespace。 1、host模式，--net=host 但如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的network namespace，而是贺宿主机共用一个network namespace。容器将不会虚拟出自己的网卡、配置自己的ip等，而是使用宿主机的ip和端口。容器进程可以跟主机其它 root 进程一样可以打开低范围的端口，可以访问本地网络服务比如 D-bus，还可以让容器做一些影响整个主机系统的事情，比如重启主机。因此使用这个选项的时候要非常小心。如果进一步的使用 --privileged=true，容器会被允许直接配置主机的网络堆栈。 例如，我们在10.10.101.105/24的机器上用host模式启动一个含有web应用的docker容器，监听tcp 80端口。在容器中执行任何类似ifconfig命令查看网络环境时，看到的都是宿主机上的信息。而外界访问容器中的应用，则直接使用10.10.101.105:80即可，不用任何nat转换，就如直接跑在宿主机中一样。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。 2、container模式，--net=container:NAME_or_ID 这个模式指定新创建的容器和已经存在的一个容器共享一个network namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡、配置自己的ip，而是和一个指定的容器共享ip、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo回环网卡设备通信。 3、none模式，--net=none 这个模式和前两个不同。在这种模式下，docker容器拥有自己的network namespace，但是，并不为docker容器进行任何网络配置。也就是说，这个docker容器，只用lo回环网络，没有网卡、ip、路由等信息。需要我们自己为docker容器添加网卡、配置ip等。这个封闭的网络很好的保证了容器的安全性。 4、bridge模式，--net=bridge bridge模式是docker默认的网络设置，此模式会为每一个容器分配network namespace、设置ip等，并将一个主机上的docker容器连接到一个虚拟网桥上。 1.在主机上创建一对虚拟网卡veth pair设备。veth设备总是成对出现的，它们组成了一个数据的通道，数据从一个设备进入，就会从另一个设备出来。因此，veth设备常用来连接两个网络设备。 2.docker将veth pair设备的一端放在新创建的容器中，并命名为eth0。另一端放在主机中，以veth65f9这样类似的名字命名，并将这个网络设备加入到docker0网桥中，可以通过brctl show命令查看。 3.从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。 5、user-defined模式 用户自定义模式，主要可选的有三种网络驱动：bridge、overlay、macvlan。bridge驱动用于创建类似于前面提到的bridge网络。overlay和macvlan驱动主要用于跨主机的网络。 overlay模式 overlay网络将多个docker守护进程连接在一起，并使集群服务能够相互通信。还可以使用overlay网络来实现swarm集群和独立容器之间的通信，或者不同docker守护进程上的两个独立容器之间的通信。该策略实现了在这些容器之间进行操作系统级别路由的需求。 macvlan模式 macvlan网络允许为容器分配mac地址，使其显示为网络上的物理设备。docker守护进程通过其mac地址将流量路由到容器。对于希望直连到物理网络的传统应用程序而言，使用macvlan模式一般是最佳的选择，而不应该通过docker宿主机的网络进行路由。 二、跨主机通讯模式 docker在垮主机通信方面一直比较弱。目前主要有容器网络模型（CNM）和容器网络接口（CNI）。k8s和docker之间通信采纳的是CNI。 1、CNM模式（container network model） CNM是一个被 Docker 提出的规范。现在已经被Cisco Contiv, Kuryr, Open Virtual Networking (OVN), Project Calico, VMware 和 Weave 这些公司和项目所采纳。 libnetwork是CNM的原生实现。它为docker daemon和网络驱动程序之间提供了接口。网络控制器负责将驱动和一个网络进行对接。每个驱动程序负责管理它所拥有的网络以及为该网络提供的各种服务。网络驱动可以按提供方式被划分为原生驱动（libnetwork内置的或docker支持的）或者远程驱动（第三方插件）。原生驱动包括none、bridge、overlay以及macvlan。驱动也可以按照使用范围被划分为本地（单主机）和全局（多主机）。 2、CNI模式（container network interface） CNI是google和coreos主导制定的容器网络标准，可以理解成一个协议。这个标准是在rkt网络提议基础上发展去来的，综合考虑了灵活性、扩展性、ip分配、多网卡等因素。 CNI本身实现了一些基本插件，比如bridge、ipvlan、macvlan、loopback、vlan等网络接口管理插件，还有dhcp、host-local等ip管理插件，并且主流的container网络解决方案都有对应CNI的支持能力，比如Flannel、Calico、Weave、Contiv、SR-IOV、Amazon ECS CNI Plugins等。 三、docker swarm 网络 q: quantum v: veth b: bridge o: openvswitch qvb: quamtum veth bridge qvo: quamtum veth openvswitch br-int(集成网桥) br-int是由OpenvSwitch虚拟化出来的网桥，但事实上它已经充当了一个虚拟交换机的功能了。br-int的主要职责就是把它所在的计算节点上的VM都连接到它这个虚拟交换机上面，然后利用br-tun的穿透功能，实现了不同计算节点上的VM连接在同一个逻辑上的虚拟交换机上面的功能。 br-tun(通道网桥) br-tun同样也是是OpenvSwitch虚拟化出来的网桥，但是它不是用来充当虚拟交换机的，它的存在只是用来充当一个通道层，通过它上面的设备G与其他物理机上的br-tun通信，构成一个统一的通信层。这样的话，网络节点和计算节点、计算节点和计算节点这样就会点对点的形成一个以GRE为基础的通信网络，互相之间通过这个网络进行大量的数据交换。这样，网络节点和计算节点之间的通信就此打通了 br-ex(外部网桥) 当数据从router中路由出来后，就会通过L、K传送到br-ex这个虚拟网桥上，而br-ex实际上是混杂模式加载在物理网卡上，实时接收着网络上的数据包。至此，计算节点上的VM就可以与外部的网络进行自由的通信了。当然，前提是要给这个VM已经分配了float-ip。 overlay networks 管理swarm中docker守护进程间的通信，可以将服务附加到一个或多个已存在的overlay网络上，使得服务与服务之间能够通信。 ingress network 是一个特殊的overlay网络，用于服务节点间的负载均衡。但任何swarm节点在发布的端口上接受到请求时，它将该请求交给一个名为IPVS的模块。IPVS跟踪参与该服务的所有IP地址，选择其中一个，并通过ingress网络将请求路由到它。 初始化或加入swarm集群时会自动创建ingress网络，大多数情况下，用户不需要自定义配置。 ingress ingress就是为了进入集群的请求提供路由规则的集合，ingress可以给k8s service提供集群外部访问的URL、负载均衡、SSL终止、HTTP路由等。为了配置这些ingress规则，集群管理员需要部署一个ingress controller，它监听ingress和service的变化，并根据规则配置负载均衡并提供访问入口。每个ingress都需要配置rules，目前k8s仅支持http规则。 docker_gwbridge 是一种桥接网络，将overlay网络（包括ingress网络）连接到一个单独的docker守护进程的物理网络。默认情况下，服务正在运行的每个容器都连接到本地docker守护进程主机的docker_gwbridge网络。 docker_gwbridge网络在初始化或加入swarm时自动创建。大多数情况下，用户不需要自定义配置，但是docker允许自定义。 （一）查看容器网络有三张网卡 # docker exec 4b805190a52b ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 80: eth0@if81: mtu 1450 qdisc noqueue state UP link/ether 02:42:0a:ff:00:21 brd ff:ff:ff:ff:ff:ff inet 10.255.0.33/16 brd 10.255.255.255 scope global eth0 valid_lft forever preferred_lft forever 84: eth2@if85: mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:12:00:09 brd ff:ff:ff:ff:ff:ff inet 172.18.0.9/16 brd 172.18.255.255 scope global eth2 valid_lft forever preferred_lft forever 86: eth1@if87: mtu 1450 qdisc noqueue state UP link/ether 02:42:0a:00:04:13 brd ff:ff:ff:ff:ff:ff inet 10.0.4.19/24 brd 10.0.4.255 scope global eth1 valid_lft forever preferred_lft forever （二）网卡eth2@if85，172.18.0.9/16 1、查看宿主机网络，启动有一个网桥docker_gwbridge # docker network ls NETWORK ID NAME DRIVER SCOPE b7d628709937 docker_gwbridge bridge local 2、查看docker_gwbridge详情，能看到容器4b805190a5的网络挂载在该网桥上 # docker inspect docker_gwbridge [ { \"Name\": \"docker_gwbridge\", \"Id\": \"b7d6287099378a319ac5649af8bf40a1aa4ecdaa35a1d88d202922a94ebc9a02\", \"Created\": \"2019-05-23T09:44:30.296117498Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"172.18.0.0/16\", \"Gateway\": \"172.18.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": { \"Network\": \"\" }, \"ConfigOnly\": false, \"Containers\": { \"4b805190a52b793ee947654e2dc290535c17d134d52391a4ac53fc22ada7056b\": { \"Name\": \"gateway_29319ccc19b3\", \"EndpointID\": \"ede79bc08c8d4acbc18f893e20975bef25526b4a6a279d0ab026a545f4a4f040\", \"MacAddress\": \"02:42:ac:12:00:09\", \"IPv4Address\": \"172.18.0.9/16\", \"IPv6Address\": \"\" }, \"ingress-sbox\": { \"Name\": \"gateway_ingress-sbox\", \"EndpointID\": \"7d8688816cc8f54a7730b4d6a9bf70425fc5202a5cd39a2f23b51741e6443bcc\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.18.0.2/16\", \"IPv6Address\": \"\" } }, \"Options\": { \"com.docker.network.bridge.enable_icc\": \"false\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.name\": \"docker_gwbridge\" }, \"Labels\": {} } ] 3、查看宿主机iptables规则，发往172.18.0.0/16的包都转到docker_gwbridge网桥上 # iptables -t nat -S -A POSTROUTING -o docker_gwbridge -m addrtype --src-type LOCAL -j MASQUERADE -A POSTROUTING -s 172.18.0.0/16 ! -o docker_gwbridge -j MASQUERADE -A DOCKER -i docker_gwbridge -j RETURN 4、分别查看宿主机和容器内地址情况，可以看到宿主机的 vethc0a30fb@if84 对应容器内的 eth2@if85，设备号相连 # ip ad 8: docker_gwbridge: mtu 1500 qdisc noqueue state UP group default link/ether 02:42:e5:6e:d4:18 brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global docker_gwbridge valid_lft forever preferred_lft forever inet6 fe80::42:e5ff:fe6e:d418/64 scope link valid_lft forever preferred_lft forever 85: vethc0a30fb@if84: mtu 1500 qdisc noqueue master docker_gwbridge state UP group default link/ether 16:8e:ba:d8:93:7d brd ff:ff:ff:ff:ff:ff link-netnsid 16 inet6 fe80::148e:baff:fed8:937d/64 scope link valid_lft forever preferred_lft forever # docker exec 4b805190a52b ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 84: eth2@if85: mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:12:00:09 brd ff:ff:ff:ff:ff:ff inet 172.18.0.9/16 brd 172.18.255.255 scope global eth2 valid_lft forever preferred_lft forever （三）网卡eth0@if81，10.255.0.33/16 1、查看网络信息，overlay的网络 ingress # docker network ls NETWORK ID NAME DRIVER SCOPE b7d628709937 docker_gwbridge bridge local mp3mrjuxu0et ingress overlay swarm 2、查看ingress详情，子网10.255.0.0/16，同一个命名空间的容器有10.255.0.33/16 ]# docker inspect ingress [ { \"Name\": \"ingress\", \"Id\": \"mp3mrjuxu0et4io8q1yprzour\", \"Created\": \"2019-05-23T09:44:29.413075311Z\", \"Scope\": \"swarm\", \"Driver\": \"overlay\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"10.255.0.0/16\", \"Gateway\": \"10.255.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Ingress\": true, \"ConfigFrom\": { \"Network\": \"\" }, \"ConfigOnly\": false, \"Containers\": { \"4b805190a52b793ee947654e2dc290535c17d134d52391a4ac53fc22ada7056b\": { \"Name\": \"218aacad-5a9d-4f70-86a4-309031c76ea8_my_kafka1.1.f6nspi9w41ka9fkqs8a81i95b\", \"EndpointID\": \"1a65106d3c0b470f0ef4d140d7d5537321ff5a2061696a3d2f18c9334195ac51\", \"MacAddress\": \"02:42:0a:ff:00:21\", \"IPv4Address\": \"10.255.0.33/16\", \"IPv6Address\": \"\" }, \"ingress-sbox\": { \"Name\": \"ingress-endpoint\", \"EndpointID\": \"5888a58e173a0a72ee8a48a57259accb17e3c90e815b4d93de780b3f7951f358\", \"MacAddress\": \"02:42:0a:ff:00:03\", \"IPv4Address\": \"10.255.0.3/16\", \"IPv6Address\": \"\" } }, \"Options\": { \"com.docker.network.driver.overlay.vxlanid_list\": \"4096\" }, \"Labels\": {}, \"Peers\": [ { \"Name\": \"803317bab0a7\", \"IP\": \"20.1.1.11\" }, { \"Name\": \"48c5f2b4c862\", \"IP\": \"20.1.1.13\" } ] } ] 3、查看命名空间ingress-sbox的iptables规则 # nsenter --net=/var/run/docker/netns/ingress_sbox iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT -N DOCKER_OUTPUT -N DOCKER_POSTROUTING -A POSTROUTING -d 10.255.0.0/16 -m ipvs --ipvs -j SNAT --to-source 10.255.0.3 4、查看命名空间ingress-sbox的地址情况 # nsenter --net=/var/run/docker/netns/ingress_sbox ip ad 6: eth0@if7: mtu 1450 qdisc noqueue state UP group default link/ether 02:42:0a:ff:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.255.0.3/16 brd 10.255.255.255 scope global eth0 valid_lft forever preferred_lft forever 5、查看命名空间ingress，mp3mrjuxu0et # nsenter --net=/var/run/docker/netns/1-mp3mrjuxu0 ip a 5: vxlan0: mtu 1450 qdisc noqueue master br0 state UNKNOWN group default link/ether 7e:49:77:a5:a8:32 brd ff:ff:ff:ff:ff:ff link-netnsid 0 7: veth0@if6: mtu 1450 qdisc noqueue master br0 state UP group default link/ether b2:d9:de:6a:63:b5 brd ff:ff:ff:ff:ff:ff link-netnsid 1 6、查看容器端口，有一个20229 # docker inspect 4b805190a52b [ { \"Config\": { \"Env\": [ \"KAFKA_ADVERTISED_HOST_NAME=192.168.21.192\", \"KAFKA_ADVERTISED_PORT=20229\", \"KAFKA_BROKER_ID=1\", \"KAFKA_ZOOKEEPER_CONNECT=my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181,\"] } } ] 7、查看ingress mangle表，找到目的端口到20229的规则 # nsenter --net=/var/run/docker/netns/ingress_sbox iptables -t mangle -S -A PREROUTING -p tcp -m tcp --dport 20229 -j MARK --set-xmark 0x11e/0xffffffff 8、查看对应规则的 0x11e 对应10进制值 (venv36) [root@hl ~]# python Python 3.6.6 (default, Aug 13 2018, 18:24:23) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> int('0x11e',16) 286 >>> 9、查看ipvsadm信息，找到端口转发到10.255.0.33 # yum -y install ipvsadm # nsenter --net=/var/run/docker/netns/ingress_sbox ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn FWM 262 rr -> 10.255.0.13:0 Masq 1 0 0 FWM 286 rr -> 10.255.0.33:0 Masq 1 0 0 四、其他 1、查看容器网络 docker network ls 2、使用命名空间进入虚拟机 ip netns ls ip netns exec qdhcp-2e6ff4d6-b5ee-4390-892c-568ab5d71107 ip a ip netns exec qdhcp-2e6ff4d6-b5ee-4390-892c-568ab5d71107 ssh root@20.1.1.11 "},"notes/docker/docker_swarm.html":{"url":"notes/docker/docker_swarm.html","title":"docker swarm 相关","keywords":"","body":"docker swarm 相关 一、docker swarm常用命令 docker swarm集群开放了三个端口： 2377端口， 用于集群管理通信 7946端口， 用于集群节点之间的通信 4789端口， 用于overlay网络流量 1、初始化swarm manager并制定网卡地址 docker swarm init --advertise-addr 192.168.10.10 Swarm initialized: current node (anvkkvtgrxloqwleiyjjsw2gh) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-08ejd2bmn0yhns2cvgkpa0vnu095i4yc2t8jlbdqyvd3tekx6h-e5ore8jctvoycgw2vb1elne2z 192.168.110.82:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 2、强制删除集群，如果是manager，需要添加 --force、 docker swarm leave --force docker node rm node1 --force # 在manager节点上执行 docker swarm update 3、查看swarm worker的连接令牌 docker swarm join-token worker 4、查看swarm manager的连接令牌 docker swarm join-token manager 5、使旧令牌无效并生成新令牌 docker swarm join-token --rotate docker node inspect node1 --pretty 6、将节点升级为manager docker node promote node1 7、将节点降级为worker docker node demote node1 8、查看服务列表 docker service ls 9、查看服务的具体信息 docker service ps nginx 10、创建一个指定name、version、run cmd的服务 docker service create --name helloworld alping:3.6 ping docker.com 11、创建一个指定name、port、replicas的服务 docker service create --name my_web --replicas 3 -p 80:80 nginx 12、为指定的服务更新一个端口 docker service update --publish-add 80:80 my_web 13、为指定的服务删除一个端口 docker service update --publish-rm 80:80 my_web 14、将redis:3.0.6更新至redis:3.0.7 docker service update --image redis:3.0.7 redis 15、配置运行环境，指定工作目录及环境变量 docker service create --name helloworld --env MYVAR=myvalue --workdir /tmp --user my_user alping ping docker.com 16、更新helloworld服务的运行命令 docker service update --args \"ping www.baidu.com\" helloworld 17、创建一个overlay网络 docker network create --driver overlay my_network docker network create --driver overlay --subnet 10.10.10.0/24 --gateway 10.10.10.1 my-network docker service create --name test --replicas 3 --network my-network redis docker service update --network-rm my-network test docker service update --network-add my_network test 18、创建群组并配置cpu和内存 docker service create --name my_nginx --reserve-cpu 2 --reserve-memory 512m --replicas 3 nginx docker service update --reserve-cpu 1 --reserve-memory 256m my_nginx 19、更新配置 docker config ls docker config inspect mysql docker config rm mysql docker service update --config-add mysql mysql docker service update --config-rm mysql mysql docker config create homepage index.html docker service create --name nginx --publish 80:80 --replicas 3 --config src=homepage,target=/usr/share/nginx/html/index.html nginx 20、使用编排文件 version: '3.5' networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: driver: overlay name: kafaka_net_218aacad-5a9d-4f70-86a4-309031c76ea8 services: my_kafka1: depends_on: - my_kafka_zk1 - my_kafka_zk2 - my_kafka_zk3 environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.21.192 KAFKA_ADVERTISED_PORT: 20229 KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181, healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - my_kafka_zk1 - '2181' timeout: 10s image: 192.168.21.12:5000/tmp/kafka:2.12-2.1.0 labels: - com.tmp.kafka.role=kafka networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka1 ports: - mode: ingress protocol: tcp published: 20229 target: 9092 my_kafka2: depends_on: - my_kafka_zk1 - my_kafka_zk2 - my_kafka_zk3 environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.21.192 KAFKA_ADVERTISED_PORT: 20230 KAFKA_BROKER_ID: 2 KAFKA_ZOOKEEPER_CONNECT: my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181, healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - my_kafka_zk1 - '2181' timeout: 10s image: 192.168.21.12:5000/tmp/kafka:2.12-2.1.0 labels: - com.tmp.kafka.role=kafka networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka2 ports: - mode: ingress protocol: tcp published: 20230 target: 9092 my_kafka3: depends_on: - my_kafka_zk1 - my_kafka_zk2 - my_kafka_zk3 environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.21.192 KAFKA_ADVERTISED_PORT: 20231 KAFKA_BROKER_ID: 3 KAFKA_ZOOKEEPER_CONNECT: my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181, healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - my_kafka_zk1 - '2181' timeout: 10s image: 192.168.21.12:5000/tmp/kafka:2.12-2.1.0 labels: - com.tmp.kafka.role=kafka networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka3 ports: - mode: ingress protocol: tcp published: 20231 target: 9092 my_kafka_kafka-manager: depends_on: - my_kafka_zk1 - my_kafka_zk2 - my_kafka_zk3 environment: ZK_HOSTS: my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181, healthcheck: interval: 10s retries: 30 test: - CMD-SHELL - curl 127.0.0.1:9000 | grep Active timeout: 15s image: 192.168.21.12:5000/tmp/kafka-manager:1.3.1.8 labels: - com.tmp.kafka.role=kafka_manager networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka_kafka-manager ports: - mode: ingress protocol: tcp published: 20232 target: 9000 my_kafka_zk1: environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=my_kafka_zk1:2888:3888 server.2=my_kafka_zk2:2888:3888 server.3=my_kafka_zk3:2888:3888 hostname: my_kafka_zk1 image: 192.168.21.12:5000/tmp/zookeeper:3.4.13 labels: - com.tmp.kafka.role=zookeeper networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka_zk1 ports: - mode: ingress protocol: tcp published: 20226 target: 2181 my_kafka_zk2: environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=my_kafka_zk1:2888:3888 server.2=my_kafka_zk2:2888:3888 server.3=my_kafka_zk3:2888:3888 hostname: my_kafka_zk2 image: 192.168.21.12:5000/tmp/zookeeper:3.4.13 labels: - com.tmp.kafka.role=zookeeper networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka_zk2 ports: - mode: ingress protocol: tcp published: 20227 target: 2181 my_kafka_zk3: environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=my_kafka_zk1:2888:3888 server.2=my_kafka_zk2:2888:3888 server.3=my_kafka_zk3:2888:3888 hostname: my_kafka_zk3 image: 192.168.21.12:5000/tmp/zookeeper:3.4.13 labels: - com.tmp.kafka.role=zookeeper networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka_zk3 ports: - mode: ingress protocol: tcp published: 20228 target: 2181 docker stack deploy -c kafka.yaml my_kafka 二、docker swarm 网络相关 swarm 网络 "},"notes/docker/docker_compose.html":{"url":"notes/docker/docker_compose.html","title":"docker compose 相关","keywords":"","body":"docker compose 相关 一、安装 https://github.com/docker/compose/releases mkdir -p ~/.docker/cli-plugins/ curl -SL https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) \\ -o ~/.docker/cli-plugins/docker-compose chmod +x ~/.docker/cli-plugins/docker-compose docker compose version 二、docker-compose --help build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information 三 、常用命令 docker-compose up -d nginx 构建建启动nignx容器 docker-compose exec nginx bash 登录到nginx容器中 docker-compose down 删除所有nginx容器,镜像 docker-compose ps 显示所有容器 docker-compose restart nginx 重新启动nginx容器 docker-compose build nginx 构建镜像 docker-compose build --no-cache nginx 不带缓存的构建 docker-compose logs nginx 查看nginx的日志 docker-compose logs -f nginx 查看nginx的实时日志 docker-compose config -q 验证（docker-compose.yml）文件配置，当配置正确时，不输出任何内容，当文件配置错误，输出错误信息 docker-compose events --json nginx 以json的形式输出nginx的docker日志 docker-compose pause nginx 暂停nignx容器 docker-compose unpause nginx 恢复ningx容器 docker-compose rm nginx 删除容器（删除前必须关闭容器） docker-compose stop nginx 停止nignx容器 docker-compose start nginx 启动nignx容器 docker-compose run --no-deps --rm php-fpm php -v 在php-fpm中不启动关联容器，并容器执行php -v 执行完成后删除容器 "},"notes/docker/docker_registry.html":{"url":"notes/docker/docker_registry.html","title":"docker registry 相关","keywords":"","body":"docker registry 相关 一、docker registry 1、拉取registry镜像 docker pull registry 2、启动registry镜像 docker run -d --name registry -p 4000:5000 --restart=always -v /opt/registry/:/var/lib/registry/docker/registry registry:latest 3、测试，拉取busybox镜像，push到registy中 docker pull busybox docker tag busybox 127.0.0.1:4000/busybox docker push 127.0.0.1:4000/busybox 4、查看registry仓库 registry api curl http://127.0.0.1:4000/v2/_catalog curl http://127.0.0.1:4000/v2/contrail-agent-u14.04/tags/list curl http://127.0.0.1:4000/v2/kolla/centos-source-nova-api/tags/list curl http://127.0.0.1:4000/v2/flannel/manifests/latest 5、配置docker registy tee /etc/docker/daemon.json 6、打包registry镜像文件 tar -zcvf registry-20190618.tar.gz /opt/registry/ 7、再次使用 tar -zxvf registry-20190618.tar.gz -C /opt/ docker run -d --name registry -p 4000:5000 --restart=always -v /opt/registry/:/var/lib/registry/docker/registry registry:latest 二、docker harbor doker harbor 1、安装docker、docker-compose yum -y install yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum -y install docker-ce systemctl start docker systemctl enable docker curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 2、下载harbor-offline-installer-v1.5.2.tgz tar -xvf harbor-offline-installer-v1.5.2.tgz cd harbor 3、编辑配置文件 harbor.cfg hostname = 192.168.21.232 customize_crt = off harbor_admin_password = admin 4、安装 ./install.sh 5、使用 use harbor 浏览器登陆 192.168.21.232 创建项目公开项目 test 客户端配置 tee /etc/docker/daemon.json "},"notes/docker/openshift.html":{"url":"notes/docker/openshift.html","title":"openshift 相关","keywords":"","body":"openshift 相关 Master Node提供的组件：API Server (负责处理客户端请求, 包括node、user、administrator和其他的infrastructure系统)；Controller Manager Server (包括scheduler和replication controller)；OpenShift客户端工具 (oc) Compute Node(Application Node) 部署application Infra Node 运行router、image registry和其他的infrastructure服务(由管理员安装的系统服务Application) etcd 可以部署在Master Node，也可以单独部署， 用来存储共享数据：master state、image、 build、deployment metadata等 Pod 最小的Kubernetes object，可以部署一个或多个container https://blog.51cto.com/7308310/2171091 一、单节点部署 v3.11 1、安装相关软件 yum -y install epel-release git python-pip gcc python-devel centos-release-openshift-origin311 2、下载源码 https://github.com/openshift/openshift-ansible/tree/v3.11 git clone https://github.com/openshift/openshift-ansible.git cd openshift-ansible git tag git checkout v3.11 git describe 3、修改依赖ansible的版本为2.7.x vim requirements.txt ansible==2.7.12 4、安装依赖 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 5、编写hosts.localhost cat inventory/hosts.localhost #bare minimum hostfile [OSEv3:children] masters nodes etcd [OSEv3:vars] # if your target hosts are Fedora uncomment this #ansible_python_interpreter=/usr/bin/python3 openshift_deployment_type=origin openshift_portal_net=172.30.0.0/16 # localhost likely doesn't meet the minimum requirements openshift_disable_check=disk_availability,memory_availability,docker_image_availability,docker_storage openshift_node_groups=[{'name': 'node-config-all-in-one', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true']}] [masters] localhost ansible_connection=local [etcd] localhost ansible_connection=local [nodes] # openshift_node_group_name should refer to a dictionary with matching key of name in list openshift_node_groups. localhost ansible_connection=local openshift_node_group_name=\"node-config-all-in-one\" 6、做好本地对自己的免密，修改hosts文件 cat /etc/hosts 192.168.21.90 hl ssh-keygen ssh-copy-id hl 7、准备 ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml 8、检查 openshift 源 vim /etc/yum.repos.d/CentOS-OpenShift-Origin311.repo [centos-openshift-origin311] name=CentOS OpenShift Origin baseurl=http://buildlogs.centos.org/centos/7/paas/x86_64/openshift-origin311/ enabled=1 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-PaaS [centos-openshift-origin311-testing] name=CentOS OpenShift Origin Testing baseurl=http://buildlogs.centos.org/centos/7/paas/x86_64/openshift-origin311/ enabled=0 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-PaaS [centos-openshift-origin311-debuginfo] name=CentOS OpenShift Origin DebugInfo baseurl=http://debuginfo.centos.org/centos/7/paas/x86_64/ enabled=0 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-PaaS [centos-openshift-origin311-source] name=CentOS OpenShift Origin Source baseurl=http://vault.centos.org/centos/7/paas/Source/openshift-origin311/ enabled=0 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-PaaS yum makecache fast 9、提前拉取镜像（deploy_cluster.yml会拉取） imgs=(docker.io/cockpit/kubernetes:latest docker.io/openshift/origin-control-plane:v3.11 docker.io/openshift/origin-deployer:v3.11 docker.io/openshift/origin-docker-registry:v3.11 docker.io/openshift/origin-pod:v3.11 quay.io/coreos/etcd:v3.2.22 docker.io/openshift/origin-haproxy-router:v3.11 docker.io/openshift/origin-control-plane:v3.11.0 docker.io/openshift/origin-deployer:v3.11.0 docker.io/openshift/origin-haproxy-router:v3.11.0 docker.io/openshift/origin-pod:v3.11.0 docker.io/openshift/origin-docker-registry:v3.11.0) for img in ${imgs[@]};do docker pull $img;done 10、提前安装相关软件（deploy_cluster.yml会安装） yum -y install origin-node-3.11* origin-clients-3.11* origin-3.11* conntrack-tools 11、因版本原因可能会出错，prerequisites 导致 pip uninstall pyOpenSSL cryptography pip install pyOpenSSL cryptography pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 12、部署 ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml 13、卸载（部署失败是可用，从第7步重新开始） ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall_openshift.yml 14、创建管理用户和密码 htpasswd -c /etc/origin/master/htpasswd admin 15、登录 https://yani.srv.world:8443/console 二、多节点部署 配置calico 1、hosts文件 [OSEv3:children] masters nodes etcd nfs [OSEv3:vars] ansible_ssh_user=root # If ansible_ssh_user is not root, ansible_become must be set to true #ansible_become=true openshift_deployment_type=origin # openshift version openshift_image_tag=v3.11 openshift_release=v3.11 # use calico os_sdn_network_plugin_name=cni openshift_use_calico=true openshift_use_openshift_sdn=false openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}] #openshift_master_htpasswd_users={'admin': '$apr1$RbOvaj8r$LEqJqG6V/O/i7Pfyyyyyy.', 'user': '$apr1$MfsFK97I$enQjqHCh2LL8w4EBwNrrrr'} openshift_master_default_subdomain=apps.srv.world # allow unencrypted connection within cluster openshift_docker_insecure_registries=172.30.0.0/16 #openshift_disable_check=disk_availability,docker_storage,memory_availability,docker_image_availability openshift_disable_check=docker_storage,docker_image_availability [masters] master.srv.world openshift_schedulable=true containerized=false [etcd] master.srv.world [nfs] master.srv.world [nodes] # defined values for [openshift_node_group_name] in the file below # [/usr/share/ansible/openshift-ansible/roles/openshift_facts/defaults/main.yml] #master.srv.world openshift_node_group_name='node-config-master-infra' #master.srv.world openshift_node_group_name='node-config-master' #node2.srv.world openshift_node_group_name='node-config-infra' #master.srv.world openshift_node_group_name='node-config-all-in-one' master.srv.world openshift_node_group_name='node-config-master-infra' node1.srv.world openshift_node_group_name='node-config-compute' 2、安装calicoctl客户端 curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctl chmod a+x calicoctl mv calicoctl /bin calicoctl node status 3、nfs [root@ctrl ~]# cat /etc/exports /var/nfsshare *(rw,sync,no_subtree_check,no_root_squash) git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/nfs-client # https://medium.com/faun/openshift-dynamic-nfs-persistent-volume-using-nfs-client-provisioner-fcbb8c9344e # https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/README.md kubectl patch storageclass managed-nfs-storage -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 4、若要更改网络相关，还需要删除cni相关的配置文件 rm -rf /etc/cni/net.d/* 三、使用 1、查看节点信息 oc get nodes oc get nodes --show-labels=true 2、oc client tool oc whoami system:admin 在其他节点运行oc命令需要先登录 oc login https://yani.srv.world:8443 -u developer 退出 os logout 3、查看openshift资源类型 oc api-resources 4、查看pod oc get pods 5、查看pod详细信息 oc describe pods "},"notes/docker/convert_iso_to_docker_img.html":{"url":"notes/docker/convert_iso_to_docker_img.html","title":"将ISO镜像转换为docker镜像","keywords":"","body":"将ISO镜像转换为docker镜像 一、转换 1、准备 iso 例： ubuntu-16.04.6-desktop-amd64.iso 2、安装工具 squashfs-tools yum install -y squashfs-tools 3、创建两个目录 mkdir rootfs unquashfs 4、挂载 iso mount -o loop ubuntu-16.04.6-desktop-amd64.iso rootfs 5、找到 filesystem.squashfs.file 文件路径 find . -type f | grep filesystem.squashfs find . -type f | grep filesystem.squashfs ./rootfs/casper/filesystem.squashfs ./rootfs/casper/filesystem.squashfs.gpg 6、使用 unsquashfs 解压 filesystem.squashfs 到 unsquashfs 文件夹 unsquashfs -f -d unsquashfs/ rootfs/casper/filesystem.squashfs 7、压缩并导入到 docker tar -C unsquashfs -c . | docker import - ubuntu/myimg 8、查看 docker images|grep \"ubuntu/mying\" 二、安装桌面 1、clone 以下项目 https://github.com/hlyani/docker-ubuntu-desktop.git git clone https://github.com/hlyani/docker-ubuntu-desktop.git 2、修改 dockerfile 文件，将镜像改为刚才构建好的 FROM ubuntu/myimg ENV DEBIAN_FRONTEND noninteractive ENV USER root COPY sources.list /etc/apt/ RUN apt-get update && \\ apt-get install -y --no-install-recommends ubuntu-desktop && \\ apt-get install -y gnome-panel gnome-settings-daemon metacity nautilus gnome-terminal && \\ apt-get install -y tightvncserver && \\ mkdir /root/.vnc ADD xstartup /root/.vnc/xstartup ADD passwd /root/.vnc/passwd RUN chmod 600 /root/.vnc/passwd CMD /usr/bin/vncserver :1 -geometry 1280x800 -depth 24 && tail -f /root/.vnc/*:1.log EXPOSE 5901 3、测试、运行构建好的镜像 vnc://:5901 password: password docker run -dp 5901:5901 queeno/ubuntu-desktop "},"notes/docker/mount_usb_to_docker.html":{"url":"notes/docker/mount_usb_to_docker.html","title":"挂载USB到docker容器","keywords":"","body":"挂载USB到docker容器 一、基础准备 1、部署 docker 环境，拉取或构建带vnc的docker镜像 https://hub.docker.com/r/x11vnc/desktop git clone https://github.com/hlyani/docker-ubuntu-vnc-desktop.git git clone https://github.com/hlyani/x11vnc-desktop.git docker pull x11vnc/desktop docker pull centminmod/docker-ubuntu-vnc-desktop 3、查看主机 usb 设备 apt -y install usbutils lsusb -D /dev/bus/usb/001/001 lsusb -s 001:001 4、查看视频设备 ls /dev/video* 5、运行 docker 在运行docker时添加--privileged，这样可以放开 docker 的所有系统操作权限，但是这种操作带来的安全风险比较大。 docker run -itu0 --rm -p 6080:6080 --privileged=true --name test --device=/dev/video0 --device=/dev/video1 -v /dev/bus/usb:/dev/bus/usb x11vnc/desktop docker run -itu0 --name test --privileged=true --net=host -v /dev/bus/usb:/dev/bus/usb ubuntu bash docker run -itu0 --name test --privileged=true --net=host -v /dev/bus/usb:/dev/bus/usb ubuntu bash docker run -it --rm --device=/dev/video0 -e DISPLAY=unix$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix whynot0/opencamrea:v1 docker run -it --rm --runtime=nvidia -v /dev/bus/usb:/dev/bus/usb -e NVIDIA_VISIBLE_DEVICES=0 -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=unix$DISPLAY cwaffles/openpose-python docker run -t -i --device=/dev/ttyUSB0 ubuntu bash 二、挂载摄像头并显示视频 https://blog.csdn.net/weixin_40922744/article/details/103245166 https://yq.aliyun.com/articles/606756 1、安装摄像头第三方软件 apt install -y camorama apt install -y cheese 2、运行 camorama cheese apt-get install guvcview guvcview -d /dev/video "},"notes/docker/local_static_provisioner.html":{"url":"notes/docker/local_static_provisioner.html","title":"local-static-provisioner","keywords":"","body":"local static provisioner https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner 一、安装 1、拉取代码 git clone --depth=1 -b v2.7.0 https://github.com/kubernetes/kubernetes helm repo add sig-storage-local-static-provisioner https://kubernetes-sigs.github.io/sig-storage-local-static-provisioner helm template --debug sig-storage-local-static-provisioner/local-static-provisioner --version --namespace > local-volume-provisioner.generated.yaml kubectl create -f local-volume-provisioner.generated.yaml 2、安装 helm install lp ./helm/provisioner 3、配置 vim values.yaml classes: - name: local-storage hostDir: /opt/local-provisioner volumeMode: Filesystem fsType: ext4 storageClass: true image: registry.k8s.io/sig-storage/local-volume-provisioner:v2.6.0 必须mount，provisioner会实时检测hostDir目录下的mount目录，并自动创建pv mkdir -p /opt/local-provisioner/pv0 mount --bind /opt/local-provisioner/pv0 /opt/local-provisioner/pv0 二、其他 根据本地文件夹创建 pv apiVersion: v1 kind: PersistentVolume metadata: name: local-pv spec: capacity: storage: 10Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /opt/local-pv nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: ingress operator: In values: - master 创建pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi volumeMode: Filesystem storageClassName: local-storage test apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: local-storage --- kind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: nginx:1.21.3 volumeMounts: - mountPath: \"/data\" name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim "},"notes/golang/go.html":{"url":"notes/golang/go.html","title":"golang 相关","keywords":"","body":"Go 语言相关 一、并发 1、GMP模型 GMP：机制是实现协程和并发的重要机制。 GMP机制采用了M:N模型，M个goroutine映射到N个OS thread上执行。 golang中有一个协程的概念，更轻量级的调度单元。 占用栈空间小（2KB～2GB） 上下文都在用户态切换，不会涉及到内核态 协程切换速度大约1～2微秒，golang中协程切换速度为0.2微秒左右。 GMP的模型如何设计？ 全局队列：当P中的本地队列中有协程G溢出时，会被放到全局队列中。 P的本地队列：P内置的G队列，存的数量不超过256个。 一、当队列P1中的G1在运行过程中新建G2时，G2优先存放在P1的本地队列中，如果队列满了，则会把P1队列中一半的G移动到全局队列。 二、如果P的本地队列为空，那么他会先到全局队列中获取G，如果全局队列中没有G，则会尝试从其他线程绑定的P中偷取一半的G。 g 1、g是goroutine，是对协程的抽象，是一种轻量级的线程实现，可以在单个线程中同时运行多个协程，从而实现并发执行。 2、g有自己的运行栈、状态、以及执行的任务函数（用户通过go func指定） 3、g需要绑定到p才能执行，在g的视角中，p是它的cpu 4、G需要调度到M上才能运行，M是真正的执行者，调度器最多可以创建1万个线程。 5、当一个goroutine阻塞或执行完毕时，对应的m会返回给p，p将m标记为空闲状态，并从队列中取出下一个goroutine映射到该m上执行。 goroutin有三个状态： Waiting：goroutine正在等待systemcall执行完毕，或正在等待锁。 Runable：goroutine想要在M上执行指令。 Executing：goroutine正在M上执行指令当中。 p 1、p是调度器Processor，联系g与m 2、p的数量决定了g最大并行数量，可由用户通过GOMAXPROCS进行设定（超过CPU核数无意义） 3、go语言中的中阶层，负责将goroutine映射到m(OS thread)上执行，同时也负责调度和管理m(OS thread)。 4、p负责协程的调度和管理，维护了一个goroutine队列，用于存储待执行的goroutine，当一个goroutine需要执行时，p会从队列中取出一个goroutine，并将其映射到一个空闲的m上执行。 5、p的数量由启动时环境变量$GOMAXPROCS或者由runtime的方法GOMAXPROCS()决定，因为M需要和P绑定才能运行G，P的个数取决于设置的GOMAXPROCS，默认被设置为可用的CPU核数。 m 1、m是machine是go中线程的抽象 2、m不直接执行g，而是先和p绑定，由其实现代理 3、借由p的存在，m无需和g绑死，也无需记录g的状态信息，因此g在全生命周期中可实现跨m执行 4、操作系统中的线程实现，是操作系统调度和管理的最小执行单元，可以执行计算任务和系统调用等操作。 5、M的数量和P不一定匹配，可以设置很多M，M和P绑定才可运行，多余的M处于休眠状态。 6、m的数量，go程序启动时，会设置m的最大数量，默认10000，但是内核很难创建出这么多线程，因此默认情况下m的最大数量取决于内核。也可以调用runtime/debug中的SetMaxThreads函数 ，手动设置m的最大数量。 P和M都是程序运行时就被创建好了的吗？ P和M创建时间不同。 P：在确定P的最大数量n后，运行时系统会根据这个数量创建n个P。 M：内核级线程的初始化是由内核管理的，当没有足够的M来关联P并运行其中的可运行的G时会请求创建新的M。比如M在运行G1时被阻塞了，此时需要新的M去绑定P，如果没有在休眠的M则需要新建M。 G在MP模型中流动过程？ 1、调用go func()创建一个goroutine。 2、新创建的G优先保存在P的本地队列中，如果P的本地队列已经满了就会保存在全局队列中。 3、M需要在P的本地队列弹出一个可执行的G，如果P的本地队列为空，则先会去全局队列在中获取G，如果全局队列也为空则去其他P中偷取一半的G放到自己的P中。 4、G将相关参数传输给M，为M执行G做准备。 5、当M执行某一个G时，如果发生了系统调用会导致M阻塞，如果当前P队列中有一些G，runtime会将线程M和P分离，然后再获取空闲的线程或创建一个新的内核级线程来服务这个P，阻塞调用完成后G被销毁将值返回。 6、销毁G，将执行结果返回。 7、当M系统调用结束后，这个M会尝试获取一个空闲的P执行，如果获取不到P，那么这个线程M变成休眠状态，加入到空闲线程。 2、并发 并发：相继发生，同一时间间隔发生 并行：同时进行，同一时刻 3、sync 1.sync.Mutex互斥锁 保证同一时间只有一个goroutine可以获取锁，从而实现对共享资源的互斥访问。 Lock 获取锁，会阻塞直到获取锁为止 Unlock释放锁 TryLock尝试获取锁，如果锁已经被占用则不会等待直接返回 var count int var mutex sync.Mutex func increase() { mutex.Lock() defer mutex.Unlock() count++ } 2.sync.RWMutex读写互斥锁 提供更细粒度的读写访问控制。 RLock获取锁 RUnlock释放读锁 Lock获取写锁 Unlock释放写锁 var count int var rwMutex sync.RWMutex func read() { rwMutex.RLock() defer rwMutex.RUnlock() print(count) } func write() { rwMutex.Lock() defer rwMutex.Unlock() count++ } 3.sync.WaitGroup等待组 用于等待一组goroutine结束后再继续执行 Add添加一个等待单位 Done表示一个等待单位完成 Wait阻塞直到所有等待单位完成 var wg sync.WaitGroup func worker() { defer wg.Done() // do work } wg.Add(1) go worker() wg.Wait() // 等待worker完成 4.sync.Once一次性初始化 提供一次性初始化的功能，确保某个初始化逻辑只执行一次。单例模式。 var once sync.Once var config *Config func initialize() { config = loadConfig() } func GetConfig() *Config { once.Do(initialize) return config } 5.sync.Map线程安全nao 一个可以并发安全使用的map 内部使用锁机制来保证并发安全，相比传统map有更好的扩展性。 var configMap sync.Map configMap.Store(\"timeout\", 500) if _, ok := configMap.Load(\"timeout\"); ok { // 使用超时 } 6.sync.Pool对象池 实现了一个可以重用的对象池 对象池可以有效的减少对象频繁创建和销毁的性能开销 var bufferPool sync.Pool func NewBuffer() *Buffer { v := bufferPool.Get() if v == nil { return &Buffer{} } return v.(*Buffer) } bufferPool.Put(b) bufferPool.Get() 实现一个阻塞式的消息队列，支持多个接受者 type MessageQueue struct { queue []interface{} mutex sync.RWMutex } func NewMessageQueue() *MessageQueue { return &MessageQueue{queue: make([]interface{}, 0)} } func (q *MessageQueue) Enqueue(msg interface{}) { q.mutex.Lock() defer q.mutex.Unlock() ​ q.queue = append(q.queue, msg) } func (q *MessageQueue) Dequeue() interface{} { q.mutex.RLock() defer q.mutex.RUnlock() ​ // 获取首元素 ​ msg := q.queue[0] ​ q.queue = q.queue[1:] ​ return msg } 二、反射 二、GC 三、struct 四、Interface 常用包 1、Sync big os 1.os.Args 获取命令行参数 args[0]永远都是程序本身的路径,args[1:]包含所有参数。 for _, arg := range os.Args[1:] { fmt.Println(arg) } 2.os.Create 创建文件 file, err := os.Create(\"test.txt\") if err != nil { return err } defer file.Close() 3.os.Open 打开文件 file, err := os.Open(\"test.txt\") if err != nil { return err } defer file.Close() 4.os.StartProcess启动新进程 executable := \"/path/to/test\" args := []string{\"-a\", \"123\"} attr := &os.ProcAttr{ Files: []*os.File{os.Stdin, os.Stdout, os.Stderr}, } _, err := os.StartProcess(executable, args, attr) 5.os.Getenv 获取环境变量 6.os.Chdir切换当前工作目录 7.os.Mkdir 创建目录 func main() { err := os.Mkdir(\"testdir\", 0755) if err != nil { log.Fatal(err) } } 8.os.Remove 删除文件 9.os.Rename 重命名文件 10.os.Truncate 可以对文件进行截断,使得文件只保留截断之前的内容。 func main() { f, err := os.OpenFile(\"test.txt\", os.O_RDWR, 0644) if err != nil { log.Fatal(err) } defer f.Close() err = os.Truncate(\"test.txt\", 10) if err != nil { log.Fatal(err) } } 11.os.TempDir 可以获取当前系统的临时文件目录。 12.os.Getwd 可以获取当前进程的工作目录。 13.os.Hostname 可以获取主机的名称。 14.os.Environ 获取环境变量 env := os.Environ() for _, v := range env { fmt.Println(v) } 15.os.Exit 退出当前进程 16.os.IsExist 判断文件或文件夹是否存在 func main() { if _, err := os.Stat(\"/path/to/file\"); os.IsNotExist(err) { fmt.Println(\"file does not exist\") } } 17.os.Stat 获取文件信息 info, err := os.Stat(\"test.txt\") if err != nil { log.Fatal(err) } fmt.Println(info.Size()) // 文件大小 18.os.ReadFile 读取整个文件 data, err := os.ReadFile(\"test.txt\") if err != nil { log.Fatal(err) } fmt.Print(string(data)) 19.os.WriteFile 写入文件 data := []byte(\"Hello World\") err := os.WriteFile(\"data.txt\", data, 0644) if err != nil { log.Fatal(err) } time 创建和获取时间 package main import ( \"fmt\" \"time\" ) func main() { // 获取当前时间 now := time.Now() fmt.Println(\"Current time:\", now) // 创建指定时间 specificTime := time.Date(2023, time.October, 25, 22, 30, 0, 0, time.UTC) fmt.Println(\"Specific time:\", specificTime) } 时间的格式化与解析 package main import ( \"fmt\" \"time\" ) func main() { // 时间格式化 now := time.Now() formattedTime := now.Format(\"2006-01-02 15:04:05\") fmt.Println(\"Formatted time:\", formattedTime) // 时间解析 parsedTime, err := time.Parse(\"2006-01-02 15:04:05\", \"2023-10-25 22:30:12\") if err != nil { fmt.Println(\"Error:\", err) return } fmt.Println(\"Parsed time:\", parsedTime) } 时区处理与转换 package main import ( \"fmt\" \"time\" ) func main() { // 获取时区 local := time.Now() fmt.Println(\"Local time:\", local) // 转换时区 shanghaiTimeZone, err := time.LoadLocation(\"Asia/Shanghai\") if err != nil { fmt.Println(\"Error:\", err) return } ShanghaiTime := local.In(shanghaiTimeZone) fmt.Println(\"Shanghai time:\", ShanghaiTime) } 定时器与超时控制 package main import ( \"fmt\" \"time\" ) func main() { // 创建定时器，等待2秒 timer := time.NewTimer(2 * time.Second) // 等待定时器触发 时间间隔与持续时间 package main import ( \"fmt\" \"time\" ) func main() { // 计算时间间隔 start := time.Now() time.Sleep(2 * time.Second) end := time.Now() duration := end.Sub(start) fmt.Println(\"Duration:\", duration) } 时间的比较与计算 package main import ( \"fmt\" \"time\" ) func main() { // 比较时间 time1 := time.Date(2023, time.October, 25, 19, 0, 0, 0, time.UTC) time2 := time.Date(2023, time.October, 25, 22, 0, 0, 0, time.UTC) if time1.Before(time2) { fmt.Println(\"time1 is before time2.\") } // 计算时间 diff := time2.Sub(time1) fmt.Println(\"Time difference:\", diff) } Context flag log IO 一、常用 1、调试显示代码位置 log.SetFlags(log.Llongfile) log.Print(\"\") where := func() { _, file, line, _ := runtime.Caller(1) log.Printf(\"%s:%d\", file, line) } where() 2、go mod go mod download 下载依赖的module到本地cache（默认为$GOPATH/pkg/mod目录） go mod edit 编辑go.mod文件 go mod graph 打印模块依赖图 go mod init 初始化当前文件夹, 创建go.mod文件 go mod tidy 增加缺少的module，删除无用的module go mod vendor 将依赖复制到vendor下 go mod verify 校验依赖 go mod why 解释为什么需要依赖 3、清理缓存 go clean -modcache rm -rf mod.sum go mod tidy 4、go.mod replace 本地源 go.mod //go env -w GOPRIVATE=192.168.0.10 GOINSECURE=192.168.0.10 github.com/containerd/containerd => 192.168.0.10/aaa/containerd v1.0.0 git clone --single-branch --branch=v1.0.0 --depth=1 http://192.168.0.10/aaa/containerd build/aaa/containerd replace github.com/containerd/containerd => build/aaa/containerd 5、编译 go tool compile -d help go build -ldflags=\"-s -w\" -o main main.go -s 忽略符号表和调试信息 -w 忽略DWARFv3调试信息，使用该选项后将无法使用gdb进行调试 -gcflags=\"-l -N\"可以关闭代码优化，从而缩短编译时间 6、静态编译 go build -o output.exe -ldflags=\"-s -w -extldflags '-static'\" -ldflags=\"-s -w -extldflags '-static'\" 用于去除可执行文件中的符号表和调试信息，并将所有的依赖库链接为静态库 CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o output -ldflags=\"-s -w -extldflags '-static'\" 7、压缩 upx 1-9，9代表最高压缩率 go build -ldflags=\"-s -w\" -o main main.go && upx -9 main 8、go mod go mod download 下载依赖的module到本地cache（默认为$GOPATH/pkg/mod目录） go mod edit 编辑go.mod文件 go mod graph 打印模块依赖图 go mod init 初始化当前文件夹, 创建go.mod文件 go mod tidy 增加缺少的module，删除无用的module go mod vendor 将依赖复制到vendor下 go mod verify 校验依赖 go mod why 解释为什么需要依赖 9、交叉编译 GOOS=linux GOARCH=arm64 go build ... GOOS=linux GOARCH=amd64 go build ... 10、调试打印 func pp(values ...interface{}) { for _, v := range values { fmt.Println(\"-----------------------------------------------------------------------------------\") fmt.Println(v) fmt.Println(\"-----------------------------------------------------------------------------------\") } } 11、获取变量类型 type := reflect.TypeOf(num) switch val.(type) fmt.Printf(\"The type of num is: %T\\n\", num) 二、Go 语言环境安装 https://golang.org/dl/ wget -O /usr/local/go1.23.4.linux-amd64.tar.gz https://golang.org/dl/go1.23.4.linux-amd64.tar.gz tar -C /usr/local -zxvf /usr/local/go1.23.4.linux-amd64.tar.gz echo \"export PATH=$PATH:/usr/local/go/bin\" |tee >> /etc/profile source /etc/profile 三、基础语法 1、第一个程序 vim hello.go package main import \"fmt\" func main() { fmt.Println(\"Hello World!\") } go run hello.go func main() 是程序开始执行的函数。main 函数是每一个可执行程序所必须包含的，一般来说都是在启动后第一个执行的函数（如果有 init() 函数则会先执行该函数）。 2、声明 var a int = 10 var b = 10 c := 10 k1, k2, k3 = 1, 2, 3 _, b = 5, 7 var balance = [5]float32{1000.0, 2.0, 3.4, 7.0, 50.0} a := [3][4]int{ {0, 1, 2, 3}, {4, 5, 6, 7}, {8, 9, 10, 11}, } // 这种不带声明格式的只能在函数体中出现 vname1, vname2, vname3 := v1, v2, v3 // 这种因式分解关键字的写法一般用于声明全局变量 var ( vname1 v_type1 vname2 v_type2 ) 空白标识符 _ 常量的定义格式： const identifier [type] = value 3、字符串相关操作 // 拼接 fmt.Println(\"hello\" + \"world\") // %d 表示整型数字，%s 表示字符串 var stockcode=123 var enddate=\"2020-12-31\" var url=\"Code=%d & endDate=%s\" var target_url=fmt.Sprintf(url,stockcode,enddate) fmt.Println(target_url) 4、运算符 运算符 描述 ++，-- ==，!=，>=， &&，\\ \\ ，! =，+=，-=，*=，/=，%= >=，&=，^=，\\ = & &a，变量的实际地址 * *a，指针变量 A=60 B=13 A = 0011 1100 B = 0000 1101 位运算符 描述 示例 & 按位与运算符 A&B=0000 1100 \\ 按位或运算符 A\\ B=0011 1101 ^ 按位异或运算符，两对应的二进位相异时，结果为1。 A^B=0011 0001 左移运算符，左移n位就是乘以2的n次方。把 A >> 右移运算符，右移n位就是除以2的n次方。把>>左边的运算数的各二进位全部右移若干位。 A>>2=15，0000 1111 5、条件语句、循环语句 if a switch { case grade == \"A\" : fmt.Printf(\"优秀!\\n\" ) case grade == \"B\", grade == \"C\" : fmt.Printf(\"良好\\n\" ) default: fmt.Printf(\"差\\n\" ); } fallthrough ? func main() { var c1, c2, c3 chan int var i1, i2 int select { case i1 = sum := 0 for i := 0; i for sum numbers := [6]int{1, 2, 3, 5} for i,x:= range numbers { fmt.Printf(\"第 %d 位 x 的值 = %d\\n\", i,x) } var a int = 10 LOOP: for a for true { fmt.Printf(\"这是无限循环。\\n\"); } for { fmt.Printf(\"这是无限循环。\\n\"); } 6、指针 一个指针变量指向一个值的内存地址 var ip *int /*指向整型*/ var fp *float32 /*指向浮点型*/ if(ptr != nil) /* ptr 不是空指针 */ if(ptr == nil) /* ptr 是空指针 */ var a int = 20 /*声明实际变量*/ var ip *int /*声明指针参数*/ ip = &a /*指针变量赋值*/ fmt.Printf(\"a 变量的内存地址：%x\\n\", &a) // 0xc0000160b0 fmt.Printf(\"ip 变量存储的指针地址：%x\\n\", ip) // 0xc0000160b0 fmt.Printf(\"*ip 变量的值：%d\\n\", *ip) // 20 7、函数 func max(num1, num2 int) int { var result int if (num1 > num2) { result = num1 } else { result = num2 } return result } 语言函数作为实参 package main import ( \"fmt\" \"math\" ) func main(){ /* 声明函数变量 */ getSquareRoot := func(x float64) float64 { return math.Sqrt(x) } /* 使用函数 */ fmt.Println(getSquareRoot(9)) } 闭包，可以直接使用函数内的变量，不必申明。 func getSequence() func() int { i := 0 return func() int { i += 1 return i } } 8、结构体 type Books struct { title string author string subject string book_id int } func main(){ var Book1 Books Book1.title = \"test\" var struct_pointer *Books struct_pointer = &Book1 struct_pointer.title } 9、切片 Slice var slice1 []type = make([]type, length, capacity) slice1 := make([]type, len) var numbers = make([]int,3,5) if(numbers == nil){ fmt.Printf(\"切片是空的\") } fmt.Printf(\"len=%d cap=%d slice=%v\\n\",len(numbers),cap(numbers),numbers) // len()获取长度，cap()切片最长可以达多少 numbers := []int{0,1,2,3,4,5,6,7,8} printSlice(numbers) fmt.Println(\"numbers[1:4] ==\", numbers[1:4]) // 末尾插入0 numbers = append(numbers, 0) // 保存到一个临时的切片 tmp := append([]int{}, slice[2:]...) // 拷贝 numbers 的内容到 numbers1 numbers1 := make([]int, len(numbers), (cap(numbers))*2) copy(numbers1, numbers) // 删除下标为index的元素 slice = append(slice[:index], slice[index+1:]...) // 删除index1~index2之间的元素 slice = append(slice[:index1], slice[index2:]...) 10、范围 Range nums := []int{2, 3, 4} for k, v := range nums { fmt.Println(k, v) } 11、集合 Map var map_variable map[key_data_type]value_data_type 或 map_variable := make(map[key_data_type]value_data_type) countryCapitalMap := map[string]string{\"France\": \"Paris\", \"Italy\": \"Rome\"} countryCapitalMap [ \"France\" ] = \"巴黎\" countryCapitalMap [ \"Italy\" ] = \"罗马\" delete(countryCapitalMap, \"France\") 12、类型转换 var sum int = 17 var count int = 5 var mean float32 mean = float32(sum)/float32(count) var a int64 = 3 var b int32 b = int32(a) 13、错误处理 type error interface { Error() string } type DivideError struct { dividee int divider int } func (de *DivideError) Error() string { strFormat := ` Cannot proceed, the divider is zero. dividee: %d divider: 0 ` return fmt.Sprintf(strFormat, de.dividee) } func Divide(varDividee int, varDivider int) (result int, errorMsg string) { if varDivider == 0 { dData := DivideError{ dividee: varDividee, divider: varDivider, } errorMsg = dData.Error() return } else { return varDividee / varDivider, \"\" } } func main() { // 正常情况 if result, errorMsg := Divide(100, 10); errorMsg == \"\" { fmt.Println(\"100/10 = \", result) } // 当除数为零的时候会返回错误信息 if _, errorMsg := Divide(100, 0); errorMsg != \"\" { fmt.Println(\"errorMsg is: \", errorMsg) } } 14、并发 goroutine 是轻量级线程，goroutine 的调度是由 Golang 运行时进行管理的。同一个程序中的所有 goroutine 共享同一个地址空间。 package main import ( \"fmt\" \"time\" ) func say(s string) { for i := 0; i 15、通道（channel） 通道是用来传递数据的一个数据结构，通道可用于两个goroutine之间通过传递一个指定类型的值来同步运行和通讯。操作符 带缓冲区的通道允许发送端的数据发送和接收端的数据获取处于异步状态，不过缓冲区的大小是有限的。 如果通道不带缓冲区，发送方会阻塞直到接收方从通道中接受到了值。如果通道带缓冲，发送方则会阻塞直到发送的值被拷贝到缓冲区内；如果缓冲区已满，则意味着需要等待直到某个接收方获取到一个值，接收方在有值可以接受之前一直阻塞。 ch 如果通道接受不到数据后 ok 就为 false，这时通道就可以使用 close() 函数来关闭。 v, ok := 四、实例 type MinStack struct { stack []int minStack []int } func Constructor() MinStack { return MinStack{ stack: []int{}, minStack: []int{math.MaxInt64}, } } func (this *MinStack) Push(val int) { this.stack = append(this.stack, val) this.minStack = append(this.minStack, Min(val, this.minStack[len(this.minStack) - 1])) } nums []int sort.Ints(nums) math.inf math.MaxInt64 五、dlv 调试 1、安装dlv go install github.com/go-delve/delve/cmd/dlv@latest 2、编译程序 GODEBUG=1 make 3、通过dlv运行程序 /opt/go/bin/dlv exec ./bin/containerd --headless --listen 0.0.0.0:2345 --api-version 2 4、配置vscode lauch.json { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Launch Package\", \"type\": \"go\", \"request\": \"attach\", \"mode\": \"remote\", \"remotePath\": \"/opt/containerd\", \"port\": 2345, \"dlvLoadConfig\": { \"followPointers\": true, \"maxVariableRecurse\": 1, \"maxStringLen\": 2048, \"maxArrayValues\": 64, \"maxStructFields\": -1 }, \"host\": \"192.168.0.127\" } ] } 5、vscode F5 启用调试 "},"notes/openstack/whatisruntime.html":{"url":"notes/openstack/whatisruntime.html","title":"区分当前系统运行环境","keywords":"","body":"区分当前系统运行环境 demidecode是一个可以将DMI table中的内容以人类可读格式导出的工具。DMI（也被称为SMBIOS）Table中保存的是该表包含系统硬件组成的描述，以及其他有用的信息，例如序列号和BIOS版本等。 一、物理机 dmidecode -s system-product-name X10DRH 二、虚拟机 1、kvm dmidecode -s system-product-name KVM 2、OpenStack dmidecode -s system-product-name OpenStack Nova 3、VMware dmidecode -s system-product-name VMware Virtual Platform 4、VirtualBox dmidecode -s system-product-name VirtualBox 三、容器 1、docker docker 容器通常会在\"/\"目录下有个一个dockerenv文件。老版本可能是dockerinit文件。 ls /*docker* /.dockerenv 或者直接通过查看cgroup信息中是否包含docker字段。 cat /proc/1/cgroup 2、K8S 查看环境变量 env | grep KUBE 如果有根目录下有docker文件，且env有k8s环境变量说明容器运行时使用docker，否则是containerd。可通过cgroup进一步确认。 cat /proc/1/cgroup|grep docker 11:pids:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod10cb94c2_305c_4edd_99d0_09428515a8c7.slice/cri-containerd-272a744e650bf5f6e13a3f83ee21f9f8bb4d8a527d3656db057e1f21b10b0871.scope "},"notes/openstack/qemu_make.html":{"url":"notes/openstack/qemu_make.html","title":"QEMU 源码编译","keywords":"","body":"QEMU 源码编译 QEMU 是一个支持跨平台虚拟化的虚拟机，有 user mode 和 system mode 两种配置方式。其中 qemu 在 system mode 配置下模拟出整个计算机，可以在 qemu 之上运行一个操作系统。VMware 和 Virtualbox 之类通常只能在 x86 计算机上虚拟出一个 x86 虚拟机，而 QEMU 支持在 x86 上虚拟出一个 ARM 虚拟机。qemu 在 user mode 配置下，可以运行跟当前平台指令集不同的平台的可执行程序。 QEMU 中有两个重要的名词 host 和 target （guest），其中 host 表示 qemu 程序本身运行的平台，target（guest）表示 qemu 虚拟出的计算平台（system mode）或支持的可执行程序的运行平台（user mode）。 一、安装依赖 1、安装 ninja git clone git://github.com/ninja-build/ninja.git && cd ninja ./configure.py --bootstrap cp ninja /usr/bin/ ninja --version 2、安装其他依赖 dnf install -y glib2 glib2-devel gtk2-devel 二、编译 QEMU 1、拉取源码 git clome https://github.com/qemu/qemu.git 2、开始构建 cd qemu/ mkdir build cd build ../configure --target-list=aarch64-softmmu,aarch64-linux-user ../configure --target-list=aarch64-softmmu,aarch64-linux-user --enable-spice --enable-vnc --enable-guest-agent --enable-libusb --enable-usb-redir ../configure --target-list=x86_64-softmmu,x86_64-linux-user --enable-kvm --enable-spice --enable-vnc --enable-guest-agent --enable-libusb --enable-usb-redir ../configure --target-list=aarch64-softmmu,aarch64-linux-user,x86_64-softmmu,x86_64-linux-user --enable-kvm --enable-spice --enable-vnc --enable-guest-agent --enable-libusb --enable-usb-redir --target-list 指定需要编译的 target（guest） aarch64-softmmu 表示要编译 system mode 的 QEMU aarch64-linux-user 表示要编译 user mode 的 QEMU 3、编译 make -j10 4、安装 make install 5、验证 qemu-system-aarch64 --version qemu-aarch64 --version qemu-aarch64 是用户模式的模拟器（更精确的表述应该是系统调用模拟器） qemu-system-aarch64 是系统模拟器，可以模拟出整个机器并运行操作系统 qemu-aarch64 仅可用来运行二进制文件，因此你可以交叉编译完例如hello world之类的程序然后交给 qemu-aarch64 来运行，简单而高效。 而 qemu-system-aarch64 则需要把hello world程序下载到客户机操作系统能访问到的硬盘里才能运行。 三、配置 UEFI 启动 1、安装 edk2.git-aarch64 wget https://www.kraxel.org/repos/firmware.repo -O /etc/yum.repos.d/firmware.repo yum install edk2.git-aarch64 2、配置 qemu vim /etc/libvirt/qemu.conf nvram = [\"/usr/share/edk2.git/aarch64/QEMU_EFI-pflash.raw:/usr/share/edk2.git/aarch64/vars-template-pflash.raw\"] 3、重启 libvirt systemctl restart libvirtd 四、configure 可选参数 [root@centos8 build]# ../configure --help Usage: configure [options] Options: [defaults in brackets after descriptions] Standard options: --help print this message --prefix=PREFIX install in PREFIX [/usr/local] --interp-prefix=PREFIX where to find shared libraries, etc. use %M for cpu name [/usr/gnemul/qemu-%M] --target-list=LIST set target list (default: build all non-deprecated) Available targets: aarch64-softmmu alpha-softmmu arm-softmmu avr-softmmu cris-softmmu hppa-softmmu i386-softmmu m68k-softmmu microblazeel-softmmu microblaze-softmmu mips64el-softmmu mips64-softmmu mipsel-softmmu mips-softmmu moxie-softmmu nios2-softmmu or1k-softmmu ppc64-softmmu ppc-softmmu riscv32-softmmu riscv64-softmmu rx-softmmu s390x-softmmu sh4eb-softmmu sh4-softmmu sparc64-softmmu sparc-softmmu tricore-softmmu x86_64-softmmu xtensaeb-softmmu xtensa-softmmu aarch64_be-linux-user aarch64-linux-user alpha-linux-user armeb-linux-user arm-linux-user cris-linux-user hppa-linux-user i386-linux-user m68k-linux-user microblazeel-linux-user microblaze-linux-user mips64el-linux-user mips64-linux-user mipsel-linux-user mips-linux-user mipsn32el-linux-user mipsn32-linux-user nios2-linux-user or1k-linux-user ppc64le-linux-user ppc64-linux-user ppc-linux-user riscv32-linux-user riscv64-linux-user s390x-linux-user sh4eb-linux-user sh4-linux-user sparc32plus-linux-user sparc64-linux-user sparc-linux-user x86_64-linux-user xtensaeb-linux-user xtensa-linux-user Deprecated targets: ppc64abi32-linux-user,tilegx-linux-user,lm32-softmmu, unicore32-softmmu --target-list-exclude=LIST exclude a set of targets from the default target-list Advanced options (experts only): --cross-prefix=PREFIX use PREFIX for compile tools [] --cc=CC use C compiler CC [cc] --iasl=IASL use ACPI compiler IASL [iasl] --host-cc=CC use C compiler CC [cc] for code run at build time --cxx=CXX use C++ compiler CXX [c++] --objcc=OBJCC use Objective-C compiler OBJCC [cc] --extra-cflags=CFLAGS append extra C compiler flags QEMU_CFLAGS --extra-cxxflags=CXXFLAGS append extra C++ compiler flags QEMU_CXXFLAGS --extra-ldflags=LDFLAGS append extra linker flags LDFLAGS --cross-cc-ARCH=CC use compiler when building ARCH guest test cases --cross-cc-flags-ARCH= use compiler flags when building ARCH guest tests --make=MAKE use specified make [make] --python=PYTHON use specified python [/usr/bin/python3] --sphinx-build=SPHINX use specified sphinx-build [] --meson=MESON use specified meson [] --ninja=NINJA use specified ninja [] --smbd=SMBD use specified smbd [/usr/sbin/smbd] --with-git=GIT use specified git [git] --static enable static build [no] --mandir=PATH install man pages in PATH --datadir=PATH install firmware in PATH/qemu --localedir=PATH install translation in PATH/qemu --docdir=PATH install documentation in PATH/qemu --bindir=PATH install binaries in PATH --libdir=PATH install libraries in PATH --libexecdir=PATH install helper binaries in PATH --sysconfdir=PATH install config in PATH/qemu --localstatedir=PATH install local state in PATH (set at runtime on win32) --firmwarepath=PATH search PATH for firmware files --efi-aarch64=PATH PATH of efi file to use for aarch64 VMs. --with-suffix=SUFFIX suffix for QEMU data inside datadir/libdir/sysconfdir/docdir [qemu] --with-pkgversion=VERS use specified string as sub-version of the package --enable-debug enable common debug build options --enable-sanitizers enable default sanitizers --enable-tsan enable thread sanitizer --disable-strip disable stripping binaries --disable-werror disable compilation abort on warning --disable-stack-protector disable compiler-provided stack protection --audio-drv-list=LIST set audio drivers list: Available drivers: oss alsa sdl pa --block-drv-whitelist=L Same as --block-drv-rw-whitelist=L --block-drv-rw-whitelist=L set block driver read-write whitelist (affects only QEMU, not qemu-img) --block-drv-ro-whitelist=L set block driver read-only whitelist (affects only QEMU, not qemu-img) --enable-trace-backends=B Set trace backend Available backends: dtrace ftrace log simple syslog ust --with-trace-file=NAME Full PATH,NAME of file to store traces Default:trace- --disable-slirp disable SLIRP userspace network connectivity --enable-tcg-interpreter enable TCG with bytecode interpreter (TCI) --enable-malloc-trim enable libc malloc_trim() for memory optimization --oss-lib path to OSS library --cpu=CPU Build for host CPU [x86_64] --with-coroutine=BACKEND coroutine backend. Supported options: ucontext, sigaltstack, windows --enable-gcov enable test coverage analysis with gcov --disable-blobs disable installing provided firmware blobs --with-vss-sdk=SDK-path enable Windows VSS support in QEMU Guest Agent --with-win-sdk=SDK-path path to Windows Platform SDK (to build VSS .tlb) --tls-priority default TLS protocol/cipher priority string --enable-gprof QEMU profiling with gprof --enable-profiler profiler support --enable-debug-stack-usage track the maximum stack usage of stacks created by qemu_alloc_stack --enable-plugins enable plugins via shared library loading --disable-containers don't use containers for cross-building --gdb=GDB-path gdb to use for gdbstub tests [/usr/bin/gdb] Optional features, enabled with --enable-FEATURE and disabled with --disable-FEATURE, default is enabled if available: system all system emulation targets user supported user emulation targets linux-user all linux usermode emulation targets bsd-user all BSD usermode emulation targets docs build documentation guest-agent build the QEMU Guest Agent guest-agent-msi build guest agent Windows MSI installation package pie Position Independent Executables modules modules support (non-Windows) module-upgrades try to load modules from alternate paths for upgrades debug-tcg TCG debugging (default is disabled) debug-info debugging information sparse sparse checker safe-stack SafeStack Stack Smash Protection. Depends on clang/llvm >= 3.7 and requires coroutine backend ucontext. gnutls GNUTLS cryptography support nettle nettle cryptography support gcrypt libgcrypt cryptography support auth-pam PAM access control sdl SDL UI sdl-image SDL Image support for icons gtk gtk UI vte vte support for the gtk UI curses curses UI iconv font glyph conversion support vnc VNC UI support vnc-sasl SASL encryption for VNC server vnc-jpeg JPEG lossy compression for VNC server vnc-png PNG compression for VNC server cocoa Cocoa UI (Mac OS X only) virtfs VirtFS virtiofsd build virtiofs daemon (virtiofsd) libudev Use libudev to enumerate host devices mpath Multipath persistent reservation passthrough xen xen backend driver support xen-pci-passthrough PCI passthrough support for Xen brlapi BrlAPI (Braile) curl curl connectivity membarrier membarrier system call (for Linux 4.14+ or Windows) fdt fdt device tree kvm KVM acceleration support hax HAX acceleration support hvf Hypervisor.framework acceleration support whpx Windows Hypervisor Platform acceleration support rdma Enable RDMA-based migration pvrdma Enable PVRDMA support vde support for vde network netmap support for netmap network linux-aio Linux AIO support linux-io-uring Linux io_uring support cap-ng libcap-ng support attr attr and xattr support vhost-net vhost-net kernel acceleration support vhost-vsock virtio sockets device support vhost-scsi vhost-scsi kernel target support vhost-crypto vhost-user-crypto backend support vhost-kernel vhost kernel backend support vhost-user vhost-user backend support vhost-user-blk-server vhost-user-blk server support vhost-vdpa vhost-vdpa kernel backend support spice spice rbd rados block device (rbd) libiscsi iscsi support libnfs nfs support smartcard smartcard support (libcacard) u2f U2F support (u2f-emu) libusb libusb (for usb passthrough) live-block-migration Block migration in the main migration stream usb-redir usb network redirection support lzo support of lzo compression library snappy support of snappy compression library bzip2 support of bzip2 compression library (for reading bzip2-compressed dmg images) lzfse support of lzfse compression library (for reading lzfse-compressed dmg images) zstd support for zstd compression library (for migration compression and qcow2 cluster compression) seccomp seccomp support coroutine-pool coroutine freelist (better performance) glusterfs GlusterFS backend tpm TPM support libssh ssh block device support numa libnuma support libxml2 for Parallels image format tcmalloc tcmalloc support jemalloc jemalloc support avx2 AVX2 optimization support avx512f AVX512F optimization support replication replication support opengl opengl support virglrenderer virgl rendering support xfsctl xfsctl support qom-cast-debug cast debugging support tools build qemu-io, qemu-nbd and qemu-img tools bochs bochs image format support cloop cloop image format support dmg dmg image format support qcow1 qcow v1 image format support vdi vdi image format support vvfat vvfat image format support qed qed image format support parallels parallels image format support sheepdog sheepdog block driver support (deprecated) crypto-afalg Linux AF_ALG crypto backend driver capstone capstone disassembler support debug-mutex mutex debugging support libpmem libpmem support xkbcommon xkbcommon support rng-none dummy RNG, avoid using /dev/(u)random and getrandom() libdaxctl libdaxctl support NOTE: The object files are built at the place where configure is launched "},"notes/openstack/openstack.html":{"url":"notes/openstack/openstack.html","title":"OpenStack 相关","keywords":"","body":"OpenStack 相关 一、查看虚拟机console地址 openstack console url show novatest 二、虚拟机开机启动 resume_guests_state_on_host_boot=true 三、将虚拟机状态改为active nova reset-state --active fcfcb78b-4a58-49da-af39-d6c08a109db7 四、扩展传统模式根磁盘 qemu-img info aa.qcow2 qemu-img resize aa.qcow2 +10G lsblk resize2fs /dev/vda1 yum install xfsprogs xfs_growfs -d /mnt lsblk df -h 五、使用virsh挂载卷 pvcreate /dev/xvdb4 使用pvcreate转换 pvdisplay 查看已经存在的pv vgcreate myVG /dev/xvdb4 创建VG，可利用已经存在的VG名（myVG），同一VG名下的一组PV构成一个VG vgdisplay 查看VG 创建完成VG之后，才能从VG中划分一个LV lvcreate -l 100%FREE -n myLV myVG mkfs -t ext4 /dev/vg/instances mkfs -t ext4 /dev/vg/edu blkid /dev/myVG/myLV lvcreate -L 500G -n edu vg virsh attach-disk --domain instance-00000110 --source /dev/mysdb/edu --target vdb --persistent 六、resize、迁移功能 修改nova配置 nova.conf中，将allow_resize_to_same_host=True注释掉 一、 $ ssh-keygen -f id_rsa -b 1024 -P \"\" $ cp -a /var/lib/nova/.ssh/id_rsa.pub /var/lib/nova/.ssh/authorized_keys $ scp /var/lib/nova/.ssh/id_rsa root@otherHost:/var/lib/nova/.ssh/id_rsa $ scp /var/lib/nova/.ssh/authorized_keys root@otherHost:/var/lib/nova/.ssh/authorized_keys # chown nova:nova /var/lib/nova/.ssh/id_rsa /var/lib/nova/.ssh/authorized_keys 二、 vim /etc/libvirt/libvirtd.conf listen_tls = 0 listen_tcp = 1 tcp_port = \"16509\" listen_addr = \"0.0.0.0\" unix_sock_group = \"root\" auth_tcp = \"none\" vim /etc/init/libvirt-bin.conf env libvirtd_opts=\"-d -l\" vim /etc/init/nova-compute.conf exec start-stop-daemon --start --chuid root --exec /usr/bin/nova-compute -- --config-file=/etc/nova/nova.conf --config-file=/etc/nova/nova-compute.conf service libvirt-bin restart virsh -c qemu+tcp://compute2/system 七、修改linux密码 cloud-config userdata = ''' chpasswd: list: | root:{password} expire: False ''' nova boot --user-data=userdata #/bin/sh passwd 八、修改cpu quota openstack quota list --compute openstack quota set --cores 200 c2978d74024447b7bf0cacbd4b4e9af6 #openstack quota set --cores -1 c2978d74024447b7bf0cacbd4b4e9af6 九、恢复虚拟机ERROR状态 可能是因为该虚拟机所属的宿主机down机了，但上面虚拟机一直处于硬重启状态，所以这台虚拟机肯定是无法迁移或疏散出去的，因此疏散主机就会报下面错误咯。 1、找出该VM的id。 2、查找数据库中该id状态 use nova; select * from instances where uuid='xxxxx'\\G; update instances set vm_state='active' where uuid='xxxxx'; update instances set power_state=1 where uuid='xxxxx'; openstack 虚拟机错误状态恢复 # 查看虚拟机列表 nova list # 重置错误状态 nova reset-state --active uuid # 软重启 nova reboot uuid # 硬重启 nova reboot --hard uuid # 关机 nova stop uuid # 开机 nova start uuid 3、硬重启该VM即可。 "},"notes/openstack/openstack_faq.html":{"url":"notes/openstack/openstack_faq.html","title":"OpenStack 中遇到的问题","keywords":"","body":"openstack中遇到的问题 [TOC] 一、配置ceilometer采集间隔 ceilometer相关配置 1、修改ceilometer采集数据时间间隔。 sed -i '/^-\\ interval: /c -\\ interval:\\ 60' /etc/kolla/ceilometer-compute/polling.yaml docker restart ceilometer_compute 2、使用配置在重新部署kolla时依然生效。 cp /etc/kolla/ceilometer-compute/polling.yaml /etc/kolla/config/ gnocchi相关配置 将基于low策略的metric更改基于medium： 1，删除原有 policy-rule 。 gnocchi archive-policy-rule delete default 2，创建新的 policy-rule 。 gnocchi archive-policy-rule create -a medium -m \"*\" default -a 指明。 3，删除所有metric，gnocchi会重新自动的建立基于新policy的metric 。 for i in `gnocchi metric list -c id -f value`; do gnocchi metric delete $i; done gnocchi相关配置 gnocchi archive-policy create -d granularity:0:03:00,points:3360 granularity为时间频率 points为 保存周期/时间频率 二、ceph bug，虚拟机异常关机之后启动失败 由于 ceph版本过新，虚拟机异常关机之后导致启动失败，需要手动配置一下 caps，bug地址：https://bugs.launchpad.net/kolla-ansible/+bug/1760065 。 1、配置完成需要重启 osd, 首先将 osd 标记为 unout。 # docker exec ceph_mon ceph osd set noout noout is set 2、更新 client.nova 的 caps，首先获取目前的 caps, 记住 caps osd 的值。 # docker exec ceph_mon ceph auth get client.nova [client.nova] key = AQBu0BdbeUD1DBAAgBiKOqKCK71j2T+gC0xQRw== caps mon = \"allow r\" caps osd = \"allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=volumes-cache, allow rwx pool=vms, allow rwx pool=vms-cache, allow rwx pool=images, allow rwx pool=images-cache\" 然后更新 mon 的caps，添加 allow command \"osd blacklist\" 权限, 注意osd 的cap不变，使用上面获取的。 # docker exec ceph_mon ceph auth caps client.nova mon 'allow r, allow command \"osd blacklist\"' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=volumes-cache, allow rwx pool=vms, allow rwx pool=vms-cache, allow rwx pool=images, allow rwx pool=images-cache' updated caps for client.nova 3、更新 client.cinder 的 caps。 首先获取目前的 cinder 的caps, 记住 caps osd 的值 # docker exec ceph_mon ceph auth get client.cinder [client.cinder] key = AQC0zxdbAxupKRAAaMi5VM/pkJXU99IIIQjhCA== caps mon = \"allow r\" caps osd = \"allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=volumes-cache, allow rwx pool=vms, allow rwx pool=vms-cache, allow rx pool=images, allow rx pool=images-cache\" 然后更新 cinder的 mon 的caps，添加 allow command “osd blacklist” 权限, 注意osd 的cap不变，使用上面获取的cap。 # docker exec ceph_mon ceph auth caps client.cinder mon 'allow r, allow command \"osd blacklist\"' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=volumes-cache, allow rwx pool=vms, allow rwx pool=vms-cache, allow rx pool=images, allow rx pool=images-cache' updated caps for client.cinder 4、重启所有 ceph_mon, ceph_osd, ceph_mgr 5、配置完成取消 osd 的 unout 标记 # docker exec ceph_mon ceph osd unset noout noout is set 6、如要重新部署或者更新组件需参照上面步骤，将“osd blacklist”权限移除。 三、neutron ovs 导致CPU占用过高的问题 查看进程,openvswith agent CPU利用率,一直100% bug:https://bugs.launchpad.net/neutron/+bug/1750777 commit： \"Do not start conntrack worker thread from init\" https://opendev.org/openstack/neutron/commit/4c8b97eca32c9c2beadf95fef14ed5b7d8981c5a \"Remove race and simplify conntrack state management\" https://opendev.org/openstack/neutron/commit/cca870abbc4f8a784c9cbed817ff307ab3094daa 四、loadbalancer状态始终处于pending_update 修改数据库 update lbaas_loadbalancers set provisioning_status='ACTIVE' where id='c3b48167-9924-493e-a6c5-c4df5a8fd643'; 五、虚拟机在线迁移失败 error : \"Live Migration failure: 'ascii' codec can't encode characters in position 251-252“ bug：https://bugs.launchpad.net/nova/+bug/1768807 commit:Handle unicode characters in migration params 原因如下： if six.PY2: - params = {key: str(value) if isinstance(value, unicode) - else value + params = {key: encodeutils.to_utf8(value) + if isinstance(value, six.text_type) else value for key, value in params.items()} self._domain.migrateToURI3( 六、ceph间隙失败导致\"ImageNotFound\" error:'ceph job fails intermittently with \"ImageNotFound: [errno 2] error opening image volume-ac0586ce-26c6-4e04-a4d6-78a941a93ad7 at snapshot None\"' bug: https://bugs.launchpad.net/cinder/+bug/1765845 commit:Handle ImageNotFound exception in _get_usage_info correctly https://opendev.org/openstack/cinder/commit/3cc8cbdee7c98e1fcf41bb27d81e34fef3a2d032 七、ceilometer内存监控不准确 commit：inspector: memory: use usable of memoryStats if available https://github.com/openstack/ceilometer/commit/2dee485da7a6f2cdf96525fabc18a8c27c8be570 八、镜像验证问题 问题描述：通过volume创建镜像，镜像的元数据中会多一个signature_verified=False, 并且使用该镜像创建虚拟机会失败。 Support Image Signature Verification cinder.conf 解决方式，1、删除镜像元数据中的signature_verified。2、修改cinder的配置文件 verify_glance_signatures 默认是enabled。 verify_glance_signatures=disabled 九、cinder 配置 cinder.conf [DEFAULT] enable_force_upload = true verify_glance_signatures = disabled enable_force_upload = true 表示卷上传为镜像时，允许在 in-use 状态上传。 verify_glance_signatures = disabled 表示在创建虚拟机时选择创建新卷的方式，不检查镜像是否签名正确。 如果不增加该配置，从卷创建的镜像无法直接启动虚拟机，并且后端报错为无法创建硬盘（尝试多次均失败，无关键问题信息）。 十、nova 配置 nova.conf [DEFAULT] resize_confirm_window = 1 allow_resize_to_same_host = true resize_confirm_window = 1 表示1s后自动确认修改。 在 resize 虚拟机时，需要再次确认修改。增加该配置，无需手动确认。 allow_resize_to_same_host = true 允许 resize 虚拟机时，调度到同一台主机上。 resize_confirm_window = 0 自动确认时间 0是禁用 1代表1s后自动确认 十一、keystone 配置 policy.yaml # 提权，让普通用户也可以查询角色信息。api 在登录时会向 openstack 查询权限信息，并返回给前端使用。 # 如果登录时不反回角色信息，前端无法渲染合适的页面给多种角色。 \"identity:list_role_assignments\": \"(role:reader and system_scope:all) or (role:reader and domain_id:%(target.domain_id)s) or (role:__member__) or (role:member)\" 用户、角色、项目，这三者共同协作实现了权限功能。 一个用户在不同的项目下可以具有不同的角色，不同的角色对应着不同的行为。 认证方式： project_id + 无作用域 token。出现此类认证方式是为了减少反复生成token，一份token可以无缝在多个项目下使用。 有作用域的 token。 不建议采用此类方式 用户名密码。不建议采用此类方式 角色介绍 openstack 内置了多个可用角色，目前常用角色为 admin、member。 admin 角色： 在该项目下，用户具有超级管理员权限，可以管理所有项目的资源（不包含特殊资源，比如 swift、keypair）。 member 角色： 在该项目下，用户具有项目管理员权限，可以管理该项目下的所有资源（不包含特殊资源，比如 keypair）。 当前项目需求，增加一个新的角色，该角色权限低于 member。本意为只允许使用项目基本资源，而不能管理项目基本资源。 junior_member 角色： 在该项目下，用户仅能使用基本资源，无法管理基本资源。 角色是实际权限行为由 policy 中的规则控制。每个组件对应一个 policy，分别控制组件自身的权限。 创建一个新的角色，通常意味着需要改写 policy 文件来生成对应的实际控制权限。 policy 文件编写注意事项 新创建的角色实际权限等同于member。 通过网络组件进行测试，内置角色之间似乎存在继承的关系，因此改变一个内置角色的权限，可能会影响其它的内置角色。（未深究） 在使用 kolla-ansible 进行部署时，注意不同版本支持的编写方式不同。当前 T 版本的 kolla-ansible 不支持 neutron policy.yaml，不支持 nova 的 policy 配置。 junior_member 实际权限参考 其它权限参考member。 额外权限限制： 网络 禁止创建网络 禁止修改网络 禁止删除网络 禁止创建子网 禁止修改子网 禁止删除子网 路由 禁止创建路由 禁止修改路由 禁止删除路由 禁止更新外网 禁止添加，删除子网 浮动ip 禁止创建浮动ip 禁止删除浮动ip "},"notes/openstack/delete_cinder_volume.html":{"url":"notes/openstack/delete_cinder_volume.html","title":"删除 Cinder 僵尸实例","keywords":"","body":"删除cinder僵尸实例 1、使环境变量生效 source admin-openrc 2、查看所有的磁盘 cinder list --all 3、将磁盘的状态改为 available cinder reset-state --state available 4、将磁盘状态改为 detached cidner reset-state --attach-status detached 5、删除磁盘 cinder delete "},"notes/openstack/add_nic_to_vm.html":{"url":"notes/openstack/add_nic_to_vm.html","title":"给虚拟机添加网卡","keywords":"","body":"给虚拟机添加网卡 1、查看虚拟机 openstack server list +--------------------------------------+---------------------------+---------+--------------------------------+------------------------------------+-------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+---------------------------+---------+--------------------------------+------------------------------------+-------------+ | dc15eae5-3974-4e57-bdcd-6d6ebd7451d5 | qwehl | ACTIVE | test=20.1.1.3, 20.1.1.19 | | m1.medium | +--------------------------------------+---------------------------+---------+--------------------------------+------------------------------------+-------------+ 2、查看虚拟机详情（后面为虚拟机id） openstack server show dc15eae5-3974-4e57-bdcd-6d6ebd7451d5 +-----------------------------+----------------------------------------------------------+ | Field | Value | +-----------------------------+----------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-STS:power_state | Running | | OS-EXT-STS:task_state | None | | OS-EXT-STS:vm_state | active | | OS-SRV-USG:launched_at | 2019-03-01T04:29:23.000000 | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | test=20.1.1.3, 20.1.1.19 | | config_drive | | | created | 2019-03-01T04:29:10Z | | flavor | m1.medium (3) | | hostId | f3eaa989744784defe74e480a76a8af1a6e0960a78f1e98ff8e6e463 | | id | dc15eae5-3974-4e57-bdcd-6d6ebd7451d5 | | image | | | key_name | None | | name | qwehl | | progress | 0 | | project_id | 97eaa64704bd4f549f3abf99a6decdc1 | | properties | image='443cceba-8ac0-4336-9818-bc524bb49c74' | | security_groups | name='default' | | | name='default' | | status | ACTIVE | | updated | 2019-03-01T04:29:23Z | | user_id | 8cf334fc34814d62acdaef2a7a862654 | | volumes_attached | id='43a49283-f333-41fb-928f-4ce91762dc58' | +-----------------------------+----------------------------------------------------------+ 3、查看网络详情 openstack network show test +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | nova | | created_at | 2019-02-18T01:59:37Z | | description | | | dns_domain | None | | id | 2e6ff4d6-b5ee-4390-892c-568ab5d71107 | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | None | | is_vlan_transparent | None | | location | None | | mtu | 1450 | | name | test | | port_security_enabled | True | | project_id | 97eaa64704bd4f549f3abf99a6decdc1 | | provider:network_type | None | | provider:physical_network | None | | provider:segmentation_id | None | | qos_policy_id | None | | revision_number | 4 | | router:external | Internal | | segments | None | | shared | False | | status | ACTIVE | | subnets | 70ee3596-2aa9-4367-aaa8-d95d183ad802 | | tags | | | updated_at | 2019-02-18T01:59:38Z | +---------------------------+--------------------------------------+ 4、创建端口 openstack port create --network 2e6ff4d6-b5ee-4390-892c-568ab5d71107 port 5、查看端口 openstack port list 6、查看虚拟机端口 nova interface-list dc15eae5-3974-4e57-bdcd-6d6ebd7451d5 +------------+--------------------------------------+--------------------------------------+--------------+-------------------+ | Port State | Port ID | Net ID | IP addresses | MAC Addr | +------------+--------------------------------------+--------------------------------------+--------------+-------------------+ | ACTIVE | 47197121-e07a-4b81-9dda-7eac2918ab93 | 2e6ff4d6-b5ee-4390-892c-568ab5d71107 | 20.1.1.3 | fa:16:3e:53:e7:cd | +------------+--------------------------------------+--------------------------------------+--------------+-------------------+ 7、挂载端口 nova interface-attach --port-id 57258e65-9e27-42a8-8039-10bf7f6624ba dc15eae5-3974-4e57-bdcd-6d6ebd7451d5 8、再次查看虚拟机端口 nova interface-list dc15eae5-3974-4e57-bdcd-6d6ebd7451d5 +------------+--------------------------------------+--------------------------------------+--------------+-------------------+ | Port State | Port ID | Net ID | IP addresses | MAC Addr | +------------+--------------------------------------+--------------------------------------+--------------+-------------------+ | ACTIVE | 47197121-e07a-4b81-9dda-7eac2918ab93 | 2e6ff4d6-b5ee-4390-892c-568ab5d71107 | 20.1.1.3 | fa:16:3e:53:e7:cd | | ACTIVE | 57258e65-9e27-42a8-8039-10bf7f6624ba | 2e6ff4d6-b5ee-4390-892c-568ab5d71107 | 20.1.1.19 | fa:16:3e:9c:c5:42 | +------------+--------------------------------------+--------------------------------------+--------------+-------------------+ 9、重启虚拟机 openstack server reboot dc15eae5-3974-4e57-bdcd-6d6ebd7451d5 "},"notes/openstack/make_openstack_image.html":{"url":"notes/openstack/make_openstack_image.html","title":"制作 OpenStack 镜像","keywords":"","body":"制作openstack镜像 一、制作windows镜像 1、准备软件 virt-manager virtio-win-0.1.164.iso virtio spice-guest-tools-latest.exe spice-guest-tools CloudbaseInitSetup_0_9_11_x64.msi cloudbase 2、封装系统sysprep C:\\Windows\\System32\\Sysprep 3、# 压缩镜像的稀疏文件。 bsdtar -zcvf win10.qcow2.tar.gz win10.qcow2 4、# 解压镜像。 tar -xvSf win10.qcow2.tar.gz 5、压缩镜像 qemu-img convert -c -O qcow2 source.qcow2 shrunk.qcow2 virt-sparsify /path/to/source.qcow2 --compress /path/to/output.qcow2 6、转换镜像格式为raw。 qemu-img convert -O raw win10.qcow2 win10.raw 7、上传镜像到glance。 glance image-create --progress --disk-format raw --container-format bare --name win10 --property hw_video_model=vga --property os_type=windows --file win10.raw openstack image create --container-format bare --disk-format raw --public --protected --file win10.raw win10 二、制作linux镜像 1、安装 python 虚拟环境并进入。 apt install python3-virtualenv squashfs-tools libguestfs-tools python3 -m venv venv3 source venv3/bin/activate 2、linux 镜像通过 diskimage-builder 制作。 diskimage-builder pip install diskimage-builder 3、制作 ubuntu 镜像 build-ubuntu.sh #!/usr/bin/env bash set -eux echo \"deb http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ DIB_RELEASE-backports main restricted universe multiverse\" > sources.list.template # deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu DIB_RELEASE-security multiverse export DIB_RELEASE=focal # https://cloud-images.ubuntu.com/ export DIB_CLOUD_IMAGES=\"https://mirrors.ustc.edu.cn/ubuntu-cloud-images/${DIB_RELEASE}/current\" export DIB_DISTRIBUTION_MIRROR=\"https://mirrors.tuna.tsinghua.edu.cn/ubuntu\" cp sources.list.template sources.list sed -i \"s/DIB_RELEASE/${DIB_RELEASE}/g\" sources.list export DIB_APT_SOURCES=\"$(pwd)/sources.list\" export DIB_CLOUD_INIT_DATASOURCES=\"ConfigDrive, OpenStack\" # cloud-init datasource is ConfigDrive, OpenStack #export DIB_CLOUD_INIT_ALLOW_SSH_PWAUTH=\"yes\" export DIB_DEV_USER_USERNAME=\"ubuntu\" export DIB_DEV_USER_PASSWORD=\"qwe\" export DIB_DEV_USER_PWDLESS_SUDO=\"yes\" export DIB_DEV_USER_SHELL=\"/bin/bash\" rm -f /tmp/image.log DATE=$(date +%Y%m%d) disk-image-create -x ubuntu vm apt-sources cloud-init-datasources cloud-init selinux-permissive devuser -o ubuntu-server-${DIB_RELEASE}-x86_64-${DATE}.raw -t raw --checksum -x --logfile /tmp/image.log virt-sysprep --root-password password:qwe -a ubuntu-server-${DIB_RELEASE}-x86_64-${DATE}.raw # openstack image create --disk-format qcow2 --container-format bare --public --property os_type=linux --file cirros-0.5.2-x86_64-disk.img cirros # glance image-create --visibility public --property os_type=linux--file build-ubuntu-server-7.5-x86_64-20180920.raw --container-format bare --disk-format raw --name ubuntu-server-7.5-x8 6_64-20180920 --progress bash build-ubuntu.sh 4、制作centos镜像。 提前下载好官方镜像到当前目录 如：CentOS-7-x86_64-GenericCloud-1808.qcow2 centos-7-1808 wget https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-1808.qcow2.xz xz -d CentOS-7-x86_64-GenericCloud-1808.qcow2.xz build-centos.sh #!/usr/bin/env bash set -eux #export ELEMENTS_PATH=tripleo-image-elements/elements:heat-agents:appcenter-config/elements #export DIB_RELEASE=xenial export DIB_LOCAL_IMAGE=\"CentOS-7-x86_64-GenericCloud-1808.qcow2\" export DIB_CLOUD_INIT_DATASOURCES=\"OpenStack\" # FIXME: we have not use config drive? export DIB_DISTRIBUTION_MIRROR=http://mirrors.163.com/centos #export DIB_CLOUD_INIT_ALLOW_SSH_PWAUTH=\"yes\" #export DIB_DEV_USER_USERNAME=\"centos\" #export DIB_DEV_USER_PASSWORD=\"123456\" # FIXME: weak password #export DIB_DEV_USER_PWDLESS_SUDO=\"yes\" #export DIB_DEV_USER_SHELL=\"/bin/bash\" rm -f /tmp/image.log disk-image-create -x centos7 vm cloud-init-datasources cloud-init selinux-permissive -o centos-server-7.5-x86_64-$(date +%Y%m%d).raw -t raw --checksum -x --logfile /tmp/image.log #glance image-create --visibility public --property os_type=linux--file build-centos-server-7.5-x86_64-20180920.raw --container-format bare --disk-format raw --name centos-server-7.5-x86_64-20180920 --progress bash build-centos.sh 三、使用qemu-img制作镜像 1）、制作centos 1、创建磁盘等 # qemu-img create -f qcow2 /tmp/centos.qcow2 10G # virt-install --virt-type kvm --name centos --ram 1024 \\ --disk /tmp/centos.qcow2,format=qcow2 \\ --network network=default \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --os-type=linux --os-variant=centos7.0 \\ --location=/data/isos/CentOS-7-x86_64-NetInstall-1611.iso 2、安装ACPI、让hypervisor可以重启和关闭虚拟机 # yum install -y acpid # systemctl enable acpid 3、安装cloud-init yum install -y cloud-init # The top level settings are used as module # and system configuration. # A set of users which may be applied and/or used by various modules # when a 'default' entry is found it will reference the 'default_user' # from the distro configuration specified below users: - default # If this is set, 'root' will not be able to ssh in and they # will get a message to login instead as the above $user (ubuntu) disable_root: true # This will cause the set+update hostname module to not operate (if true) preserve_hostname: false # Example datasource config # datasource: # Ec2: # metadata_urls: [ 'blah.com' ] # timeout: 5 # (defaults to 50 seconds) # max_wait: 10 # (defaults to 120 seconds) # The modules that run in the 'init' stage cloud_init_modules: - migrator - seed_random - bootcmd - write-files - growpart - resizefs - set_hostname - update_hostname - update_etc_hosts - ca-certs - rsyslog - users-groups - ssh # The modules that run in the 'config' stage cloud_config_modules: # Emit the cloud config ready event # this can be used by upstart jobs for 'start on cloud-config'. - emit_upstart - disk_setup - mounts - ssh-import-id - locale - set-passwords - grub-dpkg - apt-pipelining - apt-configure - package-update-upgrade-install - landscape - timezone - salt-minion - mcollective - disable-ec2-metadata - runcmd - byobu # The modules that run in the 'final' stage cloud_final_modules: - rightscale_userdata - scripts-per-once - scripts-vendor - scripts-per-once - scripts-per-boot - scripts-per-instance - scripts-user - phone-home - final-message - power-state-change # System and/or distro specific settings # (not accessible to handlers/transforms) system_info: # This will affect which distro class gets used distro: ubuntu # Default user name + that default users groups (if added/used) default_user: name: ubuntu lock_passwd: false plain_text_passwd: '123456' gecos: Ubuntu groups: [adm, audio, cdrom, dialout, dip, floppy, netdev, plugdev, sudo, video] sudo: [\"ALL=(ALL) NOPASSWD:ALL\"] shell: /bin/bash ssh_svcname: ssh 3、安装cloud-utils-growpart来允许分区来调整大小 yum install -y cloud-utils-growpart 4、为了虚拟机能进入metadata服务器，禁用默认的zeroconf route # echo \"NOZEROCONF=yes\" >> /etc/sysconfig/network 5、配置控制台，编辑/etc/default/grub中的GRUB_CMDLIME_LINUX选择。删除 rhgb quiet 并且添加 console=tty0 console=ttyS0,115200n8 ... GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=cl/root rd.lvm.lv=cl/swap console=tty0 console=ttyS0,115200n8\" grub2-mkconfig -o /boot/grub2/grub.cfg 6、关机 poweroff 7、清除MAC地址等详细信息 yum -y install libguestfs-tools virt-sysprep -d centos 8、取消libvirt domain 的定义 virsh undefine centos 2）、其他 apt-get install kvm qemu-img create -f raw /root/ubuntu16.04.1.raw 20G virt-install --virt-type kvm --name ubuntu16.04.1 --ram 10240 \\ --cdrom=/root/ubuntu-16.04.1-server-amd64.iso \\ --disk /root/ubuntu16.04.1.raw,format=raw \\ --network network=default \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --os-type=linux --os-variant=ubuntutrusty virsh list --all virsh vncdisplay ubuntu16.04.1 virsh start trusty --paused virsh attach-disk --type cdrom --mode readonly ubuntu16.04.1 \"\" hdc virsh resume ubuntu16.04.1 apt-get install cloud-init dpkg-reconfigure cloud-init /sbin/shutdown -h now virt-sysprep -d ubuntu16.04.1 # rm -rf /etc/udev/rules.d/70-persistent-net.rules GRUB_CMDLINE_LINUX_DEFAULT=\"text console=tty0 console=ttyS0,115200n8\" update-grub openstack image create \"cirros\" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw # 其他 virt-install --virt-type kvm --name centos --ram 10240 \\ --disk /root/CentOS7.raw,format=raw \\ --network network=default \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --os-type=linux --os-variant=rhel7 \\ --location=/root/CentOS-7-x86_64-DVD-1511.iso qemu-img create -f qcow2 win7.qcow2 20G virt-install --connect qemu:///system \\ --name win7 --ram 10240 --vcpus 4 \\ --network network=default,model=virtio \\ --disk path=/root/win7/win7.qcow2,format=qcow2,device=disk,bus=virtio \\ --cdrom /root/win7/cn_windows_7_ultimate_x64_dvd_x15-66043.iso \\ --disk path=/root/win7/virtio-win-0.1.126.iso,device=cdrom \\ --vnc --os-type windows --os-variant win2k8 三、FAQ 1、禁用cloud-init网络 cat /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg network: {config: disabled} 2、disable cloud init networking echo \"network: {config: disabled}\" > /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg "},"notes/openstack/openstack_win_vm_diaplay.html":{"url":"notes/openstack/openstack_win_vm_diaplay.html","title":"OpenStack Windows 虚拟机分辨率问题","keywords":"","body":"openstack windows虚拟机分辨率问题 glance image-upload --property hw_video_model=vga …… glance image-update --property hw_video_model=vga [image_uuid] Libvirt architecture - name of guest hardware architecture eg i686, x86_64, ppc64 hw_cdrom_bus - name of the CDROM bus to use eg virtio, scsi, ide hw_disk_bus - name of the hard disk bus to use eg virtio, scsi, ide hw_floppy_bus - name of the floppy disk bus to use eg fd, scsi, ide hw_qemu_guest_agent - boolean 'yes' or 'no' to enable QEMU guest agent hw_rng - name of the RNG device type eg virtio (pending merge) hw_scsi_model - name of the SCSI bus controller eg 'virtio-scsi', 'lsilogic', etc (pending merge) hw_video_model - name of the video adapter model to use, eg cirrus, vga, xen, qxl hw_video_ram - MB of video RAM to provide eg 64 (pending merge) hw_vif_model - name of a NIC device model eg virtio, e1000, rtl8139 hw_watchdog_action - action to take when watchdog device fires eg reset, poweroff, pause, none (pending merge) os_command_line - string of boot time command line arguments for the guest kernel VMWare https://zhuanlan.zhihu.com/p/26670418 https://wiki.openstack.org/wiki/VirtDriverImageProperties "},"notes/openstack/kolla_network.html":{"url":"notes/openstack/kolla_network.html","title":"Kolla 网络规划","keywords":"","body":"kolla网络规划 kolla_internal_vip_address: \"192.168.110.10\" kolla_external_vip_address: \"192.168.21.10\" network_interface: \"em1\" kolla_external_vip_interface: \"em2\" #api_interface: \"{{ network_interface }}\" #storage_interface: \"{{ network_interface }}\" #cluster_interface: \"{{ network_interface }}\" #tunnel_interface: \"{{ network_interface }}\" #dns_interface: \"{{ network_interface }}\" neutron_external_interface: \"em3\" em1：管理网网卡（internel、存储。。。） em2：管理网外部网卡（public） em3：外网网卡（浮动ip等，up网卡，不用配置ip） "},"notes/openstack/kolla_deploy.html":{"url":"notes/openstack/kolla_deploy.html","title":"Kolla 部署相关","keywords":"","body":"kolla部署相关 一、环境要求 两张网卡 除系统盘外，至少1块磁盘 二、安装 kolla deploy 1、安装相关依赖 #CentOS dnf install -y python3-devel libffi-devel gcc openssl-devel python3-libselinux python3-venv #Ubuntu apt install -y python3-dev libffi-dev gcc libssl-dev python3-venv 2、安装docker aliyun 安装docker-ce tsinghua 安装docker-ce curl -sSL https://get.docker.io | bash or curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 3、python 虚拟环境 mkdir /kolla python3 -m venv /kolla/venv3 source /kolla/venv3/bin/activate 4、禁用防火墙 setenforce 0 systemctl stop firewalld systemctl disable firewalld iptables -L 5、安装 kolla 和 kolla-ansible cd /kolla git clone -b stable/wallaby --depth=1 https://github.com/openstack/kolla git clone -b stable/wallaby --depth=1 https://github.com/openstack/kolla-ansible pip install ./kolla pip install ./kolla-ansible pip install docker python-openstackclient 6、复制配置文件globals.yml、password.yml到/etc中 cp -r kolla-ansible/etc/kolla /etc/ cp -r kolla-ansible/ansible/inventory/multinode . 7、修改multinode [control] node1 ansible_python_interpreter=/kolla/venv3/bin/python3 [network] node1 ansible_python_interpreter=/kolla/venv3/bin/python3 [compute] node1 ansible_python_interpreter=/kolla/venv3/bin/python3 [monitoring] node1 ansible_python_interpreter=/kolla/venv3/bin/python3 [storage] node1 ansible_python_interpreter=/kolla/venv3/bin/python3 [deployment] node1 ansible_connection=local ansible_python_interpreter=/kolla/venv3/bin/python3 8、ceph 准备 部署参考，ceph部署 ceph osd pool create volumes 128 ceph osd pool create images 128 ceph osd pool create backups 128 ceph osd pool create vms 128 rbd pool init volumes rbd pool init images rbd pool init backups rbd pool init vms ceph auth get-or-create client.glance mon 'profile rbd' osd 'profile rbd pool=images' ceph auth get-or-create client.cinder mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images' ceph auth get-or-create client.cinder-backup mon 'profile rbd' osd 'profile rbd pool=backups' ceph auth get-or-create client.glance|tee ceph.client.glance.keyring ceph auth get-or-create client.cinder|tee ceph.client.cinder.keyring ceph auth get-or-create client.cinder-backup|tee ceph.client.cinder-backup.keyring 9、部署前配置 kolla-genpwd tree /etc/kolla/config/ ├── cinder │ ├── ceph.client.cinder.keyring │ ├── ceph.conf │ ├── cinder-backup │ │ ├── ceph.client.cinder-backup.keyring │ │ ├── ceph.client.cinder.keyring │ │ └── ceph.conf │ └── cinder-volume │ ├── ceph.client.cinder.keyring │ └── ceph.conf ├── glance │ ├── ceph.client.glance.keyring │ └── ceph.conf ├── glance.conf └── nova ├── ceph.client.cinder.keyring └── ceph.conf vim /etc/kolla/globals.yaml config_strategy: \"COPY_ALWAYS\" kolla_base_distro: \"ubuntu\" kolla_install_type: \"source\" openstack_release: \"wallaby\" kolla_internal_vip_address: \"192.168.0.243\" kolla_external_vip_address: \"{{ kolla_internal_vip_address }}\" docker_registry: 192.168.0.10:3000 docker_namespace: \"kolla\" network_interface: \"eno1\" neutron_external_interface: \"eno2\" keepalived_virtual_router_id: \"242\" enable_chrony: \"yes\" enable_cinder: \"yes\" enable_cinder_backup: \"no\" enable_cyborg: \"no\" enable_fluentd: \"yes\" enable_magnum: \"yes\" external_ceph_cephx_enabled: \"yes\" ceph_glance_keyring: \"ceph.client.glance.keyring\" ceph_glance_user: \"glance\" ceph_glance_pool_name: \"images\" ceph_cinder_keyring: \"ceph.client.cinder.keyring\" ceph_cinder_user: \"cinder\" ceph_cinder_pool_name: \"volumes\" ceph_nova_keyring: \"{{ ceph_cinder_keyring }}\" #ceph_nova_user: \"nova\" ceph_nova_user: \"{{ ceph_cinder_user }}\" ceph_nova_pool_name: \"vms\" glance_backend_ceph: \"yes\" glance_backend_file: \"no\" cinder_backend_ceph: \"yes\" nova_backend_ceph: \"yes\" 10、部署前检查（可选） kolla-ansible -i /kolla/multinode prechecks 11、拉取镜像（可选） kolla-ansible -i /kolla/multinode pull kolla-ansible pull -e 'ansible_python_interpreter=/kolla/venv3/bin/python3' 12、部署 kolla-ansible -i /kolla/multinode deploy # 只部署某些组件 kolla-ansible -i /kolla/multinode deploy --tags=\"haproxy\" # 过滤部署某些组件 kolla-ansible -i /kolla/multinode deploy --skip-tags=\"haproxy\" 13、生成 admin-openrc.sh kolla-ansible -i /kolla/multinode post-deploy 14、运行 init-runonce init-runonce参考 cd /usr/share/kolla-ansible vim init-runonce ./init-runonce 二、其他问题 1、ceph pg size问题 pool_pg_num 默认是128，用户需要根据需要进行修改,对于此平台我们提供以下计算公式供用户设定pool_pg_num。 40*osd_size >= replic_size *(8 + pool_pg_num) 注: osd_size为集群中osd的总数，此处即为所有存储服务器上用作存储的硬盘数目之和。 replic_size为每个pool的副本数，默认为3. pool_pg_num是我们需要计算的pg数，pool_pg_num必须为2的N次方。 2、kolla-ansible自带工具 # 可用于从系统中移除部署的容器 /usr/local/share/kolla-ansible/tools/cleanup-containers #可用于移除由于残余网络变化引发的docker启动的neutron-agents主机 /usr/local/share/kolla-ansible/tools/cleanup-host #可用于从本地缓存中移除所有的docker image /usr/local/share/kolla-ansible/tools/cleanup-images 3、mariadb集群出现故障 #!/bin/bash ansible -i multinode all -m shell -a 'docker stop mariadb' ansible -i multinode all -m shell -a \"sed -i 's/safe_to_bootstrap: 0/safe_to_bootstrap: 1/g' /var/lib/docker/volumes/mariadb/_data/grastate.dat\" kolla-ansible mariadb_recovery -i multinode 4、Some qemu processes were detected.\\nDocker will not be able to stop the nova_libvirt container with those running. pgrep qemu ##查找qemu的进程ID kill -9 5、清除 iptables 规则 iptables -F; iptables -X; iptables -Z 6、清除上次部署 kolla-ansible destroy -i multinode --yes-i-really-really-mean-it 7、rabbitmq异常 先重启所有节点rabbitmq（多适用于关机导致的异常） ansible -i multinode all -m shell -a 'docker restart rabbitmq' #Multiple different configurations with equal version numbers detected. Shutting down. 如果重启节点没用，再删除并重新部署所有节点rabbitmq（多适用于部署时出现的异常） ansible -i multinode all -m shell -a 'docker rm -f rabbitmq' ansible -i multinode all -m shell -a 'docker volume rm rabbitmq' ansible -i multinode all -m shell -a 'rm -rf /etc/kolla/rabbitmq' kolla-ansible deploy -i multinode 8、nova_libvirt异常 ansible -i multinode all -m shell -a 'rm -rf /var/run/libvirtd.pid;docker restart nova_libvirt nova_compute' 9、扩展 修改globals.yaml 若只是修改一些不涉及组件和镜像的配置（不增删容器），只需修改完globals后，upgrade、post-deploy即可 kolla-ansible upgrade -i multinode kolla-ansible post-deploy -i multinode 若新开组件和关闭组件(包括增加新的容器，如新起osd容器)，以及更新openstack版本，不清理之前配置直接deploy即可 # Valid option is Docker repository tag openstack_release: “4.0.0” #修改为最新的tag kolla-ansible deploy -i multinode kolla-ansible post-deploy -i multinode 如果修改了/etc/config/[server].conf kolla-ansible reconfigure --tags [server] -i multinode 10、缩减 ceph mon(例删除mon 10.0.0.1) docker exec ceph_mon ceph mon dump docker exec ceph_mon ceph mon rm 10.0.0.1 docker exec ceph_mon ceph osd crush remove 10.0.0.1 删除各个节点/var/lib/docker/volumes/ceph_mon_config/_data/ceph.conf对应条目 删除multinode对应条目 osd(例删除osd.0) docker exec ceph_mon ceph osd out osd.0 docker exec ceph_mon ceph osd crush rm osd.0 docker exec ceph_mon ceph auth del osd.0 docker exec ceph_mon ceph osd down osd.0 docker exec ceph_mon ceph osd rm osd.0 openstack 减少controller（控制）节点 vim multinode 去掉相关控制节点 kolla-ansible deploy -i multinode 减少compute（计算）节点 openstack compute service list openstack compute service delete ID vim multinode 去掉相关计算节点 11、修改openstack容器内的源码 例nova-api： docker exec -itu0 nova_api bash cd /var/lib/kolla/venv/lib/python2.7/site-packages/nova/ 12、指定python环境 kolla-ansible pull -e 'ansible_python_interpreter=/root/venv3/bin/python3' kolla-ansible -e 'ansible_python_interpreter=/root/venv3/bin/python3' -i multinode prechecks kolla-ansible -e 'ansible_python_interpreter=/root/venv3/bin/python3' -i multinode deploy [control] kolla1 ansible_python_interpreter=/root/venv3/bin/python3 kolla2 ansible_python_interpreter=/root/venv3/bin/python3 kolla3 ansible_python_interpreter=/root/venv3/bin/python3 13、防止部署neutron-dhcp-agent时失败，配置kolla.conf # Create the drop-in unit directory for docker.service mkdir -p /etc/systemd/system/docker.service.d # Create the drop-in unit file tee /etc/systemd/system/docker.service.d/kolla.conf 14、禁用宿主机的libvirt # CentOS 7 systemctl stop libvirtd.service systemctl disable libvirtd.service # Ubuntu service libvirt-bin stop update-rc.d libvirt-bin disable /usr/sbin/libvirtd: error while loading shared libraries: libvirt-admin.so.0: cannot open shared object file: Permission denied sudo apparmor_parser -R /etc/apparmor.d/usr.sbin.libvirtd 15、旧版本，配置磁盘（新版本默认使用 bluestore） [storage] storage_node1_hostname ceph_osd_store_type=bluestore storage_node2_hostname ceph_osd_store_type=bluestore storage_node3_hostname ceph_osd_store_type=filestore storage_node4_hostname ceph_osd_store_type=filestore parted /dev/sdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_BS 1 -1 # 日志和数据在同一个磁盘 parted /dev/xvdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1 parted /dev/xvdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1 ansible -i multinode all -m shell -a 'parted /dev/vdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1' # 使用日志盘 parted /dev/vdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDB 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 parted /dev/vdd -s mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDB_J 0% 5GB \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 5GB 100% parted /dev/vdb print parted /dev/vdc print 16、配置网络 bond0 宿主机网卡，管理网网卡（internel、存储。。。） bond1：管理网外部网卡（public,vip） bond2：外网网卡（浮动ip等，up网卡，不用配置ip） bond3:存储网络、tunnel网络 ip addr show ip link set bond2 up 17、添加config文件（可选） mkdir /etc/kolla/config cd /etc/kolla/config touch ceph.conf touch cinder.conf touch nova.conf touch keystone.conf touch polling.yaml [global] osd pool default size = 3 osd pool default min size = 2 mon_max_pg_per_osd = 200 osd crush update on start = false [DEFAULT] enable_force_upload = true [vnc] novncproxy_base_url = http://111.111.111.111:6080/vnc_auto.html [libvirt] cpu_mode = host-passthrough [token] driver = keystone.token.backends.sql.Token expiration = 86400 sources: - interval: 180 meters: - cpu - cpu_util - cpu_l3_cache - memory.usage - network.incoming.bytes - network.incoming.packets - network.outgoing.bytes - network.outgoing.packets - disk.device.read.bytes - disk.device.read.requests - disk.device.write.bytes - disk.device.write.requests - disk.device.usage - disk.device.iops name: some_pollsters 18、调整日志 ln -sf /var/lib/docker/volumes/kolla_logs/_data/ /var/log/kolla 19、格式转换与上次镜像 qemu-img convert -f qcow2 -O raw cirros-0.5.2-x86_64-disk.img cirros-0.5.2-x86_64-disk.raw openstack image create --disk-format qcow2 --container-format bare --public --property os_type=linux --file cirros-0.5.2-x86_64-disk.img cirros 20、镜像 kolla/ubuntu-source-nova-compute:wallaby kolla/ubuntu-source-cinder-volume:wallaby kolla/ubuntu-source-nova-novncproxy:wallaby kolla/ubuntu-source-cinder-api:wallaby kolla/ubuntu-source-cinder-scheduler:wallaby kolla/ubuntu-source-nova-conductor:wallaby kolla/ubuntu-source-nova-ssh:wallaby kolla/ubuntu-source-nova-api:wallaby kolla/ubuntu-source-nova-scheduler:wallaby kolla/ubuntu-source-keystone-ssh:wallaby kolla/ubuntu-source-keystone:wallaby kolla/ubuntu-source-keystone-fernet:wallaby kolla/ubuntu-source-magnum-api:wallaby kolla/ubuntu-source-magnum-conductor:wallaby kolla/ubuntu-source-neutron-server:wallaby kolla/ubuntu-source-placement-api:wallaby kolla/ubuntu-source-horizon:wallaby kolla/ubuntu-source-neutron-dhcp-agent:wallaby kolla/ubuntu-source-neutron-metadata-agent:wallaby kolla/ubuntu-source-neutron-l3-agent:wallaby kolla/ubuntu-source-neutron-openvswitch-agent:wallaby kolla/ubuntu-source-glance-api:wallaby kolla/ubuntu-source-cyborg-agent:wallaby kolla/ubuntu-source-cyborg-conductor:wallaby kolla/ubuntu-source-cyborg-api:wallaby kolla/ubuntu-source-heat-engine:wallaby kolla/ubuntu-source-heat-api-cfn:wallaby kolla/ubuntu-source-heat-api:wallaby kolla/ubuntu-source-kolla-toolbox:wallaby kolla/ubuntu-source-mariadb-server:wallaby kolla/ubuntu-source-mariadb-clustercheck:wallaby kolla/ubuntu-source-openvswitch-db-server:wallaby kolla/ubuntu-source-nova-libvirt:wallaby kolla/ubuntu-source-openvswitch-vswitchd:wallaby kolla/ubuntu-source-rabbitmq:wallaby kolla/ubuntu-source-fluentd:wallaby kolla/ubuntu-source-memcached:wallaby kolla/ubuntu-source-chrony:wallaby kolla/ubuntu-source-cron:wallaby kolla/ubuntu-source-keepalived:wallaby kolla/ubuntu-source-haproxy:wallaby 三、添加节点 adding-and-removing-hosts kolla-ansible -i bootstrap-servers kolla-ansible -i pull kolla-ansible -i deploy 四、卸载节点 l3_id=$(openstack network agent list --host --agent-type l3 -f value -c ID) #target_l3_id=$(openstack network agent list --host --agent-type l3 -f value -c ID) openstack router list --agent $l3_id -f value -c ID | while read router; do openstack network agent remove router $l3_id $router --l3 # openstack network agent add router $target_l3_id $router --l3 done openstack network agent set $l3_id --disable dhcp_id=$(openstack network agent list --host --agent-type dhcp -f value -c ID) #target_dhcp_id=$(openstack network agent list --host --agent-type dhcp -f value -c ID) openstack network list --agent $dhcp_id -f value -c ID | while read network; do openstack network agent remove network $dhcp_id $network --dhcp # openstack network agent add network $target_dhcp_id $network --dhcp done kolla-ansible -i stop --yes-i-really-really-mean-it openstack network agent list --host -f value -c ID | while read id; do openstack network agent delete $id done openstack compute service list --os-compute-api-version 2.53 --host -f value -c ID | while read id; do openstack compute service delete --os-compute-api-version 2.53 $id done openstack compute service set nova-compute --disable openstack server list --all-projects --host -f value -c ID | while read server; do openstack server migrate --live-migration $server done "},"notes/openstack/kolla_image_build.html":{"url":"notes/openstack/kolla_image_build.html","title":"Kolla 镜像构建","keywords":"","body":"kolla镜像构建 官网地址 1、环境准备 git clone https://github.com/openstack/kolla.git cd kolla git checkout -b queens origin/stable/queens pip install tox tox -e genconfig 2、配置 build配置文件 kolla/etc/kolla/kolla-build.conf [DEFAULT] base = centos base_tag = 7.4.1708 #tarballs_base = http://tarballs.openstack.org install_type = sourc tag = queens logs_dir = /home/hl/kolla_build/kolla/log [ceilometer-base] location = http://127.0.0.1/tars/ceilometer-10.0.0.tar.gz [cinder-base] location = http://127.0.0.1/tars/cinder-12.0.1.tar.gz [horizon] location = http://127.0.0.1/tars/horizon-13.0.0.tar.gz [horizon-plugin-fwaas-dashboard] #location = $tarballs_base/neutron-fwaas-dashboard/neutron-fwaas-dashboard-1.3.0.tar.gz location = http://127.0.0.1/tars/neutron-fwaas-dashboard-1.3.0.tar.gz [horizon-plugin-neutron-lbaas-dashboard] location = http://192.168.110.12/tars/neutron-lbaas-dashboard-4.0.0.tar.gz [horizon-plugin-trove-dashboard] location = http://192.168.110.12/tars/trove-dashboard-10.0.0.tar.gz [neutron-base] location = http://192.168.110.12/tars/neutron-12.0.4.tar.gz [nova-base] location = http://192.168.110.12/tars/nova-17.0.2.tar.gz 自定义编译某个组件tar包，将http://tarballs.openstack.org中的相应包，下载，解压，上传到自己git仓库 cd /var/www/html/git/nova/ git pull origin master rm -rf nova-17.0.2.tar.gz git archive --format=tar.gz --prefix=nova-17.0.2/ master >nova-17.0.2.tar.gz \\cp -rf /var/www/html/git/nova/nova-17.0.2.tar.gz /var/www/html/tars/ cd /var/www/html/git/cinder/ git pull origin master rm -rf cinder-12.0.1.tar.gz git archive --format=tar.gz --prefix=cinder-12.0.1/ master >cinder-12.0.1.tar.gz \\cp -rf /var/www/html/git/cinder/cinder-12.0.1.tar.gz /var/www/html/tars/ build需要的项目列表文件 ~/kp.list nova neutron keystone glance cinder ceph magnum horizon toolbox fluentd openvswitch mariadb memcached rabbitmq keepalived haproxy cron iscsi 3、修改kolla/docker/base中的源为国内源 略 4、编译 cd kolla cat ~/kp.list | xargs ./tools/build.py --config-file ./etc/kolla/kolla-build.conf 5、kolla image kolla/centos-source-elasticsearch kolla/centos-source-kibana kolla/centos-source-ceilometer-compute kolla/centos-source-ceilometer-notification kolla/centos-source-ceilometer-central kolla/centos-source-heat-api kolla/centos-source-heat-api-cfn kolla/centos-source-heat-engine kolla/centos-source-cinder-volume kolla/centos-source-cinder-backup kolla/centos-source-cinder-api kolla/centos-source-cinder-scheduler kolla/centos-source-neutron-openvswitch-agent kolla/centos-source-nova-compute kolla/centos-source-neutron-lbaas-agent kolla/centos-source-neutron-l3-agent kolla/centos-source-nova-api kolla/centos-source-neutron-metadata-agent kolla/centos-source-neutron-dhcp-agent kolla/centos-source-nova-ssh kolla/centos-source-neutron-server kolla/centos-source-nova-placement-api kolla/centos-source-nova-novncproxy kolla/centos-source-nova-scheduler kolla/centos-source-nova-consoleauth kolla/centos-source-nova-conductor kolla/centos-source-keystone kolla/centos-source-panko-api kolla/centos-source-glance-api kolla/centos-source-glance-registry kolla/centos-source-gnocchi-statsd kolla/centos-source-gnocchi-api kolla/centos-source-gnocchi-metricd kolla/centos-source-collectd kolla/centos-source-ceph-mon kolla/centos-source-ceph-rgw kolla/centos-source-ceph-mgr kolla/centos-source-ceph-osd kolla/centos-source-openvswitch-db-server kolla/centos-source-openvswitch-vswitchd kolla/centos-source-kolla-toolbox kolla/centos-source-rabbitmq kolla/centos-source-nova-libvirt kolla/centos-source-fluentd kolla/centos-source-cron kolla/centos-source-mariadb kolla/centos-source-keepalived kolla/centos-source-haproxy kolla/centos-source-memcached "},"notes/openstack/devstack.html":{"url":"notes/openstack/devstack.html","title":"Devstack 相关","keywords":"","body":"devstack相关 1、添加stack user，devstack需要运行在非root用户。 sudo useradd -s /bin/bash -d /opt/stack -m stack 2、给用户提供sudo权限。 echo \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stack sudo su - stack 3、下载 devstack git clone https://git.openstack.org/openstack-dev/devstack cd devstack 4、修改pip源、软件源 略 5、创建local.conf文件，以下是mitaka版本的配置文件。 [[local|localrc]] # use TryStack git mirror GIT_BASE=http://git.trystack.cn NOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.git SPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git # Define images to be automatically downloaded during the DevStack built process. DOWNLOAD_DEFAULT_IMAGES=False IMAGE_URLS=\"http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img\" # Credentials DATABASE_PASSWORD=0127 ADMIN_PASSWORD=0127 SERVICE_PASSWORD=0127 SERVICE_TOKEN=0127 RABBIT_PASSWORD=0127 #FLAT_INTERFACE=eth0 HOST_IP=172.18.20.157 SERVICE_HOST=172.18.20.157 MYSQL_HOST=172.18.20.157 RABBIT_HOST=172.18.20.157 GLANCE_HOSTPORT=172.18.20.157:9292 # Database Backend MySQL enable_service mysql # RPC Backend RabbitMQ enable_service rabbit # Enable Keystone - OpenStack Identity Service enable_service key # Horizon - OpenStack Dashboard Service enable_service horizon # Enable Glance - OpenStack Image service enable_service g-api g-reg # Enable Cinder - Block Storage service for OpenStack VOLUME_GROUP=\"cinder-volumes\" enable_service cinder c-api c-vol c-sch c-bak # Enable Heat (orchestration) Service enable_service heat h-api h-api-cfn h-api-cw h-eng # Enable Tempest - The OpenStack Integration Test Suite enable_service tempest # Enable NoVNC enable_service n-novnc # Enabling Neutron (network) Service disable_service n-net enable_service q-svc enable_service q-agt enable_service q-dhcp enable_service q-l3 enable_service q-meta enable_service q-metering enable_service neutron ## Neutron options Q_USE_SECGROUP=True FLOATING_RANGE=\"172.18.20.0/24\" FIXED_RANGE=\"10.0.1.0/24\" NETWORK_GATEWAY=\"10.0.1.254\" Q_FLOATING_ALLOCATION_POOL=start=172.18.20.180,end=172.18.20.186 PUBLIC_NETWORK_GATEWAY=\"172.18.20.254\" Q_L3_ENABLED=True PUBLIC_INTERFACE=eth0 Q_USE_PROVIDERNET_FOR_PUBLIC=True OVS_PHYSICAL_BRIDGE=br-ex PUBLIC_BRIDGE=br-ex OVS_BRIDGE_MAPPINGS=public:br-ex # VLAN configuration. Q_PLUGIN=ml2 ENABLE_TENANT_VLANS=True # Branches KEYSTONE_BRANCH=stable/mitaka NOVA_BRANCH=stable/mitaka NEUTRON_BRANCH=stable/mitaka GLANCE_BRANCH=stable/mitaka CINDER_BRANCH=stable/mitaka HEAT_BRANCH=stable/mitaka HORIZON_BRANCH=stable/mitaka # Select Keystone's token format # Choose from 'UUID', 'PKI', or 'PKIZ' # INSERT THIS LINE... KEYSTONE_TOKEN_FORMAT=${KEYSTONE_TOKEN_FORMAT:-UUID} KEYSTONE_TOKEN_FORMAT=$(echo ${KEYSTONE_TOKEN_FORMAT} | tr '[:upper:]' '[:lower:]') # Work offline #OFFLINE=True # Reclone each time RECLONE=yes # Logging DEST=/home/stack.mitaka LOGFILE=/home/stack.mitaka/logs/stack.sh.log VERBOSE=True LOG_COLOR=True SCREEN_LOGDIR=/home/stack.mitaka/logs 6、运行安装 ./stack.sh 7、官网地址 devstack "},"notes/openstack/heat.html":{"url":"notes/openstack/heat.html","title":"Heat 相关","keywords":"","body":"heat相关 read -p \"how many groups do you want:\" groups read -p \"private_net_name:\" private_net_name for((i=1;iheat_template_version: 2013-05-23 description: > HOT template to create a new neutron network plus a router to the public network, and for deploying two servers into the new network. The template also assigns floating IP addresses to each server so they are routable from the public network. parameters: myimage: type: string description: testsssssssssssssssssssssssss constraints: - custom_constraint: glance.image default: c1026774-a7ed-4dd3-8b1e-0f67b87b42af myname: type: string description: myname default: ttttt key_name: type: string description: Name of keypair to assign to servers constraints: - custom_constraint: nova.keypair default: mykey flavor: type: string description: Flavor to use for servers constraints: - custom_constraint: nova.flavor default: 7377696f-dd6b-41bd-85ec-2043dbebcc5e networks: type: string description: Name of an existing to use for the server constraints: - custom_constraint: neutron.network resources: servertest: type: OS::Nova::Server properties: name: { get_param: myname } image: { get_param: myimage } flavor: { get_param: flavor } key_name: { get_param: key_name } networks: [{ 'network':{ get_param: networks}}] heat_template_version: 2013-05-23 description: > HOT template to create a new neutron network plus a router to the public network, and for deploying two servers into the new network. The template also assigns floating IP addresses to each server so they are routable from the public network. parameters: key_name: type: string description: Name of keypair to assign to servers constraints: - custom_constraint: nova.keypair default: mykey image: type: string description: Name of image to use for servers constraints: - custom_constraint: glance.image default: 16fa0162-35d7-4e92-8591-4f60f88c57f3 image_desktop: type: string description: Name of image to use for servers constraints: - custom_constraint: glance.image default: 0b0147f5-34d2-47c3-b20b-82513caf6f83 flavor: type: string description: Flavor to use for servers constraints: - custom_constraint: nova.flavor default: 335eb246-f1c9-426f-a4da-36c5e71741a9 flavor_desktop: type: string description: Flavor to use for desktops constraints: - custom_constraint: nova.flavor default: 335eb246-f1c9-426f-a4da-36c5e71741a9 login_pass: type: string description: Login password hidden: false default: 123456 public_net: type: string description: > ID or name of public network for which floating IP addresses will be allocated constraints: - custom_constraint: neutron.network default: 1f6a08a2-421e-4449-a768-095426d829e8 private_net_name: type: string description: Name of private network to be created private_net_cidr: type: string description: Private network address (CIDR notation) default: 192.168.1.0/24 private_net_gateway: type: string description: Private network gateway address default: 192.168.1.254 private_net_pool_begin: type: string description: Start of private network IP address allocation pool default: 192.168.1.1 private_net_pool_end: type: string description: End of private network IP address allocation pool default: 192.168.1.253 dns_nameservers: type: string description: A specified set of DNS name servers to be used default: 192.168.90.127 resources: private_net: type: OS::Neutron::Net properties: name: { get_param: private_net_name } private_subnet: type: OS::Neutron::Subnet properties: network_id: { get_resource: private_net } cidr: { get_param: private_net_cidr } gateway_ip: { get_param: private_net_gateway } dns_nameservers: [{ get_param: dns_nameservers}] allocation_pools: - start: { get_param: private_net_pool_begin } end: { get_param: private_net_pool_end } router: type: OS::Neutron::Router properties: external_gateway_info: network: { get_param: public_net } router_interface: type: OS::Neutron::RouterInterface properties: router_id: { get_resource: router } subnet_id: { get_resource: private_subnet } server1_port: type: OS::Neutron::Port properties: network_id: { get_resource: private_net } fixed_ips: [{ 'ip_address': 192.168.1.2}] server1_floating_ip: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_net } port_id: { get_resource: server1_port } server1: type: OS::Nova::Server properties: name: Master image: { get_param: image } flavor: { get_param: flavor } key_name: { get_param: key_name } networks: - port: { get_resource: server1_port } user_data: str_replace: template: | #!/bin/bash #change login password /usr/bin/expect > /etc/hosts echo \"192.168.1.3 Slave1 slave1\" >> /etc/hosts echo \"192.168.1.4 Slave2 slave2\" >> /etc/hosts sed -i \"s/127.0.1.1/ /g\" /etc/hosts sed -i \"s/ubuntu/ /g\" /etc/hosts #ENV echo \"export JAVA_HOME=/opt/jdk1.8.0_111\" >> ~/.bashrc echo \"export HADOOP_HOME=/opt/hadoop-2.7.3\" >> ~/.bashrc echo \"export HBASE_HOME=/opt/hbase-1.2.3\" >> ~/.bashrc echo \"export ZOOKEEPER_HOME=/opt/zookeeper-3.4.9\" >> ~/.bashrc echo 'export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$HBASE_HOME/bin:$HBASE_HOME/sbin:$ZOOKEEPER_HOME/bin:$PATH' >> ~/.bashrc source ~/.bashrc sed -i 's/JAVA_HOME=${JAVA_HOME}/JAVA_HOME=\\/opt\\/jdk1.8.0_111/g' /opt/hadoop-2.7.3/etc/hadoop/hadoop-env.sh ##########################HADOOP#################### #configure /opt/hadoop-2.7.3/etc/hadoop/slaves echo \"Slave1\" > /opt/hadoop-2.7.3/etc/hadoop/slaves echo \"Slave2\" >> /opt/hadoop-2.7.3/etc/hadoop/slaves touch /opt/hadoop-2.7.3/etc/hadoop/master echo \"Master\" > /opt/hadoop-2.7.3/etc/hadoop/master #configure /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '21i fs.defaultFS' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '22i hdfs://Master:9000' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '25i hadoop.tmp.dir' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '26i file:/usr/local/hadoop/tmp' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '27i Abase for other temporary directories.' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '21i dfs.namenode.http-address' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '22i Master:50070' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '25i dfs.namenode.secondary.http-address' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '26i Master:50090' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '27i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '29i dfs.replication' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '30i 1' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '31i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '32i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '33i dfs.namenode.name.dir' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '34i file:/usr/local/hadoop/tmp/dfs/name' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '35i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '36i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '37i dfs.datanode.data.dir' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '38i file:/usr/local/hadoop/tmp/dfs/data' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '39i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml mv /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '21i mapreduce.framework.name' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '22i yarn' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '25i mapreduce.jobhistory.address' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '26i Master:10020' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '27i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '29i mapreduce.jobhistory.webapp.address' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '30i Master:19888' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '31i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '18i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '19i yarn.resourcemanager.hostname' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '20i Master' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '21i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '22i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '23i yarn.nodemanager.aux-services' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '24i mapreduce_shuffle' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '25i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml #Instantiate NameNode ####################################ZOO KEEPER#################################### #configure /opt/zookeeper-3.4.9/conf/zoo.cfg mv /opt/zookeeper-3.4.9/conf/zoo_sample.cfg /opt/zookeeper-3.4.9/conf/zoo.cfg sed -i 's/dataDir=\\/tmp\\/zookeeper/dataDir=\\/opt\\/zookeeper-3.4.9\\/data/g' /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.0=Master:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.1=Slave1:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.2=Slave2:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg mkdir /opt/zookeeper-3.4.9/data/ touch /opt/zookeeper-3.4.9/data/myid echo '0'> /opt/zookeeper-3.4.9/data/myid sed -i '131i JAVA=/opt/jdk1.8.0_111/bin/java' /opt/zookeeper-3.4.9/bin/zkServer.sh ####################################HBASE######################################### #configure /opt/hbase-1.2.3/conf/hbase-env.sh echo \"export JAVA_HOME=/opt/jdk1.8.0_111\" >> /opt/hbase-1.2.3/conf/hbase-env.sh echo \"export HBASE_MANAGES_ZK=false\" >> /opt/hbase-1.2.3/conf/hbase-env.sh #configure /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '24i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '25i hbase.rootdir' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '26i hdfs://Master:9000/hbase' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '27i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '28i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '29i hbase.cluster.distributed' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '30i true' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '31i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '32i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '33i hbase.zookeeper.quorum' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '34i Master,Slave1,Slave2' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '35i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '36i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '37i hbase.zookeeper.property.dataDir' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '38i /opt/zookeeper-3.4.9/data/' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '39i ' /opt/hbase-1.2.3/conf/hbase-site.xml #configure /opt/hbase-1.2.3/conf/regionservers echo 'Master' > /opt/hbase-1.2.3/conf/regionservers echo 'Slave1' >> /opt/hbase-1.2.3/conf/regionservers echo 'Slave2' >> /opt/hbase-1.2.3/conf/regionservers echo '#!/bin/bash passwd=login_pass /usr/bin/expect /root/start.sh chmod 777 /root/start.sh nohup /root/start.sh > /root/start.log #echo Login password echo \"######################################################\" echo \"######################################################\" echo \"Login as 'root' user. Default password: \"login_pass echo \"######################################################\" echo \"######################################################\" echo \"Login as 'root' user. Default password: \"login_pass | tee > /root/Login_Password params: login_pass: { get_param: login_pass } server2_port: type: OS::Neutron::Port properties: network_id: { get_resource: private_net } fixed_ips: [{ 'ip_address': 192.168.1.3}] server2: type: OS::Nova::Server properties: name: Slave1 image: { get_param: image } flavor: { get_param: flavor } key_name: { get_param: key_name } networks: - port: { get_resource: server2_port } user_data: str_replace: template: | #!/bin/bash #change login password /usr/bin/expect > /etc/hosts echo \"192.168.1.3 Slave1 slave1\" >> /etc/hosts echo \"192.168.1.4 Slave2 slave2\" >> /etc/hosts #ENV echo \"export JAVA_HOME=/opt/jdk1.8.0_111\" >> ~/.bashrc echo \"export HADOOP_HOME=/opt/hadoop-2.7.3\" >> ~/.bashrc echo \"export HBASE_HOME=/opt/hbase-1.2.3\" >> ~/.bashrc echo \"export ZOOKEEPER_HOME=/opt/zookeeper-3.4.9\" >> ~/.bashrc echo 'export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$HBASE_HOME/bin:$HBASE_HOME/sbin:$ZOOKEEPER_HOME/bin:$PATH' >> ~/.bashrc source ~/.bashrc sed -i 's/JAVA_HOME=${JAVA_HOME}/JAVA_HOME=\\/opt\\/jdk1.8.0_111/g' /opt/hadoop-2.7.3/etc/hadoop/hadoop-env.sh ##########################HADOOP#################### #configure /opt/hadoop-2.7.3/etc/hadoop/slaves echo \"Slave1\" > /opt/hadoop-2.7.3/etc/hadoop/slaves echo \"Slave2\" >> /opt/hadoop-2.7.3/etc/hadoop/slaves touch /opt/hadoop-2.7.3/etc/hadoop/master echo \"Master\" > /opt/hadoop-2.7.3/etc/hadoop/master #configure /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '21i fs.defaultFS' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '22i hdfs://Master:9000' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '25i hadoop.tmp.dir' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '26i file:/usr/local/hadoop/tmp' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '27i Abase for other temporary directories.' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '21i dfs.namenode.http-address' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '22i Master:50070' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '25i dfs.namenode.secondary.http-address' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '26i Master:50090' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '27i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '29i dfs.replication' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '30i 1' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '31i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '32i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '33i dfs.namenode.name.dir' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '34i file:/usr/local/hadoop/tmp/dfs/name' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '35i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '36i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '37i dfs.datanode.data.dir' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '38i file:/usr/local/hadoop/tmp/dfs/data' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '39i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml mv /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '21i mapreduce.framework.name' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '22i yarn' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '25i mapreduce.jobhistory.address' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '26i Master:10020' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '27i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '29i mapreduce.jobhistory.webapp.address' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '30i Master:19888' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '31i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '18i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '19i yarn.resourcemanager.hostname' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '20i Master' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '21i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '22i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '23i yarn.nodemanager.aux-services' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '24i mapreduce_shuffle' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '25i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml ####################################ZOO KEEPER#################################### #configure /opt/zookeeper-3.4.9/conf/zoo.cfg mv /opt/zookeeper-3.4.9/conf/zoo_sample.cfg /opt/zookeeper-3.4.9/conf/zoo.cfg sed -i 's/dataDir=\\/tmp\\/zookeeper/dataDir=\\/opt\\/zookeeper-3.4.9\\/data/g' /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.0=Master:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.1=Slave1:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.2=Slave2:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg mkdir /opt/zookeeper-3.4.9/data/ touch /opt/zookeeper-3.4.9/data/myid echo '1'> /opt/zookeeper-3.4.9/data/myid sed -i '131i JAVA=/opt/jdk1.8.0_111/bin/java' /opt/zookeeper-3.4.9/bin/zkServer.sh ####################################HBASE######################################### #configure /opt/hbase-1.2.3/conf/hbase-env.sh echo \"export JAVA_HOME=/opt/jdk1.8.0_111\" >> /opt/hbase-1.2.3/conf/hbase-env.sh echo \"export HBASE_MANAGES_ZK=false\" >> /opt/hbase-1.2.3/conf/hbase-env.sh #configure /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '24i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '25i hbase.rootdir' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '26i hdfs://Master:9000/hbase' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '27i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '28i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '29i hbase.cluster.distributed' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '30i true' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '31i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '32i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '33i hbase.zookeeper.quorum' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '34i Master,Slave1,Slave2' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '35i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '36i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '37i hbase.zookeeper.property.dataDir' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '38i /opt/zookeeper-3.4.9/data/' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '39i ' /opt/hbase-1.2.3/conf/hbase-site.xml #configure /opt/hbase-1.2.3/conf/regionservers echo 'Master' > /opt/hbase-1.2.3/conf/regionservers echo 'Slave1' >> /opt/hbase-1.2.3/conf/regionservers echo 'Slave2' >> /opt/hbase-1.2.3/conf/regionservers #echo Login password echo \"######################################################\" echo \"######################################################\" echo \"Login as 'root' user. Default password: \"login_pass echo \"######################################################\" echo \"######################################################\" echo \"Login as 'root' user. Default password: \"login_pass | tee > /root/Login_Password params: login_pass: { get_param: login_pass } server3_port: type: OS::Neutron::Port properties: network_id: { get_resource: private_net } fixed_ips: [{ 'ip_address': 192.168.1.4}] server3: type: OS::Nova::Server properties: name: Slave2 image: { get_param: image } flavor: { get_param: flavor } key_name: { get_param: key_name } networks: - port: { get_resource: server3_port } user_data: str_replace: template: | #!/bin/bash #change login password /usr/bin/expect > /etc/hosts echo \"192.168.1.3 Slave1 slave1\" >> /etc/hosts echo \"192.168.1.4 Slave2 slave2\" >> /etc/hosts #ENV echo \"export JAVA_HOME=/opt/jdk1.8.0_111\" >> ~/.bashrc echo \"export HADOOP_HOME=/opt/hadoop-2.7.3\" >> ~/.bashrc echo \"export HBASE_HOME=/opt/hbase-1.2.3\" >> ~/.bashrc echo \"export ZOOKEEPER_HOME=/opt/zookeeper-3.4.9\" >> ~/.bashrc echo 'export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin:$HBASE_HOME/bin:$HBASE_HOME/sbin:$ZOOKEEPER_HOME/bin:$PATH' >> ~/.bashrc source ~/.bashrc sed -i 's/JAVA_HOME=${JAVA_HOME}/JAVA_HOME=\\/opt\\/jdk1.8.0_111/g' /opt/hadoop-2.7.3/etc/hadoop/hadoop-env.sh ##########################HADOOP#################### #configure /opt/hadoop-2.7.3/etc/hadoop/slaves echo \"Slave1\" > /opt/hadoop-2.7.3/etc/hadoop/slaves echo \"Slave2\" >> /opt/hadoop-2.7.3/etc/hadoop/slaves touch /opt/hadoop-2.7.3/etc/hadoop/master echo \"Master\" > /opt/hadoop-2.7.3/etc/hadoop/master #configure /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '21i fs.defaultFS' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '22i hdfs://Master:9000' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '25i hadoop.tmp.dir' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '26i file:/usr/local/hadoop/tmp' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '27i Abase for other temporary directories.' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/core-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '21i dfs.namenode.http-address' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '22i Master:50070' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '25i dfs.namenode.secondary.http-address' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '26i Master:50090' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '27i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '29i dfs.replication' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '30i 1' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '31i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '32i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '33i dfs.namenode.name.dir' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '34i file:/usr/local/hadoop/tmp/dfs/name' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '35i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '36i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '37i dfs.datanode.data.dir' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '38i file:/usr/local/hadoop/tmp/dfs/data' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml sed -i '39i ' /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml mv /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '20i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '21i mapreduce.framework.name' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '22i yarn' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '23i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '24i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '25i mapreduce.jobhistory.address' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '26i Master:10020' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '27i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '28i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '29i mapreduce.jobhistory.webapp.address' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '30i Master:19888' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml sed -i '31i ' /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml #configure /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '18i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '19i yarn.resourcemanager.hostname' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '20i Master' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '21i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '22i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '23i yarn.nodemanager.aux-services' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '24i mapreduce_shuffle' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml sed -i '25i ' /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml ####################################ZOO KEEPER#################################### #configure /opt/zookeeper-3.4.9/conf/zoo.cfg mv /opt/zookeeper-3.4.9/conf/zoo_sample.cfg /opt/zookeeper-3.4.9/conf/zoo.cfg sed -i 's/dataDir=\\/tmp\\/zookeeper/dataDir=\\/opt\\/zookeeper-3.4.9\\/data/g' /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.0=Master:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.1=Slave1:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg echo \"server.2=Slave2:2888:3888\" >> /opt/zookeeper-3.4.9/conf/zoo.cfg mkdir /opt/zookeeper-3.4.9/data/ touch /opt/zookeeper-3.4.9/data/myid echo '2'> /opt/zookeeper-3.4.9/data/myid sed -i '131i JAVA=/opt/jdk1.8.0_111/bin/java' /opt/zookeeper-3.4.9/bin/zkServer.sh ####################################HBASE######################################### #configure /opt/hbase-1.2.3/conf/hbase-env.sh echo \"export JAVA_HOME=/opt/jdk1.8.0_111\" >> /opt/hbase-1.2.3/conf/hbase-env.sh echo \"export HBASE_MANAGES_ZK=false\" >> /opt/hbase-1.2.3/conf/hbase-env.sh #configure /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '24i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '25i hbase.rootdir' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '26i hdfs://Master:9000/hbase' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '27i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '28i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '29i hbase.cluster.distributed' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '30i true' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '31i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '32i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '33i hbase.zookeeper.quorum' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '34i Master,Slave1,Slave2' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '35i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '36i ' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '37i hbase.zookeeper.property.dataDir' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '38i /opt/zookeeper-3.4.9/data/' /opt/hbase-1.2.3/conf/hbase-site.xml sed -i '39i ' /opt/hbase-1.2.3/conf/hbase-site.xml #configure /opt/hbase-1.2.3/conf/regionservers echo 'Master' > /opt/hbase-1.2.3/conf/regionservers echo 'Slave1' >> /opt/hbase-1.2.3/conf/regionservers echo 'Slave2' >> /opt/hbase-1.2.3/conf/regionservers #echo Login password echo \"######################################################\" echo \"######################################################\" echo \"Login as 'root' user. Default password: \"login_pass echo \"######################################################\" echo \"######################################################\" echo \"Login as 'root' user. Default password: \"login_pass | tee > /root/Login_Password params: login_pass: { get_param: login_pass } server4_port: type: OS::Neutron::Port properties: network_id: { get_resource: private_net } fixed_ips: [{ 'ip_address': 192.168.1.5}] server4_floating_ip: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_net } port_id: { get_resource: server4_port } server4: type: OS::Nova::Server properties: name: dev1 image: { get_param: image_desktop } flavor: { get_param: flavor_desktop } key_name: { get_param: key_name } networks: - port: { get_resource: server4_port } user_data: str_replace: template: | #!/bin/bash /usr/bin/expect /root/Login_Password params: login_pass: { get_param: login_pass } server5_port: type: OS::Neutron::Port properties: network_id: { get_resource: private_net } fixed_ips: [{ 'ip_address': 192.168.1.6}] server5_floating_ip: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_net } port_id: { get_resource: server5_port } server5: type: OS::Nova::Server properties: name: dev2 image: { get_param: image_desktop } flavor: { get_param: flavor_desktop } key_name: { get_param: key_name } networks: - port: { get_resource: server5_port } user_data: str_replace: template: | #!/bin/bash /usr/bin/expect /root/Login_Password params: login_pass: { get_param: login_pass } server6_port: type: OS::Neutron::Port properties: network_id: { get_resource: private_net } fixed_ips: [{ 'ip_address': 192.168.1.7}] server6_floating_ip: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_net } port_id: { get_resource: server6_port } server6: type: OS::Nova::Server properties: name: dev3 image: { get_param: image_desktop } flavor: { get_param: flavor_desktop } key_name: { get_param: key_name } networks: - port: { get_resource: server6_port } user_data: str_replace: template: | #!/bin/bash /usr/bin/expect /root/Login_Password params: login_pass: { get_param: login_pass } outputs: server1: description: Login password of Master for user 'root' value: {get_param: login_pass} server1_private_ip: description: IP address of Master in private network value: { get_attr: [ server1, first_address ] } server1_public_ip: description: Floating IP address of Master in public network value: { get_attr: [ server1_floating_ip, floating_ip_address ] } server2: description: Login password of Slave1 for user 'root' value: {get_param: login_pass} server2_private_ip: description: IP address of Slave1 in private network value: { get_attr: [ server2, first_address ] } server3: description: Login password of Slave2 for user 'root' value: {get_param: login_pass} server3_private_ip: description: IP address of Slave2 in private network value: { get_attr: [ server3, first_address ] } server4: description: Login password of dev1 for user 'ubuntu' value: {get_param: login_pass} server4_private_ip: description: IP address of dev1 in private network value: { get_attr: [ server4, first_address ] } server4_public_ip: description: Floating IP address of ubuntu in public network value: { get_attr: [ server4_floating_ip, floating_ip_address ] } server5: description: Login password of dev2 for user 'ubuntu' value: {get_param: login_pass} server5_private_ip: description: IP address of dev2 in private network value: { get_attr: [ server5, first_address ] } server5_public_ip: description: Floating IP address of ubuntu in public network value: { get_attr: [ server5_floating_ip, floating_ip_address ] } server6: description: Login password of dev3 for user 'ubuntu' value: {get_param: login_pass} server6_private_ip: description: IP address of dev3 in private network value: { get_attr: [ server6, first_address ] } server6_public_ip: description: Floating IP address of ubuntu in public network value: { get_attr: [ server6_floating_ip, floating_ip_address ] } "},"notes/openstack/qga.html":{"url":"notes/openstack/qga.html","title":"qemu-guest-agent 相关","keywords":"","body":"qemu-guest-agent 相关 1、配置nova nova image-meta image-id set hw_qemu_guest_agent=yes 2、配置glance glance image-create --name cirros \\ --disk-format raw \\ --container-format bare \\ --file cirros-0.3.3-x86_64-disk.raw \\ --public \\ --property hw_qemu_guest_agent=yes \\ --progress 3、可以通过命令来读取虚拟机内部真实 ip virsh qemu-agent-command instance-xxx '{\"execute\":\"guest-network-get-interfaces\"}' virsh qemu-agent-command instance-00000083 '{\"execute\":\"guest-info\"}' |python -m json.tool virsh qemu-agent-command instance-00000083 '{\"execute\":\"guest-file-open\", \"arguments\":{\"path\":\"/proc/cpuinfo\",\"mode\":\"r\"}}' virsh qemu-agent-command instance-00000083 '{\"execute\":\"guest-file-read\", \"arguments\":{\"handle\":1003,\"count\":10000}}' 4、已有功能 目前qga最新版本为1.5.50，linux已经实现下面的所有功能，windows仅支持加*的那些功能： Ø guest-sync-delimited*：宿主机发送一个int数字给qga，qga返回这个数字，并且在后续返回字符串响应中加入ascii码为0xff的字符，其作用是检查宿主机与qga通信的同步状态，主要用在宿主机上多客户端与qga通信的情况下客户端间切换过程的状态同步检查，比如有两个客户端A、B，qga发送给A的响应，由于A已经退出，目前B连接到qga的socket，所以这个响应可能被B收到，如果B连接到socket之后，立即发送该请求给qga，响应中加入了这个同步码就能区分是A的响应还是B的响应；在qga返回宿主机客户端发送的int数字之前，qga返回的所有响应都要忽略； Ø guest-sync*：与上面相同，只是不在响应中加入0xff字符； Ø guest-ping*：Ping the guest agent, a non-error return implies success； Ø guest-get-time*：获取虚拟机时间（返回值为相对于1970-01-01 in UTC，Time in nanoseconds.）； Ø guest-set-time*：设置虚拟机时间（输入为相对于1970-01-01 in UTC，Time in nanoseconds.）； Ø guest-info*：返回qga支持的所有命令； Ø guest-shutdown*：关闭虚拟机（支持halt、powerdown、reboot，默认动作为powerdown）； Ø guest-file-open：打开虚拟机内的某个文件（返回文件句柄）； Ø guest-file-close：关闭打开的虚拟机内的文件； Ø guest-file-read：根据文件句柄读取虚拟机内的文件内容（返回base64格式的文件内容）； Ø guest-file-write：根据文件句柄写入文件内容到虚拟机内的文件； Ø guest-file-seek：Seek to a position in the file, as with fseek(), and return the current file position afterward. Also encapsulates ftell()'s functionality, just Set offset=0, whence=SEEK_CUR； Ø guest-file-flush：Write file changes bufferred in userspace to disk/kernel buffers； Ø guest-fsfreeze-status：Get guest fsfreeze state. error state indicates； Ø guest-fsfreeze-freeze：Sync and freeze all freezable, local guest filesystems； Ø guest-fsfreeze-thaw：Unfreeze all frozen guest filesystems； Ø guest-fstrim：Discard (or \"trim\") blocks which are not in use by the filesystem； Ø guest-suspend-disk*：Suspend guest to disk； Ø guest-suspend-ram*：Suspend guest to ram； Ø guest-suspend-hybrid：Save guest state to disk and suspend to ram（This command requires the pm-utils package to be installed in the guest.）； Ø guest-network-get-interfaces：Get list of guest IP addresses, MAC addresses and netmasks； Ø guest-get-vcpus：Retrieve the list of the guest's logical processors； guest-set-vcpus：Attempt to reconfigure (currently: enable/disable) logical processors inside the guest。 "},"notes/openstack/ovs.html":{"url":"notes/openstack/ovs.html","title":"OVS 相关","keywords":"","body":"OVS 相关 一、OVS调试 1、ovs-vsctl show 96a55a7e-f49c-4dbe-b359-bafdff2ccad7 Manager \"ptcp:6640:92.0.0.12\" Bridge br-tun Controller \"tcp:127.0.0.1:6633\" is_connected: true fail_mode: secure Port br-tun Interface br-tun type: internal Port \"vxlan-5c00000b\" Interface \"vxlan-5c00000b\" type: vxlan options: {df_default=\"true\", in_key=flow, local_ip=\"92.0.0.12\", out_key=flow, remote_ip=\"92.0.0.11\"} Port patch-int Interface patch-int type: patch options: {peer=patch-tun} Bridge br-int Controller \"tcp:127.0.0.1:6633\" is_connected: true fail_mode: secure Port \"qvo4fab3e51-fc\" tag: 3 Interface \"qvo4fab3e51-fc\" Port int-br-ex Interface int-br-ex type: patch options: {peer=phy-br-ex} Port patch-tun Interface patch-tun type: patch options: {peer=patch-int} Port br-int Interface br-int type: internal Bridge br-ex Controller \"tcp:127.0.0.1:6633\" is_connected: true fail_mode: secure Port phy-br-ex Interface phy-br-ex type: patch options: {peer=int-br-ex} Port \"ens4\" Interface \"ens4\" Port br-ex Interface br-ex type: internal 2、网桥查询 ovs-vsctl list-br br-ex br-int br-tun 3、端口查询 ovs-vsctl list-ports br-tun patch-int vxlan-5c00000b 4、接口查询 ovs-vsctl list-ifaces br-tun patch-int vxlan-5c00000b 5、端口、接口归属查询 ovs-vsctl port-to-br vxlan-5c00000b br-tun ovs-vsctl iface-to-br vxlan-5c00000b br-tun ovs-ofctl 6、查询网桥流表 样例 # ovs-ofctl dump-flows br-tun NXST_FLOW reply (xid=0x4): # 从port1进来的包转到表1处理 cookie=0x0, duration=10970.064s, table=0, n_packets=189, n_bytes=16232, idle_age=16, priority=1,in_port=1 actions=resubmit(,1) # 从port2进来的包转到表2处理 cookie=0x0, duration=10906.954s, table=0, n_packets=29, n_bytes=5736, idle_age=16, priority=1,in_port=2 actions=resubmit(,2) # 不匹配上面两条则drop cookie=0x0, duration=10969.922s, table=0, n_packets=3, n_bytes=230, idle_age=10962, priority=0 actions=drop # 表1，单播包转到表20处理 cookie=0x0, duration=10969.777s, table=1, n_packets=26, n_bytes=5266, idle_age=16, priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20) # 多播包转到表21处理 cookie=0x0, duration=10969.631s, table=1, n_packets=163, n_bytes=10966, idle_age=21, priority=0,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,21) # 表2，port2进来的包在这里处理了.同样是转给表10处理 cookie=0x0, duration=688.456s, table=2, n_packets=29, n_bytes=5736, idle_age=16, priority=1,tun_id=0x1 actions=mod_vlan_vid:1,resubmit(,10) # 表10，进行规则学习,具体就不解释了。学习到的规则后续会给表20来使用 cookie=0x0, duration=10969.2s, table=10, n_packets=29, n_bytes=5736, idle_age=16, priority=1 actions=learn(table=20,hard_timeout=300,priority=1,NXM_OF_VLAN_TCI[0..11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0->NXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]->NXM_NX_TUN_ID[],output:NXM_OF_IN_PORT[]),output:1 # 表20, 根据目的mac设置tun_id,通过指定的port发出去 cookie=0x0, duration=682.603s, table=20, n_packets=26, n_bytes=5266, hard_timeout=300, idle_age=16, hard_age=16, priority=1,vlan_tci=0x0001/0x0fff,dl_dst=fa:16:3e:32:0d:db actions=load:0->NXM_OF_VLAN_TCI[],load:0x1->NXM_NX_TUN_ID[],output:2 # 无规则的交给表21处理 cookie=0x0, duration=10969.057s, table=20, n_packets=0, n_bytes=0, idle_age=10969, priority=0 actions=resubmit(,21) # 表21，根据vlan找到对应的出去的口 cookie=0x0, duration=688.6s, table=21, n_packets=161, n_bytes=10818, idle_age=21, priority=1,dl_vlan=1 actions=strip_vlan,set_tunnel:0x1,output:2 # drop cookie=0x0, duration=10968.912s, table=21, n_packets=2, n_bytes=148, idle_age=689, priority=0 actions=drop 7、查询网桥信息 # ovs-ofctl show br-tun OFPT_FEATURES_REPLY (xid=0x2): dpid:000096d30367a84a n_tables:254, n_buffers:256 capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP actions: output enqueue set_vlan_vid set_vlan_pcp strip_vlan mod_dl_src mod_dl_dst mod_nw_src mod_nw_dst mod_nw_tos mod_tp_src mod_tp_dst 1(patch-int): addr:4e:cb:5f:17:d4:d6 config: 0 state: 0 speed: 0 Mbps now, 0 Mbps max 6(vxlan-5c00000b): addr:ca:48:f4:a1:7e:cb config: 0 state: 0 speed: 0 Mbps now, 0 Mbps max LOCAL(br-tun): addr:96:d3:03:67:a8:4a config: PORT_DOWN state: LINK_DOWN speed: 0 Mbps now, 0 Mbps max OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0 ovs-dpctl Datapath统计信息查询：hit表示datapath命中数，missed未命中，lost表示没有传递到用户空间就丢弃了 # ovs-dpctl show system@ovs-system: lookups: hit:99183 missed:37588 lost:1 flows: 2 masks: hit:231338 total:4 hit/pkt:1.69 port 0: ovs-system (internal) port 1: br-ex (internal) port 2: ens4 port 3: br-int (internal) port 4: br-tun (internal) port 5: qvo4fab3e51-fc port 6: vxlan_sys_4789 (vxlan) 查询端口详细统计信息 # ovs-dpctl show -s system@ovs-system: lookups: hit:99202 missed:37594 lost:1 flows: 5 masks: hit:231423 total:4 hit/pkt:1.69 port 0: ovs-system (internal) RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 aborted:0 carrier:0 collisions:0 RX bytes:0 TX bytes:0 port 1: br-ex (internal) RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:136729 aborted:0 carrier:0 collisions:0 RX bytes:0 TX bytes:0 port 2: ens4 RX packets:138249 errors:0 dropped:0 overruns:0 frame:0 TX packets:24986 errors:0 dropped:0 aborted:0 carrier:0 collisions:0 RX bytes:8046532 (7.7 MiB) TX bytes:1052004 (1.0 MiB) port 3: br-int (internal) RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:57 aborted:0 carrier:0 collisions:0 RX bytes:0 TX bytes:0 port 4: br-tun (internal) RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 aborted:0 carrier:0 collisions:0 RX bytes:0 TX bytes:0 port 5: qvo4fab3e51-fc RX packets:23 errors:0 dropped:0 overruns:0 frame:0 TX packets:8 errors:0 dropped:0 aborted:0 carrier:0 collisions:0 RX bytes:2364 (2.3 KiB) TX bytes:648 port 6: vxlan_sys_4789 (vxlan) RX packets:0 errors:? dropped:? overruns:? frame:? TX packets:0 errors:? dropped:? aborted:? carrier:? collisions:? RX bytes:0 TX bytes:0 查询指定端口统计信息 # ovs-ofctl dump-ports br-tun 6 OFPST_PORT reply (xid=0x2): 1 ports port 6: rx pkts=0, bytes=0, drop=?, errs=?, frame=?, over=?, crc=? tx pkts=0, bytes=0, drop=?, errs=?, coll=? ovs-appctl 查询网桥转发规则 # ovs-appctl fdb/show br-tun port VLAN MAC Age 调试 日志查询 ### 可使用 ps -ef|grep ovsdb-server 查询conf.db的具体路径 # ovsdb-tool show-log -m /var/lib/openvswitch/conf.db 端口抓包（方式一） ### 通过进入设备OVS端口所在的网络空间进行监听，例如监听br-tun的patch-int端口 # ip netns list # ip netns exec [NAME] bash # ip addr show # tcpdump -i [DEV] 端口抓包(方式二) ### 通过设置端口镜像来抓取没有具体设备的OVS端口，例如监听br-tun的patch-int端口 # ip link add name snooper0 type dummy # ip link set dev snooper0 up # ovs-vsctl add-port br-tun snooper0 # ovs-vsctl -- set Bridge br-tun mirrors=@m -- --id=@snooper0 get Port snooper0 -- --id=@patch-int get Port patch-int -- --id=@m create Mirror name=mymirror select-dst-port=@patch-int select-src-port=@patch-int output-port=@snooper0 select_all=1 # ovs-vsctl clear Bridge br-tun mirrors # ovs-vsctl del-port br-tun snooper0 # ip link delete dev snooper0 流表匹配 # ovs-appctl ofproto/trace br-tun dl_vlan=1 "},"notes/openstack/migrate_to_other_platform.html":{"url":"notes/openstack/migrate_to_other_platform.html","title":"迁移虚拟机到其他平台","keywords":"","body":"迁移虚拟机到其他平台 1、查看虚拟机基本信息 openstack server list --all-project|grep hl_cirros bf8f03d1-5e96-4f63-8348-5e703d2c8c01 | hl_cirros | ACTIVE | hl=192.168.10.7|| 1G/1核/1GB | 2、查看虚拟机详情 openstack server show bf8f03d1-5e96-4f63-8348-5e703d2c8c01 +-------------------------------------+----------------------------------------------------------+ | Field | Value | +-------------------------------------+----------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | nova | | OS-EXT-SRV-ATTR:host | tmp1 | | OS-EXT-SRV-ATTR:hypervisor_hostname | tmp1 | | OS-EXT-SRV-ATTR:instance_name | instance-000028ae | | OS-EXT-STS:power_state | Running | | OS-EXT-STS:task_state | None | | OS-EXT-STS:vm_state | active | | OS-SRV-USG:launched_at | 2019-01-10T13:40:35.000000 | | OS-SRV-USG:terminated_at | None | | accessIPv4 | | | accessIPv6 | | | addresses | hl=192.168.10.7 | | config_drive | | | created | 2019-01-10T13:40:20Z | | flavor | 1G/1核/1GB (03ee3258-8e54-4b4f-ba58-86250b98bbb4) | | hostId | 1ca96ff2970f3f6ac5baac71da801bac8021b4a27ed4c5f7eb284d78 | | id | bf8f03d1-5e96-4f63-8348-5e703d2c8c01 | | image | | | key_name | None | | name | hl_cirros | | progress | 0 | | project_id | 14f974575ad04ce4ae60790c1470d707 | | properties | image='8b7e3b42-c5f3-4d4c-b083-a0aeaaf7df86' | | security_groups | name='default' | | status | ACTIVE | | updated | 2019-01-10T13:40:35Z | | user_id | 8cf334fc34814d62acdaef2a7a862654 | | volumes_attached | id='b0db72fd-2440-4aba-bbf9-d768c966571a' | | | id='7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b' | +-------------------------------------+----------------------------------------------------------+ 3、查看磁盘信息 openstack volume list --all-project|grep hl_cirros |7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b|hl_cirros|in-use|1|Attached to bf8f03d1-5e96-4f63-8348-5e703d2c8c01 on /dev/vdb| 4、查看磁盘详情 openstack volume show 7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b +--------------------------------+------------------------------------------------------------------------------------------+ | attachments | [{u'server_id': u'bf8f03d1-5e96-4f63-8348-5e703d2c8c01', u'attachment_id': u'e91444a3-359e-4dc8-b53c-979af407e105', u'attached_at': u'2019-01-10T13:47:52.000000', u'host_name': u'tmp1', u'volume_id': u'7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b', u'device': u'/dev/vdb', u'id': u'7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b'}] | | availability_zone | nova | bootable | false | consistencygroup_id | None | created_at | 2019-01-10T13:47:30.000000 | description | None | encrypted | False | id | 7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b | migration_status | None | multiattach | False | name | hl_cirros | os-vol-host-attr:host | tmp2@rbd-1#rbd-1 | os-vol-mig-status-attr:migstat | None | os-vol-mig-status-attr:name_id | None | os-vol-tenant-attr:tenant_id | 14f974575ad04ce4ae60790c1470d707 | properties | attached_mode='rw' | replication_status | None | size | 1 | snapshot_id | None | source_volid | None | status | in-use | type | None | updated_at | 2019-01-10T13:47:52.000000 | user_id | 8cf334fc34814d62acdaef2a7a862654 openstack volume show b0db72fd-2440-4aba-bbf9-d768c966571a +--------------------------------+-----------------------------------------------------------------------------+ | attachments | [{u'server_id': u'bf8f03d1-5e96-4f63-8348-5e703d2c8c01', u'attachment_id': u'90c57b13-6e2a-407e-9e6c-8c9612a7cb6a', u'attached_at': u'2019-01-10T13:40:28.000000', u'host_name': None, u'volume_id': u'b0db72fd-2440-4aba-bbf9-d768c966571a', u'device': u'/dev/vda', u'id': u'b0db72fd-2440-4aba-bbf9-d768c966571a'}] | | availability_zone | nova | bootable | true | consistencygroup_id | None | created_at | 2019-01-10T13:40:24.000000 | description | | encrypted | False | id | b0db72fd-2440-4aba-bbf9-d768c966571a | migration_status | None | multiattach | False | name | | os-vol-host-attr:host | tmp1@rbd-1#rbd-1 | os-vol-mig-status-attr:migstat | None | os-vol-mig-status-attr:name_id | None | os-vol-tenant-attr:tenant_id | 14f974575ad04ce4ae60790c1470d707 | properties | attached_mode='rw' | replication_status | None | size | 1 | snapshot_id | None | source_volid | None | status | in-use | type | None | updated_at | 2019-01-10T13:40:28.000000 | user_id | 8cf334fc34814d62acdaef2a7a862654 | volume_image_metadata | {u'description': u'cirros', u'checksum': u'ba3cd24377dde5dfdd58728894004abb', u'min_ram': u'128', u'disk_format': u'raw', u'image_name': u'cirros', u'image_id': u'8b7e3b42-c5f3-4d4c-b083-a0aeaaf7df86', u'container_format': u'bare', u'min_disk': u'1', u'os_type': u'windows', u'size': u'46137344'} | +--------------------------------+-------------------------------------------------+ 5、查看ceph存储池 (ceph-mon)[root@tmp1 opt]# rados lspools .rgw.root default.rgw.control default.rgw.meta default.rgw.log images volumes backups vms gnocchi default.rgw.buckets.index default.rgw.buckets.data default.rgw.buckets.non-ec c_bak 6、查看 vms pool rbd -p vms ls 7、查看 volumes pool rbd -p volumes ls|grep b0db72fd-2440-4aba-bbf9-d768c966571a volume-b0db72fd-2440-4aba-bbf9-d768c966571a rbd -p volumes ls|grep 7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b volume-7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b 8、导出volumes rbd rbd export volumes/volume-b0db72fd-2440-4aba-bbf9-d768c966571a /opt/volume-b0db72fd-2440-4aba-bbf9-d768c966571a rbd export volumes/volume-7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b /opt/volume-7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b 9、重新导入volumes rbd rbd import /opt/volume-b0db72fd-2440-4aba-bbf9-d768c966571a volumes/volume-b0db72fd-2440-4aba-bbf9-d768c966571a --image-format 2 rbd import /opt/volume-7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b volumes/volume-7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b --image-format 2 10、查看虚拟机xml配置文件 virsh # dumpxml instance-000028ae instance-000028ae bf8f03d1-5e96-4f63-8348-5e703d2c8c01 hl_cirros 2019-01-10 13:40:29 1024 1 0 0 1 admin hl 1048576 1048576 1 1024 /machine OpenStack Foundation OpenStack Nova 17.0.2 4c4c4544-004a-5410-8056-b9c04f4c5032 bf8f03d1-5e96-4f63-8348-5e703d2c8c01 Virtual Machine hvm Broadwell destroy restart destroy /usr/libexec/qemu-kvm b0db72fd-2440-4aba-bbf9-d768c966571a ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ 7b7b6f1b-56dc-4502-8e13-9bcfa3a7168b ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ +42436:+42436 ​ +42436:+42436 11、查看虚拟机网络信息 openstack network list|grep hl | 1221dbf4-fb34-4517-8f2e-738a14070271 | hl | fd08b9a6-5429-4015-a8ad-02df4c4103b0 | 12、查看网络子网信息 openstack subnet list |grep fd08b9a6-5429-4015-a8ad-02df4c4103b0 | fd08b9a6-5429-4015-a8ad-02df4c4103b0 | tmp-subnet-1221dbf4-fb34-4517-8f2e-738a14070271 | 1221dbf4-fb34-4517-8f2e-738a14070271 | 192.168.10.0/24 | 13、查看子网详情 openstack subnet show fd08b9a6-5429-4015-a8ad-02df4c4103b0 +-------------------+-----------------------------------------------------+ | Field | Value | +-------------------+-----------------------------------------------------+ | allocation_pools | 192.168.10.2-192.168.10.254 | | cidr | 192.168.10.0/24 | | created_at | 2019-01-04T03:24:05Z | | description | | | dns_nameservers | 114.114.114.114 | | enable_dhcp | True | | gateway_ip | 192.168.10.1 | | host_routes | | | id | fd08b9a6-5429-4015-a8ad-02df4c4103b0 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | name | tmp-subnet-1221dbf4-fb34-4517-8f2e-738a14070271 | | network_id | 1221dbf4-fb34-4517-8f2e-738a14070271 | | project_id | 14f974575ad04ce4ae60790c1470d707 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2019-01-04T03:24:05Z | +-------------------+-----------------------------------------------------+ 14、查看网络详情 openstack network show 1221dbf4-fb34-4517-8f2e-738a14070271 +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | nova | | created_at | 2019-01-04T03:24:04Z | | description | | | dns_domain | None | | id | 1221dbf4-fb34-4517-8f2e-738a14070271 | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | None | | is_vlan_transparent | None | | mtu | 1450 | | name | hl | | port_security_enabled | True | | project_id | 14f974575ad04ce4ae60790c1470d707 | | provider:network_type | vxlan | | provider:physical_network | None | | provider:segmentation_id | 80 | | qos_policy_id | None | | revision_number | 4 | | router:external | Internal | | segments | None | | shared | False | | status | ACTIVE | | subnets | fd08b9a6-5429-4015-a8ad-02df4c4103b0 | | tags | | | updated_at | 2019-01-04T03:24:05Z | +---------------------------+--------------------------------------+ 15、创建flavor openstack flavor create --id 0 --vcpus 1 --ram 1024 --disk 1 111 16、创建安全组 openstack security group rule create --proto icmp default openstack security group rule create --proto tcp --dst-port 22 default 17、创建网络 openstack network create hl --provider-network-type vxlan openstack subnet create hl-subnet --network hl --subnet-range 192.168.10.0/24 openstack network create --external --provider-physical-network physnet1 \\ --provider-network-type flat public1 openstack subnet create --no-dhcp \\ --allocation-pool start=192.168.21.120,end=192.168.21.130 --network public1 \\ --subnet-range 192.168.21.0/24 --gateway 192.168.21.1 public1-subnet openstack network create --provider-network-type vxlan --share --enable demo-net openstack subnet create --subnet-range 10.0.0.0/24 --network demo-net \\ --gateway 10.0.0.1 --dns-nameserver 114.114.114.114 demo-subnet openstack router create demo-router openstack router add subnet demo-router demo-subnet openstack router set --external-gateway public1 demo-router 18、创建虚拟机 nova boot --flavor 0 --image 796cabf0-6ac1-4520-ad5c-c726fa370d9d --nic auto hl https://ask.openstack.org/en/question/27156/how-to-customize-libvirtxml-for-an-instance/ 19、查看数据库中虚拟机信息 MariaDB [nova]> select*from block_device_mapping where instance_uuid='a168e661-73d5-4c9b-ba00-7c16557f7fe8'; +---------------------+---------------------+------------+-------+-------------+-----------------------+-------------+-----------+-------------+-----------+-----------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+--------------------------------------+------+---------------+--------------------------------------+ | created_at | updated_at | deleted_at | id | device_name | delete_on_termination | snapshot_id | volume_id | volume_size | no_device | connection_info | instance_uuid | deleted | source_type | destination_type | guest_format | device_type | disk_bus | boot_index | image_id | tag | attachment_id | uuid | +---------------------+---------------------+------------+-------+-------------+-----------------------+-------------+-----------+-------------+-----------+-----------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+--------------------------------------+------+---------------+--------------------------------------+ | 2019-01-15 08:23:48 | 2019-01-15 08:23:48 | NULL | 51874 | /dev/vda | 1 | NULL | NULL | NULL | 0 | NULL | a168e661-73d5-4c9b-ba00-7c16557f7fe8 | 0 | image | local | NULL | disk | NULL | 0 | 8b7e3b42-c5f3-4d4c-b083-a0aeaaf7df86 | NULL | NULL | 2febe1b0-9730-4c4a-9a73-34f9bccffb01 | +---------------------+---------------------+------------+-------+-------------+-----------------------+-------------+-----------+-------------+-----------+-----------------+--------------------------------------+---------+-------------+------------------+--------------+-------------+----------+------------+--------------------------------------+------+---------------+--------------------------------------+ 1 row in set (0.00 sec) 20、创建虚拟机 nova boot --flavor 03ee3258-8e54-4b4f-ba58-86250b98bbb4 --image 8b7e3b42-c5f3-4d4c-b083-a0aeaaf7df86 --nic auto hl nova boot --flavor 0 --image 796cabf0-6ac1-4520-ad5c-c726fa370d9d --nic auto hl "},"notes/openstack/openstack_hygon_patch.html":{"url":"notes/openstack/openstack_hygon_patch.html","title":"OpenStack对hygon的兼容","keywords":"","body":"OpenStack对hygon的兼容 一、为什么要对OpenStack进行配置 海光处理器采用了与AMD EPYC 类似的体系结构，为了便于 QEMU 虚拟化处理器提供更好的兼容模式，需要对 QEMU-KVM进行CPU Vendor ID进行替换，进而支持 QEMU 虚拟机虚拟化 ； QEMU在使用KVM虚拟化的时候只支持使用主机的CPU Vendor ID,目前海光处理器还不在此KVM虚拟化版本支持列表中，如果不配置就直接使用 QEMU/KVM，那么虚拟出来的虚拟机也为海光的 CPU VendorID，这将会导致某些OS虚拟机无法正常启动（例如Windows）； Libvirt调用QEMU创建虚拟机进程，在配置Libvirt XML文件的时候需要指定CPU Model和VendorID，把这些参数传递给QEMU，虚拟机才能正常启动。 Openstack NOVA组件调用Libvirt接口控制虚拟机的生命周期，创建虚拟机的时候生成XML文件，原版的Openstack不会传递CPU的VendorID，需要做一些修改，传递相关的CPU参数才能生成正确的XML文件。 二、配置方法 1、给nova_compute打补丁，将补丁文件Hygon_OpenStack_Train.patch拷贝到/usr/lib/python2.7/site-packages/nova目录 cd /var/lib/kolla/venv/lib/python2.7/site-packages/nova 2、应用补丁 补丁地址 git clone https://github.com/hlyani/openstack_hygon_patch.git patch -p1 3、重新nova-compute服务 systemctl restart openstack-nova-compute 4、先查看cpu型号 # lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 32 On-line CPU(s) list: 0-31 Thread(s) per core: 2 Core(s) per socket: 16 Socket(s): 1 NUMA node(s): 4 Vendor ID: HygonGenuine CPU family: 24 Model: 0 Model name: Hygon C86 7151 16-core Processor Stepping: 1 CPU MHz: 1200.000 CPU max MHz: 2000.0000 CPU min MHz: 1200.0000 BogoMIPS: 3999.77 Virtualization: AMD-V L1d cache: 32K L1i cache: 64K L2 cache: 512K L3 cache: 4096K NUMA node0 CPU(s): 0-3,16-19 NUMA node1 CPU(s): 4-7,20-23 NUMA node2 CPU(s): 8-11,24-27 NUMA node3 CPU(s): 12-15,28-31 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc art rep_good nopl nonstop_tsc extd_apicid amd_dcm aperfmperf eagerfpu pni monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_l2 hw_pstate retpoline_amd ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca 5、创建新的flavor nova flavor-create 4-8192-40-hygon 1000 8192 40 4 # win10内核的一个bug，win10对EPYC的支持有问题，hw:cpu_name='EPYC' nova flavor-key 4-8192-40-hygon set hw:cpu_name='phenom' nova flavor-key 4-8192-40-hygon set hw:cpu_vendor='AuthenticAMD' nova flavor-key 4-8192-40-hygon set hw:cpu_model_id='Hygon C86 7151 16-core Processor' nova flavor-key 4-8192-40-hygon set hw:cpu_sockets=2 nova flavor-key 4-8192-40-hygon set hw:cpu_cores=2 6、基于新创的flavor创建虚拟机即可解决win10蓝屏问题 "},"notes/openstack/lsf_install_openstack.html":{"url":"notes/openstack/lsf_install_openstack.html","title":"Lfs上安装OpenStack","keywords":"","body":"Lfs上安装OpenStack 一、环境准备 1、更新内核 支持ip6tables ip6tables-restore v1.4.21: ip6tables-restore: unable to initialize table 'filter' Error occurred at line: 2 Try `ip6tables-restore -h' or 'ip6tables-restore --help' for more information. CONFIG_NF_TABLES=m CONFIG_NF_TABLES_INET=m CONFIG_IP_NF_SECURITY=m CONFIG_IP6_NF_SECURITY=m 支持ovs TASK [module-load : Load modules] * failed: [localhost] (item=openvswitch) => {\"ansible_loop_var\": \"item\", \"changed\": false, \"item\": {\"name\": \"openvswitch\"}, \"msg\": \"modprobe: FATAL: Module openvswitch not found in directory /lib/modules/4.19.37-rt19\\n\", \"name\": \"openvswitch\", \"params\": \"\", \"rc\": 1, \"state\": \"present\", \"stderr\": \"modprobe: FATAL: Module openvswitch not found in directory /lib/modules/4.19.37-rt19\\n\", \"stderr_lines\": [\"modprobe: FATAL: Module openvswitch not found in directory /lib/modules/4.19.37-rt19\"], \"stdout\": \"\", \"stdout_lines\": []} 更新内核 rm -rf /lib/modules/* tar -zxvf 4.19.37-rt19.tar.gz -C /lib/modules/ cp vmlinuz-4.19.37-rt19 /boot/vmlinuz-4.19.37-rt19 reboot 2、安装python3.8 a、安装 #yum install gcc openssl-devel bzip2-devel libffi-devel wget https://www.python.org/ftp/python/3.8.2/Python-3.8.2.tgz tar xzf Python-3.8.2.tgz cd Python-3.8.2 ./configure --enable-optimizations make altinstall b、安装虚拟环境 /usr/local/bin/python3.8 -m venv /root/venv38 source /root/venv38/bin/activate 3、配置网络（两张网卡） vim ifconfig.enp1s0f0 ONBOOT=yes IFACE=enp1s0f0 STP=yes VIRTINT=yes CHECK_LINK=no PREFIX=24 IP_FORWARD=true INTERFACE_COMPONENTS=enp1s0f0 IP=192.168.0.34 GATEWAY=192.168.0.1 BROADCAST=192.168.0.255 SERVICE=\"ipv4-static\" vim ifconfig.enp1s0f1 ONBOOT=yes IFACE=enp1s0f1 STP=yes VIRTINT=yes CHECK_LINK=no PREFIX=24 IP_FORWARD=true INTERFACE_COMPONENTS=enp1s0f1 SERVICE=\"ipv4-static\" 4、pip源配置 mkdir -p /root/.pip/ vim /root/.pip/pip.conf [global] index-url = https://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com 5、准备cinder nfs卷 mkdir /kolla_nfs mkfs.ext4 /dev/sdb vim /etc/fstab /dev/sdb /kolla_nfs/ ext4 defaults 0 0 mount -a #yum install -y nfs-utils vim /etc/exports /kolla_nfs 192.168.5.0/24(rw,sync,no_root_squash) #systemctl restart nfs vim /etc/kolla/config/nfs_shares node1:/kolla_nfs node2:/kolla_nfs # lvm #pvcreate /dev/sdb #vgcreate cinder-volumes /dev/sdb #vim /etc/kolla/globals.yml #enable_cinder: \"yes\" #enable_cinder_backend_lvm: \"yes\" #cinder_volume_group: \"cinder-volumes\" 6、ansible 优化 vim /etc/ansible/ansible.cfg [defaults] host_key_checking=False pipelining=True forks=100 7、拷贝其他主机上的/etc/modules-load.d 文件夹 scp -r 192.168.0.30:/etc/modules-load.d/ /etc/ 8、启动docker mkdir -p /var/lib/nova/mnt /var/lib/nova/mnt1 mount --bind /var/lib/nova/mnt1 /var/lib/nova/mnt mount --make-shared /var/lib/nova/mnt mount --make-shared /run /etc/cgroupfs-mount.sh dockerd & 9、docker load 相关镜像 (venv38) root [ ~ ]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE kolla/centos-source-nova-compute train 5b0613547d7c 8 days ago 1.89GB kolla/centos-source-cinder-volume train 7b72b6446cdc 8 days ago 1.56GB kolla/centos-source-neutron-server train 431c793225c7 8 days ago 1.02GB kolla/centos-source-neutron-openvswitch-agent train 19b7ac330af6 8 days ago 1GB kolla/centos-source-cinder-api train b49c2a941c36 8 days ago 1.09GB kolla/centos-source-neutron-l3-agent train 7cda11cc0053 8 days ago 1.04GB kolla/centos-source-neutron-metadata-agent train abc6e7247cec 8 days ago 1GB kolla/centos-source-neutron-dhcp-agent train 003ca47a41f4 8 days ago 1GB kolla/centos-source-nova-api train b79a7994ba77 8 days ago 1.08GB kolla/centos-source-cinder-scheduler train d626a789ffdf 8 days ago 1.02GB kolla/centos-source-nova-novncproxy train 1fcacbbd1017 8 days ago 1.06GB kolla/centos-source-nova-conductor train 09158e7ee9f5 8 days ago 1.02GB kolla/centos-source-nova-scheduler train 2698f00947df 8 days ago 1.02GB kolla/centos-source-glance-api train 09e781e18202 8 days ago 951MB kolla/centos-source-horizon train 5e434948e0a4 8 days ago 1.03GB kolla/centos-source-placement-api train e67f15a6515e 8 days ago 921MB kolla/centos-source-keystone train 60d1ef5e4b57 8 days ago 919MB kolla/centos-source-keystone-fernet train daa9ade1ad37 8 days ago 920MB kolla/centos-source-keystone-ssh train 65f2184003de 8 days ago 921MB kolla/centos-source-openvswitch-vswitchd train 03947aba6136 8 days ago 428MB kolla/centos-source-openvswitch-db-server train 90bdff01909b 8 days ago 428MB kolla/centos-source-kolla-toolbox train 5bad6e6ae2f2 8 days ago 833MB kolla/centos-source-nova-libvirt train ea7102c16951 8 days ago 1.26GB kolla/centos-source-memcached train 2f5c7c833559 8 days ago 410MB kolla/centos-source-fluentd train efe54c6b7b37 8 days ago 667MB kolla/centos-source-mariadb train ce32f151ffcd 8 days ago 594MB kolla/centos-source-rabbitmq train cd84314358ba 8 days ago 489MB kolla/centos-source-cron train 7624bb53fa55 8 days ago 409MB 二、安装 1、修改hosts、hostname vim /etc/hosts 192.168.0.34 node1 vim /etc/hostname node1 2、安装ansible pip install ansible 3、获取kolla-ansible源码 可以从其他环境拉取下来，安装好，把venv环境打包到lfs环境中 git clone https://github.com/openstack/kolla-ansible.git cd kolla-ansible/ git checkout stable/train pip install -r requirements.txt 4、修改kolla-ansible a、修改内核模块路径 vim /root/venv38/lib/python3.8/site-packages/ansible/modules/system/modprobe.py #builtin_path = os.path.join('/lib/modules/', uname_kernel_release.strip(), builtin_path = os.path.join('/lib/modules/', '/lib/modules/4.19.37-rt19', b、修改ansible路径 (venv38) root [ ~ ]# kolla-ansible --help /root/venv38/bin/kolla-ansible: line 7: which: command not found ERROR: Ansible is not installed in the current (virtual) environment. vim /root/venv38/bin/kolla-ansible ansible_path=/root/venv38/bin/ansible c、prechecks报错 vim kolla-ansible/ansible/roles/prechecks/vars/main.yml Lfs: - \"2\" 5、kvm权限问题 部署完后进入nova_libvirt容器 virt-host-validate virsh capabilities | grep domain cat /var/cache/libvirt/qemu/capabilities/ cat /usr/share/libvirt/cpu_map.xml dmesg docker logs nova_libvirt 容器内libvirt debug信息 2020-07-23 06:48:34.871+0000: 46: debug : virFileCacheValidate:289 : Creating data for '/usr/libexec/qemu-kvm' 2020-07-23 06:48:34.872+0000: 46: debug : virFileMakePathHelper:3093 : path=/var/cache/libvirt/qemu/capabilities mode=0777 2020-07-23 06:48:34.872+0000: 46: debug : virFileMakePathHelper:3093 : path=/var/cache/libvirt/qemu mode=0777 2020-07-23 06:48:34.872+0000: 46: debug : virFileCacheLoad:149 : No cached data '/var/cache/libvirt/qemu/capabilities/3c76bc41d59c0c7314b1ae8e63f4f765d2cf16abaeea081b3ca1f5d8732f7bb1.xml' for '/usr/libexec/qemu-kvm' 2020-07-23 06:48:34.872+0000: 46: debug : virFileClose:111 : Closed fd 20 2020-07-23 06:48:34.872+0000: 46: info : virObjectNew:248 : OBJECT_NEW: obj=0x7f5ff00f5570 classname=virQEMUCaps 2020-07-23 06:48:34.872+0000: 46: debug : virQEMUCapsInitQMPCommandRun:4400 : Try to probe capabilities of '/usr/libexec/qemu-kvm' via QMP, machine none,accel=kvm:tcg 2020-07-23 06:48:34.872+0000: 46: debug : virCommandRunAsync:2585 : About to run LC_ALL=C PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOME=/root /usr/libexec/qemu-kvm -S -no-user-config -nodefaults -nographic -machine none,accel=kvm:tcg -qmp unix:/var/lib/libvirt/qemu/capabilities.monitor.sock,server,nowait -pidfile /var/lib/libvirt/qemu/capabilities.pidfile -daemonize 2020-07-23 06:48:34.873+0000: 46: debug : virFileClose:111 : Closed fd 20 2020-07-23 06:48:34.873+0000: 46: debug : virFileClose:111 : Closed fd 23 2020-07-23 06:48:34.873+0000: 46: debug : virFileClose:111 : Closed fd 25 2020-07-23 06:48:34.873+0000: 46: debug : virCommandRunAsync:2588 : Command result 0, with PID 57 2020-07-23 06:48:34.960+0000: 46: debug : virCommandRun:2436 : Result exit status 0, stdout: '' stderr: '2020-07-23 06:48:34.873+0000: 57: debug : virFileClose:111 : Closed fd 23 2020-07-23 06:48:34.873+0000: 57: debug : virFileClose:111 : Closed fd 25 2020-07-23 06:48:34.873+0000: 57: debug : virFileClose:111 : Closed fd 20 2020-07-23 06:48:34.873+0000: 57: debug : virExecCommon:474 : Setting child uid:gid to 42427:42427 with caps 0 Could not access KVM kernel module: Permission denied qemu-kvm: failed to initialize KVM: Permission denied qemu-kvm: Back to tcg accelerator vim /usr/lib/udev/rules.d/80-kvm.rules KERNEL==\"kvm\", GROUP=\"kvm\", MODE=\"0666\", OPTIONS+=\"static_node=kvm\" vim /lib/udev/rules.d/65-kvm.rules KERNEL==\"kvm\", GROUP=\"kvm\", MODE=\"0666\" 6、修改/etc/kolla/globals.yml文件 cp -r /root/venv38/share/kolla-ansible/etc_examples/kolla/ /etc/ kolla_base_distro: \"centos\" kolla_install_type: \"source\" openstack_release: \"train\" openstack_tag: \"{{ openstack_release }}\" kolla_internal_vip_address: \"192.168.0.34\" network_interface: \"eno2\" neutron_external_interface: \"eno3\" keepalived_virtual_router_id: \"34\" enable_haproxy: \"no\" enable_chrony: \"no\" enable_cinder: \"yes\" enable_cinder_backup: \"no\" enable_cinder_backend_nfs: \"yes\" enable_heat: \"no\" enable_nova_ssh: \"no\" external_ceph_cephx_enabled: \"no\" glance_backend_file: \"yes\" cinder_volume_group: \"cinder-volumes\" nova_compute_virt_type: \"kvm\" memcached_dimensions: ulimits: nofile: soft: 98304 hard: 98304 memecached 问题 exec /usr/bin/memcached -v -l 192.168.0.34 -p 11211 -c 5000 -U 0 -m 256 failed to set rlimit for open files. Try starting as root or requesting smaller maxconns value. vim /etc/kolla/globals.yml memcached_dimensions: ulimits: nofile: soft: 98304 hard: 98304 7、修改all-in-one文件 cp /root/venv38/share/kolla-ansible/ansible/inventory/* /root/ vim all-in-one [control] node1 ansible_connection=local ansible_python_interpreter=/root/venv38/bin/python3 [network] node1 ansible_connection=local ansible_python_interpreter=/root/venv38/bin/python3 [compute] node1 ansible_connection=local ansible_python_interpreter=/root/venv38/bin/python3 [storage] node1 ansible_connection=local ansible_python_interpreter=/root/venv38/bin/python3 [monitoring] node1 ansible_connection=local ansible_python_interpreter=/root/venv38/bin/python3 [deployment] node1 ansible_connection=local ansible_python_interpreter=/root/venv38/bin/python3 8、安装 kolla-ansible -i /root/all-in-one deploy 9、卸载 kolla-ansible -i /root/all-in-one destroy --yes-i-really-really-mean-it "},"notes/linux/alpine.html":{"url":"notes/linux/alpine.html","title":"alpine","keywords":"","body":"alpine https://hub.docker.com/_/alpine docker pull alpine:3.20.2 docker pull python:alpine3.20 docker pull golang:alpine3.20 FROM alpine:3.14 RUN apk add --no-cache mysql-client ENTRYPOINT [\"mysql\"] "},"notes/linux/redis.html":{"url":"notes/linux/redis.html","title":"redis","keywords":"","body":"Redis 一、高可用部署 redis-stack-sentinel SENTINEL get-master-addr-by-name mymaster SENTINEL masters SENTINEL replicas mymaster SENTINEL FAILOVER mymaster SENTINEL slaves mymaster INFO SENTINEL ACL LIST 普通认证 √ # redis requirepass qwe masterauth qwe # sentinel sentinel auth-user mymaster default sentinel auth-pass mymaster qwe acl 认证 × 未验证通过 # redis user sentinel on >{{ .Values.auth.password }} +client +subscribe +publish +ping +info +multi +slaveof +config +exec # sentinel sentinel auth-user mymaster sentinel sentinel auth-pass mymaster qwe 二、基础使用 连接 redis-cli -h 127.0.0.1 -p 6379 --user redis -a infini_rag_flow_helm ping REDISCLI_AUTH=qwe redis-cli --user default -p 6379 ping redis-cli 127.0.0.1:6379> auth infini_rag_flow_helm OK 键值操作 SET key \"hello\" # 设置 key GET key # 获取 key DEL key # 删除 key EXPIRE key 10 # 设置 key 10 秒后过期 主要类型 string → 代表是普通字符串（可用 GET 获取）。 list → 代表是列表（可用 LRANGE 获取）。 set → 代表是集合（可用 SMEMBERS 获取）。 hash → 代表是哈希表（可用 HGETALL 获取）。 zset → 代表是有序集合（可用 ZRANGE 获取）。 ZRANGE task_consumer_0 0 -1 # zset LRANGE task_consumer_0 0 -1 # list SMEMBERS task_consumer_0 # set HGETALL task_consumer_0 # hash XREVRANGE rag_flow_svr_queue + - COUNT 5 # stream 最新 5 条消息 XREAD COUNT 10 STREAMS rag_flow_svr_queue 0 XREAD BLOCK 0 STREAMS rag_flow_svr_queue $ # 持续监听新消息 操作 命令 查看 Stream 长度 XLEN rag_flow_svr_queue 查看所有消息 XRANGE rag_flow_svr_queue - + 查看最新的 N 条消息 XREVRANGE rag_flow_svr_queue + - COUNT N 监听 Stream 消息 XREAD BLOCK 0 STREAMS rag_flow_svr_queue $ 读取并标记已消费 XREADGROUP GROUP mygroup myconsumer COUNT 1 STREAMS rag_flow_svr_queue > 删除特定消息 XDEL rag_flow_svr_queue 清理旧消息 XTRIM rag_flow_svr_queue MAXLEN 1000 三、常用命令 1.基础命令 命令 说明 ping 检查 Redis 是否在线，返回 PONG auth 登录认证（如果设置了密码） select 选择数据库（默认是 0） flushdb 清空当前数据库 flushall 清空所有数据库 keys 查找符合模式的 key（不建议用于生产环境） keys * exists 判断 key 是否存在 del 删除指定 key expire 为 key 设置过期时间（单位：秒） ttl 查看 key 剩余过期时间（-1: 永不过期；-2: 不存在） type 查看 key 的类型 select 1 切换到第 1 个数据库 scan 0 使用游标非阻塞遍历所有 key，推荐用于生产环境 SCAN 0 MATCH he* 遍历匹配以 he 开头的 key SCAN 0 MATCH user:* COUNT 100 每次返回最多 100 条匹配 user:* 的 key dbsize 查看一共多少key info 显示 Redis 的所有运行信息 Info memory 显示 Redis 内存使用详情 info stats 查看命中率、命令处理量等统计信息 info commandstats 查看各命令的调用次数和耗时 monitor 实时打印所有操作命令（调试用，慎用） slowlog get 查看慢查询日志 config get 查看配置参数值，例如 config get maxmemory config set 动态修改 Redis 配置，例如 config set maxmemory 512mb redis-cli --scan --pattern \"*\" 简化的 scan 语法，便于在 shell 使用（默认使用游标） redis-cli -n 1 使用 redis-cli 操作第 1 个数据库 FT._LIST 查看索引 FT.INFO checkpoints 可查看每个索引的详细结构与配置 2.字符串(String) 命令 说明 set 设置字符串 get 获取字符串 incr 自增（整数） decr 自减（整数） append 追加字符串 mset k1 v1 k2 v2 批量设置多个键值 mget k1 k2 批量获取多个值 3.哈希(Hash) 命令 说明 示例 hset 设置 hash 字段 HSET user:1 name \"Alice\" age 25 hget 获取字段值 HGET user:1 name hgetall 获取所有字段和值 HGETALL user:1 hdel 删除字段 hexists 检查字段是否存在 hmget f1 f2 获取多个字段值 hmset f1 v1 f2 v2 批量设置多个字段 4.列表(List) 命令 说明 lpush 左侧插入 rpush 右侧插入 lpop 左侧弹出 rpop 右侧弹出 lrange 0 -1 获取所有元素 llen 获取列表长度 5.集合(Set) 命令 说明 示例 sadd 添加元素 SADD myset \"a\" \"b\" \"c\" srem 移除元素 smembers 获取所有成员 SMEMBERS myset sismember 判断成员是否存在 SISMEMBER myset \"a\" scard 获取集合元素数量 6.有序集合(Sorted Set) 命令 说明 示例 zadd 添加元素及分数 ZADD scores 100 \"Alice\" 200 \"Bob\" zrem 删除成员 zrange 0 -1 获取所有成员（按分数升序） ZRANGE scores 0 -1 WITHSCORES zrevrange 0 -1 获取所有成员（按分数降序） zscore 获取成员分数 7.发布订阅(Pub/Sub) 命令 说明 publish 向频道发送消息 subscribe 订阅频道 unsubscribe 取消订阅 8.服务器相关 命令 说明 info 查看服务器信息 monitor 实时查看所有请求（调试用） config get 获取配置项 config set 动态修改配置项 save 同步保存快照到磁盘 bgsave 异步保存快照 lastsave 上一次成功保存时间戳 shutdown 停止 Redis 服务 9.RedisJSON 基础操作 命令 说明 JSON.SET 设置 JSON 数据，path 通常是 $（表示根） JSON.GET [path] 获取 JSON 数据，默认获取全部 JSON.DEL [path] 删除 JSON 的某个路径 JSON.TYPE [path] 获取指定路径的 JSON 类型 JSON.MGET ... 批量获取多个 key 的某个路径数据 修改操作 命令 说明 JSON.NUMINCRBY JSON 中数字值递增 JSON.STRAPPEND 向 JSON 字符串追加内容 JSON.ARRAPPEND 向 JSON 数组追加元素 JSON.ARRPOP [index] 弹出 JSON 数组中的元素 JSON.ARRINSERT 在数组指定位置插入元素 JSON.ARRLEN 获取 JSON 数组长度 JSON.ARRINDEX 获取元素在数组中的下标 JSON.CLEAR [path] 清空某个 JSON 路径下的值（保留结构） $ 表示根节点 # 设置一个 JSON 对象 JSON.SET user:1 $ '{\"name\": \"Alice\", \"age\": 30, \"tags\": [\"dev\", \"redis\"]}' # 获取整个对象 JSON.GET user:1 # 获取某个字段 JSON.GET user:1 $.name # 数组追加 JSON.ARRAPPEND user:1 $.tags '\"admin\"' # 删除某字段 JSON.DEL user:1 $.age 四、其他 # 直接搜索 checkpoints 索引中 @thread_id 为 1943868169984831490 的文档，返回全部字段和匹配结果。 FT.SEARCH checkpoints \"@thread_id:{1943868169984831490}\" # 执行搜索但不返回任何文档，只返回匹配的文档数量 FT.SEARCH checkpoints \"@thread_id:{1943840008152707073}\" LIMIT 0 0 # 搜索并仅返回 1 个匹配的文档（只返回 document id，不返回字段内容） FT.SEARCH checkpoints \"@thread_id:{1943840008152707073}\" LIMIT 0 1 RETURN 0 import redis # 连接 Redis r = redis.Redis(host='localhost', port=6379, decode_responses=True) # 执行 FT.SEARCH 查询（thread_id 是 TAG 类型，需要加 {}） query = '@thread_id:{1943868169984831490}' result = r.execute_command('FT.SEARCH', 'checkpoints', query) # 模糊查询 json key 所占用内存和每个key 占用内存 redis-cli --scan --pattern \"checkpoint:*\" | while read key; do redis-cli MEMORY USAGE \"$key\" done | awk ' {sum += $1; count += 1} END { printf \"Total Keys: %d\\n\", count; printf \"Total Memory: %.2f MB\\n\", sum / 1024 / 1024; printf \"Average per Key: %.2f KB\\n\", sum / count / 1024; }' 五、优化 redis配置 # Redis 监听端口 port 6379 dir /tmp # 监听所有网卡（用于 Kubernetes 或外部访问） bind 0.0.0.0 # 启用 AOF 持久化机制（适合数据安全性要求高的业务） appendonly yes appendfsync no no-appendfsync-on-rewrite yes # 设置最大客户端连接数，防止连接耗尽资源 maxclients 10000 # 启用 TCP keepalive，单位为秒（建议开启以发现死连接） tcp-keepalive 60 tcp-backlog 10240 timeout 300 hz 100 # 禁用 protected mode，适用于内网或 Kubernetes 环境 protected-mode no # 限制 Redis 最大内存，防止系统 OOM（单位可为 kb, mb, gb） maxmemory 8gb # 内存淘汰策略：从所有键中优先淘汰最少使用的键（LRU） maxmemory-policy allkeys-lru # 启用主动碎片整理（默认关闭，建议开启以优化长期运行性能） activedefrag yes # Redis 7.x 的碎片整理参数（默认值即可，如需调优可参考以下） # 当碎片率超过该值时触发整理 active-defrag-threshold-lower 10 # 达到更高碎片率后提升整理频率 active-defrag-threshold-upper 100 # 最小整理周期占 CPU 百分比 active-defrag-cycle-min 25 # 最大整理周期占 CPU 百分比 active-defrag-cycle-max 75 repl-backlog-size 128mb client-output-buffer-limit normal 0 0 0 client-output-buffer-limit pubsub 32mb 8mb 60 # 设置密码（强烈建议设置强密码以保障安全） requirepass yourStrongPasswordHere # 设置数据库数量（默认 16；如业务只需一个，可设为 1 降低开销） databases 1 # ========================= # 🧑 ACL 用户权限配置 # ========================= # 默认用户：允许所有命令和键 user default on >strongDefaultPass allcommands allkeys # 只读用户（readonly）：只能读不能写 user readonly on >readonlyPass +@read -@write ~* # 只写用户（writer）：只能写不能读 user writer on >writerPass +@write -@read ~* # 主节点设置访问密码 requirepass yourStrongPasswordHere # 从节点设置 masterauth，用于向主节点认证 masterauth yourStrongPasswordHere # 从节点向主节点请求完整的 RDB 快照（全量同步）； # 然后持续执行命令传播（增量同步）以保持数据一致性。 replicaof redis-0.redis-headless 6379 ACL SETUSER default on >yourStrongPassword allcommands allkeys sentinel配置 port 26379 dir /tmp # Sentinel 监控主节点（mymaster 是主集群名称） sentinel monitor mymaster redis-0.redis-headless 6379 2 # 设置主节点密码（和 Redis 的 requirepass 保持一致） # 设置用于连接主/从 Redis 的密码 sentinel auth-pass mymaster {{ .Values.redis.password }} # 判断主节点“故障”需要 5 秒没有回应 sentinel down-after-milliseconds mymaster 6000 # 故障转移最多等待 10 秒 sentinel failover-timeout mymaster 10000 # 故障转移时最多同时对几个从节点同步新主节点数据 sentinel parallel-syncs mymaster 1 # 防止错误选主（主从未完全同步前不选） sentinel deny-scripts-reconfig yes # 控制 Redis 实例在 无密码保护时是否限制访问。 protected-mode no # 启用后，Redis Sentinel 将动态解析主节点或从节点的主机名（DNS 名称），而不是只解析一次并缓存 IP 地址。 sentinel resolve-hostnames yes 六、测试 1.功能测试 redis-cli ping # 应该返回 PONG redis-cli set test_key 123 # 设置一个 key redis-cli get test_key # 应该返回 123 redis-cli --raw JSON.SET doc $ '{\"name\":\"Alice\",\"age\":30}' redis-cli --raw JSON.GET doc redis-cli --raw JSON.DEL doc 或使用 redisinsight 工具图形化查看 JSON 结构。 2.性能测试 # 默认测试100000次请求，50并发，GET/SET redis-benchmark -h -p -c 50 -n 100000 # 示例：测试 JSON 设置 redis-benchmark -t set,get -n 10000 -d 256 参数 含义 -n 请求总数（如 100000） -c 并发连接数 -d 每个 value 的数据大小（单位：字节） -t 测试命令（如 set、get、incr） -q 安静模式，仅输出结果 -P 管道数量（批量请求） 3.高可用测试 # 人为停掉 master 容器/进程 docker stop redis-master # Sentinel 日志应该有主从切换 kubectl logs redis-sentinel-0 # 验证新的 master 是否生效 redis-cli -p 26379 sentinel get-master-addr-by-name mymaster redis-cli -p 26379 sentinel masters 4.内存/稳定性/模块压力测试 memtier_benchmark：支持更复杂的数据模型、JSON 结构、TLS 等 redis-py 脚本测试：你可用 Python 写脚本循环写入、删除、并发测试 七、编码 import redis r = redis.Redis(host='localhost', port=6379, decode_responses=True) r.set(\"username\", \"Alice\", ex=10) # 10 秒后过期 print(r.get(\"username\")) # Alice redis事务 MULTI SET key \"value\" INCR counter EXEC MULTI # 开启事务 SET key1 \"A\" # 设置 key1=A SET key2 \"B\" # 设置 key2=B INCR counter # 递增 counter EXEC # 执行事务 redis发布/订阅(Pub/Sub) PUBLISH mychannel \"Hello, World!\" SUBSCRIBE mychannel redis分布式锁 SET mylock \"locked\" EX 10 NX # 10 秒自动过期 lock = r.set(\"mylock\", \"1\", ex=10, nx=True) if lock: print(\"获取锁成功\") else: print(\"锁已被占用\") 哨兵连接 redis: enable: true host: 127.0.0.1 sentinel: - redis-sentinel-0.redis-sentinel-headless.default.svc.cluster.local - redis-sentinel-1.redis-sentinel-headless.default.svc.cluster.local - redis-sentinel-2.redis-sentinel-headless.default.svc.cluster.local port: 26379 db: 0 masterSet: mymaster max_connections: 1000 default_ttl: 60 socket_timeout: 3 socket_keepalive: true health_check_interval: 30 refresh_on_read: true retry_on_timeout: true sentinels = [ (host, port) for host in sentinel ] sentinel_client = Sentinel( sentinels, socket_timeout=socket_timeout, retry_on_timeout=retry_on_timeout, health_check_interval=health_check_interval ) redis_client = sentinel_client.master_for(master) response = await redis_client.ping() 八、实例化redis def __open__(self): if self.REDIS: try: self.REDIS.ping() logging.info(\"Redis connection is still alive, reuse it\") return self.REDIS except Exception: logging.info(\"Existing Redis connection is broken, reconnecting...\") self.REDIS = None logging.info(\"Connecting to Redis...\") logging.info(self.config) socket_timeout=3 socket_keepalive=True health_check_interval=30 retry_on_timeout=True max_connections=5000 decode_responses=True password = self.config.get(\"password\", None) if password == \"\": password = None db = int(self.config.get(\"db\", 1)) try: if self.config.get(\"sentinel\", None): logging.info(\"Using Redis Sentinel\") master = self.config.get(\"master\", \"mymaster\") port = int(self.config.get(\"port\", 26379)) sentinels = [ (host.strip(), port) for host in self.config[\"sentinel\"].strip(\"[]\").split(\" \") ] logging.info(f\"Sentinels: {sentinels}\") sentinel_kwargs = { \"socket_timeout\": socket_timeout, \"retry_on_timeout\": retry_on_timeout, \"health_check_interval\": health_check_interval, \"socket_keepalive\": socket_keepalive, \"decode_responses\": decode_responses } if password: sentinel_kwargs['password'] = password sentinel_client = Sentinel( sentinels, **sentinel_kwargs ) master_kwargs = { 'db': db, 'max_connections': max_connections, 'socket_timeout': socket_timeout, 'retry_on_timeout': retry_on_timeout, 'health_check_interval': health_check_interval, 'socket_keepalive': socket_keepalive, \"decode_responses\": decode_responses } if password: master_kwargs['password'] = password self.REDIS = sentinel_client.master_for( service_name=master, **master_kwargs ) else: self.REDIS = redis.StrictRedis( host=self.config[\"host\"].split(\":\")[0], port=int(self.config.get(\"host\", \":6379\").split(\":\")[1]), db=db, password=password, decode_responses=True, socket_timeout=socket_timeout, socket_keepalive=socket_keepalive, health_check_interval=health_check_interval, retry_on_timeout=retry_on_timeout, max_connections=max_connections ) if self.REDIS: ping_result = self.REDIS.ping() logging.info(f\"Redis ping result: {ping_result}\") self.register_scripts() logging.info(\"Redis connected successfully\") else: logging.warning(\"Failed to create Redis connection\") except Exception as e: logging.error(f\"Redis can't be connected: {e}\") self.REDIS = None return self.REDIS "},"notes/linux/ib.html":{"url":"notes/linux/ib.html","title":"ib","keywords":"","body":"IB 一、介绍 InfiniBand ，一种高性能计算和数据中心网络技术。它提供了一种低延迟、高带宽和可靠性。 RDMA(Remote Direct Memory Access) 二、安装 https://docs.redhat.com/zh-cn/documentation/red_hat_enterprise_linux/8/html-single/configuring_infiniband_and_rdma_networks/index#renaming-ipoib-devices_configuring-ipoib 三、测试 1、查看状态 确认 IB 网卡硬件设备是否正常识别 ip a|grep ib 6: ib0: mtu 2044 qdisc mq state UP group default qlen 256 link/infiniband 00:00:10:29:fe:80:00:00:00:00:00:00:04:3f:72:03:00:da:21:30 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff inet 192.168.1.11/8 brd 11.255.255.255 scope global ib0 查看 IB 网卡状态 ibstat CA 'mlx5_0' CA type: MT4119 Number of ports: 1 Firmware version: 16.32.1010 Hardware version: 0 Node GUID: 0x043f720300da2130 System image GUID: 0x043f720300da2130 Port 1: State: Active Physical state: LinkUp Rate: 100 Base lid: 25606 LMC: 0 SM lid: 117 Capability mask: 0x2651e848 Port GUID: 0x043f720300da2130 Link layer: InfiniBand CA 'mlx5_0': 表示设备的逻辑名称 CA类型：HCA型号为 MT4119。 端口数量：HCA有1个端口。 固件版本：HCA固件版本为 16.32.1010。 硬件版本：HCA硬件版本为0。 节点GUID：此HCA的节点全局唯一标识符(GUID)为0x043f720300da2130。 系统镜像GUID：此HCA的系统镜像GUID为0x043f720300da2130。 端口1：这提供了有关HCA端口1的信息： 状态：端口1的状态为“活动”。 物理状态：端口1的物理状态为“LinkUp”。 速率：端口1的数据传输速率为 100 Gbps。 基本LID：端口1的基本LID(本地标识符)为25606。 LMC：端口1的LMC(LID掩码计数)为0，表示仅使用基本LID。 SM LID：端口1的SM LID(子网管理器LID)为117。 能力掩码：端口1的能力掩码为0x2651e848。 端口GUID：端口1的端口GUID为0x043f720300da2130。 链接层：端口1使用的链接层协议是InfiniBand。 查看所有 InfiniBand 设备 ibv_devices device node GUID ------ ---------------- mlx5_0 043f720300da2130 详细检查设备信息 ibv_devinfo hca_id: mlx5_0 transport: InfiniBand (0) fw_ver: 16.32.1010 node_guid: 043f:7203:00da:2130 sys_image_guid: 043f:7203:00da:2130 vendor_id: 0x02c9 vendor_part_id: 4119 hw_ver: 0x0 board_id: MT_0000000010 phys_port_cnt: 1 port: 1 state: PORT_ACTIVE (4) max_mtu: 4096 (5) active_mtu: 4096 (5) sm_lid: 117 port_lid: 25606 port_lmc: 0x00 link_layer: InfiniBand 查看 PCI 设备 lspci | grep -i mellanox 61:00.0 Infiniband controller: Mellanox Technologies MT27800 Family [ConnectX-5] 检测内核驱动 lsmod | grep mlx mlx5_ib 375821 0 ib_uverbs 132749 2 mlx5_ib,rdma_ucm ib_core 357685 10 rdma_cm,ib_cm,iw_cm,mlx5_ib,ib_umad,ib_uverbs,rdma_ucm,ib_ipoib mlx5_core 1277550 1 mlx5_ib mlxfw 22321 1 mlx5_core psample 13526 1 mlx5_core devlink 48345 1 mlx5_core mlx_compat 56599 10 rdma_cm,ib_cm,iw_cm,mlx5_ib,ib_core,ib_umad,ib_uverbs,mlx5_core,rdma_ucm,ib_ipoib ptp 19231 3 igb,i40e,mlx5_core 2、使用 iperf3 测试 iperf3 可以用于测试 InfiniBand 组网的性能和带宽。为此，需要在 InfiniBand 网络中确认 InfiniBand 适配器已启用 IPoIB 功能。IPoIB是一种在InfiniBand网络上传输IP数据的方法，它允许使用标准的TCP/IP协议栈和网络应用程序。 1 在启用IPoIB之前，确保已正确配置InfiniBand适配器和子网管理器(SM)。 lsmod | grep ib_ipoib modprobe ib_ipoib 2 给IB卡配置IP ip a add 192.168.1.11/8 dev ib0 ip l set ib0 up ping 192.168.1.11 ping 192.168.1.11 -I ib0 3 使用 iperf3 测试 iperf3 -s ------------------------------------------------------------ Server listening on TCP port 5001 TCP window size: 85.3 KByte (default) ------------------------------------------------------------ [ 4] local 192.168.1.11 port 5001 connected with 192.168.1.13 port 57327 [ ID] Interval Transfer Bandwidth [ 4] 0.0-10.0 sec 14.7 GBytes 12.6 Gbits/sec 5201 iperf3 -c 192.168.1.11 ------------------------------------------------------------ Client connecting to 192.168.1.11, TCP port 5001 TCP window size: 408 KByte (default) ------------------------------------------------------------ [ 3] local 192.168.1.13 port 57327 connected with 192.168.1.11 port 5001 [ ID] Interval Transfer Bandwidth [ 3] 0.0-10.0 sec 14.7 GBytes 12.6 Gbits/sec ID：测试流的唯一标识符。 Interval：测试的时间间隔，以秒为单位。 Transfer：在测试过程中传输的总字节数。 Bitrate：传输速率，以比特每秒(bps)为单位。 Retr：在测试期间发生的重传次数。 Sender：表示此行所列出的结果来自iperf3客户端。 Receiver：表示此行所列出的结果来自iperf3服务器。 3、带宽测试 FROM ubuntu:22.04 RUN apt update && apt install -y perftest iperf3 infiniband-diags net-tools iputils-ping wrk vim iproute2 && apt clean docker build -t test.com:5000/k8s/perftest . cat 带宽测试 ib_send_bw ,在server端执行 ib_send_bw -a -c UD -d mlx5_0 -i 1 ************************************ * Waiting for client to connect... * ************************************ Max msg size in UD is MTU 4096 Changing to this MTU --------------------------------------------------------------------------------------- Send BW Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : UD Using SRQ : OFF PCIe relax order: ON ibv_wr* API : ON RX depth : 1000 CQ Moderation : 100 Mtu : 4096[B] Link type : IB Max inline data : 0[B] rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x6406 QPN 0x1652 PSN 0xb6e71d remote address: LID 0x62ca QPN 0x19e1 PSN 0xb814bc --------------------------------------------------------------------------------------- #bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps] 2 1000 0.00 5.39 2.825338 4 1000 0.00 10.67 2.797672 8 1000 0.00 21.54 2.823264 16 1000 0.00 30.30 1.985466 32 1000 0.00 86.60 2.837765 64 1000 0.00 166.40 2.726281 128 1000 0.00 339.47 2.780945 256 1000 0.00 494.67 2.026178 512 1000 0.00 1015.35 2.079434 1024 1000 0.00 2066.23 2.115820 2048 1000 0.00 3615.16 1.850961 4096 1000 0.00 5706.22 1.460792 --------------------------------------------------------------------------------------- -a：这个选项通常用于自动选择最佳的包大小来进行测试。 -c UD：指定使用 Unreliable Datagram (UD) 传输类型进行测试。UD 是一种无连接、不可靠的数据报服务，它适用于那些不需要保证数据可靠传输的场景，例如一些高性能计算或大数据处理任务。 -d mlx5_0：指定使用 mlx5_0这个 InfiniBand 设备来进行测试。mlx5_0是 InfiniBand 设备的名称，通常与特定的硬件适配器相关联。 -i1：这个选项指定了迭代次数，即测试会运行 1 次。 在client端执行 ib_send_bw -a -c UD -d mlx5_0 -i 1 192.168.1.11 Max msg size in UD is MTU 4096 Changing to this MTU --------------------------------------------------------------------------------------- Send BW Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : UD Using SRQ : OFF PCIe relax order: ON ibv_wr* API : ON TX depth : 128 CQ Moderation : 100 Mtu : 4096[B] Link type : IB Max inline data : 0[B] rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x62ca QPN 0x19e1 PSN 0xb814bc remote address: LID 0x6406 QPN 0x1652 PSN 0xb6e71d --------------------------------------------------------------------------------------- #bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps] 2 1000 5.33 4.87 2.553039 4 1000 10.72 10.44 2.738001 8 1000 21.43 21.18 2.776313 16 1000 42.56 40.84 2.676230 32 1000 85.48 85.19 2.791503 64 1000 170.97 163.61 2.680534 128 1000 339.56 333.17 2.729332 256 1000 513.44 486.25 1.991675 512 1000 1005.73 1001.93 2.051956 1024 1000 2049.45 2041.78 2.090782 2048 1000 4052.13 3222.50 1.649920 4096 1000 7301.40 6170.13 1.579554 --------------------------------------------------------------------------------------- 4、延时测试 延时测试 ib_send_lat, 在server端执行 ib_send_lat -a -c UD -d mlx5_0 -i 1 ************************************ * Waiting for client to connect... * ************************************ Max msg size in UD is MTU 4096 Changing to this MTU --------------------------------------------------------------------------------------- Send Latency Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : UD Using SRQ : OFF PCIe relax order: ON ibv_wr* API : ON RX depth : 1000 Mtu : 4096[B] Link type : IB Max inline data : 188[B] rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x6406 QPN 0x1653 PSN 0x35c980 remote address: LID 0x62ca QPN 0x19e2 PSN 0x705b26 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] 2 1000 1.65 6.25 1.67 1.70 0.28 3.52 6.25 4 1000 1.83 7.17 1.86 1.90 0.33 2.19 7.17 8 1000 1.76 6.06 1.80 1.84 0.26 2.08 6.06 16 1000 1.65 6.21 1.68 1.70 0.21 1.97 6.21 32 1000 1.78 12.80 1.81 1.94 0.77 6.75 12.80 64 1000 1.73 6.09 1.76 1.78 0.22 1.95 6.09 128 1000 1.84 5.93 1.87 1.90 0.28 2.17 5.93 256 1000 2.69 6.57 2.72 2.74 0.24 4.00 6.57 512 1000 2.44 6.21 2.48 2.52 0.25 3.71 6.21 1024 1000 2.56 6.55 2.60 2.63 0.29 4.54 6.55 2048 1000 2.98 6.56 3.02 3.05 0.25 4.48 6.56 4096 1000 3.71 7.94 3.73 3.76 0.16 5.04 7.94 --------------------------------------------------------------------------------------- 在client端执行 ib_send_lat -a -c UD -d mlx5_0 -i 1 192.168.1.11 Max msg size in UD is MTU 4096 Changing to this MTU --------------------------------------------------------------------------------------- Send Latency Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : UD Using SRQ : OFF PCIe relax order: ON ibv_wr* API : ON TX depth : 1 Mtu : 4096[B] Link type : IB Max inline data : 188[B] rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x62ca QPN 0x19e2 PSN 0x705b26 remote address: LID 0x6406 QPN 0x1653 PSN 0x35c980 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] 2 1000 1.65 6.28 1.67 1.70 0.28 3.52 6.28 4 1000 1.83 7.47 1.86 1.90 0.36 2.19 7.47 8 1000 1.77 7.19 1.79 1.84 0.28 2.09 7.19 16 1000 1.65 7.43 1.68 1.70 0.24 1.97 7.43 32 1000 1.78 12.79 1.81 1.94 0.77 6.74 12.79 64 1000 1.72 7.50 1.76 1.78 0.24 1.96 7.50 128 1000 1.84 6.91 1.87 1.90 0.30 2.19 6.91 256 1000 2.70 8.32 2.72 2.75 0.26 3.99 8.32 512 1000 2.44 8.03 2.48 2.52 0.27 3.77 8.03 1024 1000 2.56 8.78 2.60 2.64 0.31 4.55 8.78 2048 1000 2.98 8.01 3.02 3.05 0.27 4.50 8.01 4096 1000 3.70 7.95 3.73 3.76 0.19 5.04 7.95 --------------------------------------------------------------------------------------- 5、CPU 频率不一致问题 Conflicting CPU frequency values detected: 1200.000000 != 2000.000000. CPU Frequency is not max. cat /proc/cpuinfo | grep \"cpu MHz\" lscpu | grep \"CPU MHz\" Linux 内核提供了多种调度器（governors）来动态调整 CPU 频率，基于负载和功耗进行优化。常见的 CPU governor 包括： performance：始终运行在最大频率，适用于需要高性能的任务。 powersave：始终运行在最低频率，适用于节能。 ondemand：根据系统负载动态调整频率，适用于平衡性能和功耗。 conservative：类似于 ondemand，但调整频率的步骤更小。 userspace：允许用户手动设置频率。 # 查看 cpupower frequency-info cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor cpupower frequency-set --governor performance # 检查 CPU 频率调节驱动 lsmod | grep acpi_cpufreq modprobe acpi_cpufreq # 调整 CPU 频率在多核系统上的影响 echo \"performance\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor 6、其他命令 ibstat ibstatus ibv_devinfo ibv_devices rdma link mlnx_perf -i ib0 | grep rdma ibstat: 查询 InfiniBand 设备的基本状态 ibstatus： 网卡信息 ibv_devinfo：网卡设备信息（ibv_devinfo -d mlx5_0 -v） ibv_devices：查看本主机的 infiniband 设备 ibnodes：查看网络中的 infiniband 设备 show_gids：看看网卡支持的 roce 版本 show_counters: 网卡端口统计数据，比如发送接受数据大小 mlxconfig: 网卡配置（mlxconfig -d mlx5_1 q 查询网卡配置信息） perftest 工具集： 包括 ib_read_bw、ib_write_bw、ib_send_bw 等，用于 RDMA 的点对点测试。 连接配置 -d ：指定 RDMA 设备（如 mlx5_0）。 -i ：指定设备端口号（如 1）。 -R：使用 RDMA 读写模式。 -F：强制使用 TCP 连接的 IB 地址。 消息大小 -s ：设置数据包大小（默认 2KB）。可以测试特定消息大小的带宽性能。 --range :：设置消息大小的范围，测试多种消息大小的性能。 测试控制 -n ：设置发送消息的迭代次数（默认 1000）。 -t ：设置测试时长（秒），可以让测试持续更长时间。 -x ：设置服务等级（Service Level）。 性能优化 --cpu_util：启用 CPU 使用率统计。 --report_gbps：结果以 Gbps 为单位报告。 结果输出 -o ：将测试结果保存到指定文件。 --report_format ：指定输出格式，例如 CSV。 7、pod中使用 步骤一：安装和配置InfiniBand硬件和驱动 在物理机上安装InfiniBand网卡，并安装相应的驱动程序。 步骤二：部署InfiniBand CNI插件 首先，在Kubernetes集群中安装InfiniBand CNI插件，例如rdma-cni插件。 kubectl apply -f https://raw.githubusercontent.com/Mellanox/rdma-cni/master/k8s-rdma-cni.yaml 步骤三：创建IB网络资源 创建一个IB网络资源，例如名为ib-network的网络。 apiVersion: \"rdma.cni.k8s.io/v1\" kind: RDMAConfiguration metadata: name: ib-network spec: ibDevices: [\"mlx5_1\"] 保存为ib-network.yaml文件，并执行以下命令： kubectl apply -f ib-network.yaml 步骤四：部署Pod并指定使用IB网络 创建一个Pod，并在Pod的配置中指定使用ib-network网络资源。 apiVersion: v1 kind: Pod metadata: name: ib-pod spec: containers: - name: ib-container image: nginx resources: limits: rdma/ib-network: 1 保存为ib-pod.yaml文件，并执行以下命令： kubectl apply -f ib-pod.yaml 8、修改设备名 临时修改 ip link property del dev ibp178s0 altname ibs4 ip link set ibp178s0 down ip link set ibp178s0 name ib0 ip link set ibp178s0 up 永久修改 查看ib卡信息 udevadm info -a -p /sys/class/net/ibp178s0 ... looking at device '/devices/pci0000:a3/0000:a3:01.0/0000:a4:00.0/0000:a5:0c.0/0000:af:00.0/0000:b0:10.0/0000:b2:00.0/net/ibp178s0': KERNEL==\"ibp178s0\" SUBSYSTEM==\"net\" ATTR{addr_assign_type}==\"0\" ATTR{addr_len}==\"20\" ATTR{address}==\"00:00:02:d7:fe:80:00:00:00:00:00:00:94:6d:ae:03:00:9c:e3:f8\" ATTR{broadcast}==\"00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff\" ATTR{carrier_changes}==\"0\" ATTR{carrier_down_count}==\"0\" ATTR{carrier_up_count}==\"0\" ATTR{dev_id}==\"0x0\" ATTR{dev_port}==\"0\" ATTR{flags}==\"0x1002\" ... ATTR{threaded}==\"0\" ATTR{tx_queue_len}==\"256\" ATTR{type}==\"32\" ATTR{umcast}==\"0\" ... ip l|grep ib 8: ibp178s0: mtu 4092 qdisc noop state DOWN mode DEFAULT group default qlen 256 link/infiniband 00:00:02:d7:fe:80:00:00:00:00:00:00:94:6d:ae:03:00:9c:e3:f8 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff 创建 udev 规则 vim /etc/udev/rules.d/70-persistent-net.rules ACTION==\"add\", SUBSYSTEM==\"net\", DRIVERS==\"?*\", ATTR{type}==\"32\", ATTR{address}==\"?*00:94:6d:ae:03:00:9c:e3:f8\", NAME=\"ib0\" 配置网络 vim /etc/netplan/01-network-manager-all.yaml network: version: 2 ethernets: ens8f0np0: dhcp4: no optional: true ens8f0np1: dhcp4: no optional: true bonds: bond0: dhcp4: no addresses: - 10.1.6.27/24 nameservers: addresses: - 114.114.114.114 interfaces: - ens8f0np0 - ens8f0np1 parameters: mode: 802.3ad mii-monitor-interval: 100 lacp-rate: fast transmit-hash-policy: layer3+4 routes: - to: 0.0.0.0/0 via: 10.1.6.254 on-link: true ethernets: ib0: addresses: [12.1.6.27/16] 重启 #udevadm control --reload-rules #udevadm trigger reboot "},"notes/linux/loki.html":{"url":"notes/linux/loki.html","title":"loki","keywords":"","body":"Loki docker run -d --name=loki -p 3100:3100 grafana/loki:latest 健康检测 curl -s \"http://localhost:3100/ready\" 查询 curl -s \"http://localhost:3100/loki/api/v1/query_range?query={job='varlogs'}&limit=10\" "},"notes/linux/http_status_code.html":{"url":"notes/linux/http_status_code.html","title":"HTTP 状态码","keywords":"","body":"HTTP 状态码 ​ HTTP状态码（HTTP Status Code）是用以表示网页服务器超文本传输协议响应状态的数字代码。这些状态码由RFC 2616规范定义，并得到其他多个规范的扩展。HTTP状态码由三位数字组成，第一个数字表示响应的类型，后两个数字表示具体的响应代码。根据HTTP协议的规定，状态码被分为五个类别，每个类别都有特定的含义： 1xx（信息性状态码） 表示请求已被接收，继续处理。这类状态码一般不会出现在浏览器的最终显示中，而是在通信过程中起到辅助作用。 100（Continue 继续） 服务器已经接收到请求的一部分，客户端可以继续发送剩余的请求。服务器已接收到请求头，客户端可以继续发送请求体。 101（Switching Protocols 切换协议） 服务器根据客户端的请求切换协议。例如，切换到更高级的HTTP/1.1协议。例如，从 HTTP/1.1 切换到 WebSocket。 102（Processing 处理中，WebDAV 扩展） 服务器已收到并正在处理请求，但尚未产生响应。 2xx（成功状态码） 表示请求已成功被服务器接收、理解和处理。 200（OK 成功） 请求已成功处理，并返回所请求的资源。 201（Created 已创建） 请求已成功处理，并在服务器上创建了新的资源。通常用于 POST 或 PUT 请求后。 202（Accepted 已接受） 服务器已接受请求，但尚未处理。常用于异步处理场景。 203（Non-Authoritative Information 非权威性信息） 服务器已成功处理了请求，但返回的响应头信息可能来自另一来源。 204（No Content 无内容） 服务器成功处理了请求，但没有返回任何内容。 205（Reset Content 重置内容） 服务器成功处理了请求，且没有返回任何内容，但要求客户端重置文档视图（例如，清除表单内容以重新输入）。 206（Partial Content 部分内容） 服务器成功处理了部分请求（即对文档的某个部分进行请求）。 207（Multi-Status 多状态，WebDAV 扩展） 返回多个独立的响应信息，用于批量操作时指示各个子请求的状态。 208（Already Reported 已报告，WebDAV 扩展） 用于避免在集合资源中重复报告成员状态。 3xx（重定向状态码） 表示需要进一步操作（通常是 URL 跳转）才能完成请求。 301（Moved Permanently 永久重定向） 请求的资源已经被永久移动到新的URL上，之后的请求都应使用新的 URL。 302（Found 临时重定向） 请求的资源暂时被移动到了新的URL上，客户端应继续使用原有URL进行以后的请求（HTTP/1.0）或自动访问新的URL（HTTP/1.1及之后的版本）。 303（See Other 查看其他位置） 与302类似，但明确要求客户端使用GET方法获取资源。 304（ Not Modified 未修改） 客户端发送了一个条件请求（如使用If-Modified-Since或If-None-Match头），服务器判断资源未发生变化，返回此状态码。客户端使用缓存的版本，服务器告知资源未发生变化，不需要传输。 307（Temporary Redirect 临时重定向） 与302类似，但明确要求客户端在后续请求中继续使用POST方法（针对302可能改变请求方法的问题）。 308（Permanent Redirect 永久重定向，RFC 7538） 与 301 类似，但要求客户端在重定向时保持原有请求方法和请求体。 4xx（客户端错误状态码） 表示客户端发送的请求有错误，服务器无法理解或无法完成。 400（Bad Request 错误请求） 服务器无法理解请求的语法。 401（Unauthorized 未授权）：请求需要身份验证，但客户端没有提供有效的凭据，或者凭据不被服务器接受。 401.1：登录失败。 401.2：服务器配置问题导致登录失败。 401.3：ACL禁止访问资源。 401.4：授权被筛选器拒绝。 401.5：ISAPI或CGI授权失败。 401.7：访问被Web服务器上的URL授权策略拒绝（IIS 6.0专用）。 402（Payment Required 需要付款） 该状态码已保留，但未被使用。 403（Forbidden 禁止访问）：服务器拒绝请求访问。 403.1：禁止可执行访问。 403.2：禁止读访问。 403.3：禁止写访问。 403.4：要求SSL。 403.5：要求 SSL 128 位加密。 403.6：IP地址被拒绝。 403.7：要求客户端证书。 403.8：站点访问被拒绝。 403.9：连接的用户过多。 403.10：配置无效。 403.11：密码更改。 403.12：拒绝访问映射表。 403.13：客户端证书被吊销。 403.14：拒绝目录列表。 403.15：超出客户端访问许可。 403.16：客户端证书不受信任或无效。 403.17：客户端证书已过期或尚未生效。 403.18：在当前的应用程序池中不能执行所请求的URL（IIS 6.0专用）。 403.19：不能为这个应用程序池中的客户端执行CGI（IIS 6.0专用）。 403.20：Passport登录失败（IIS 6.0专用）。 404（Not Found 未找到）：服务器找不到请求的资源。 404.0：没有找到文件或目录。 404.1：无法在所请求的端口上访问Web站点。 404.2：Web服务扩展锁定策略阻止本请求。 404.3：MIME映射策略阻止本请求。 405（ Method Not Allowed 方法不被允许） 用来访问本页面的HTTP谓词不被允许（例如，不允许使用POST方法）。请求方法（如 POST、PUT 等）不被允许使用在该资源上。 406（Not Acceptable 无法接受） 客户端浏览器不接受所请求页面的MIME类型。 407（Proxy Authentication Required 要求代理身份验证） 请求需要通过代理服务器进行身份验证。 408（Request Timeout 请求超时） 客户端没有在服务器期望的时间内完成请求。 409（Conflict 冲突） 请求因为与资源的当前状态冲突而失败。 410（Gone 永远不可用） 请求的资源已被永久删除，且不会再有可用的副本。 411（Length Required 需要有效长度） 服务器拒绝接受没有定义Content-Length头的请求（针对HTTP/1.1）。 412（Precondition Failed 先决条件失败） 请求头中的条件导致请求失败（例如，If-Match）。 413（Payload Too Large 请求实体太大） 服务器无法处理请求，因为请求实体太大，超过了服务器的处理能力或配置的限制。 414（URI Too Long 请求URI太长） 请求的URI太长，服务器无法处理。 415（Unsupported Media Type 不支持的媒体类型） 请求的格式不被请求方法所支持（例如，使用POST方法上传文件时，文件类型不被允许）。 416（Range Not Satisfiable 请求范围不符合要求） 请求的范围无效，无法满足请求。 417（Expectation Failed 执行失败，预期失败） 期望的请求头“Expect”值未实现。 422（Unprocessable Entity 不可处理的实体） 请求实体在语义上正确，但无法按照请求的格式进行处理（WebDAV）。 423（Locked 锁定的错误） 资源被锁定，无法完成请求。 424（Failed Dependency 方法失败） 请求方法失败或请求的消息体失败（WebDAV）。 426（Upgrade Required 升级所需的） 客户端应切换到TLS/1.0（或其他更高版本的协议）以进行请求。 428（Precondition Required 先决条件必需） 请求需要满足特定的前提条件。 429（Too Many Requests 请求过多） 用户在短时间内发送了太多的请求（通常用于防止恶意攻击和爬虫）。 431（Request Header Fields Too Large 请求头字段过大） 服务器不愿意处理请求，因为请求头字段太大。 451（Unavailable For Legal Reasons 因法律原因被阻止） 请求因法律原因被阻止（例如，由于版权问题）。 499 （Client Closed Request 客户端关闭请求，Nginx 非标准） HTTP状态码499是一个非标准的状态码，它表示服务器已经成功处理了请求，但在返回响应之前，客户端主动关闭了连接。这个状态码通常由Nginx服务器引入和记录。当客户端（如浏览器）在等待服务器响应的过程中关闭连接时，如果服务器是Nginx，那么它会将此情况记录为499状态码。 5xx（服务器错误状态码） 表示服务器在处理请求时发生了错误。表示服务器在处理请求时发生内部错误，导致无法完成请求。 500（Internal Server Error 内部服务器错误）：服务器在执行请求时发生了错误。 500.100：内部ASP错误。 500.12：应用程序正忙于在Web服务器上重新启动。 500.13：Web服务器太忙。 500.14：应用程序无效。 500.15：不允许直接请求Global.asa。 500.16：UNC授权凭据不正确（IIS 6.0专用）。 500.18：URL授权存储不能打开（IIS 6.0专用）。 501（Not Implemented 未实现） 服务器不支持请求所需的功能或方法。 502（Bad Gateway 错误网关） 作为网关或代理的服务器从上游服务器接收到无效响应。 503（Service Unavailable 服务不可用） 服务器由于过载或维护暂时无法处理请求，通常为临时状态。 504（Gateway Timeout 网关超时） 作为网关或代理的服务器未能及时从上游服务器接收到响应。 505（HTTP Version Not Supported HTTP 版本不受支持） 服务器不支持请求中使用的 HTTP 协议版本。 506（Variant Also Negotiates 变体也协商） 服务器内部配置错误，导致内容协商出现循环引用。 507（Insufficient Storage 存储空间不足） 服务器无法存储完成请求所必需的内容（WebDAV 扩展）。 508（Loop Detected 循环引用检测） 服务器检测到无限循环（WebDAV 扩展）。 510（Not Extended 扩展不满足要求） 服务器要求客户端提供更多扩展信息以完成请求。 511（Network Authentication Required 需要网络认证） 客户端必须先进行网络认证才能获得网络访问权限（常见于捕获门户）。 "},"notes/linux/yq.html":{"url":"notes/linux/yq.html","title":"yq","keywords":"","body":"yq https://github.com/mikefarah/yq/ 1、下载 wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq chmod +x /usr/bin/yq 2、测试 cat a.yaml hl: yani: bbb yq .hl a.yaml yq e -i '.hl.yani = \"bbb\"' a.yaml cat a.yaml | yq .hl admin_key=$(yq '.deployment.admin.admin_key[0].key' conf/config.yaml | sed 's/\"//g') "},"notes/linux/conda.html":{"url":"notes/linux/conda.html","title":"conda","keywords":"","body":"Conda 一、下载 https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.sh 二、安装 chmod +x Miniconda3-latest-Linux-x86_64.sh ./Miniconda3-latest-Linux-x86_64.sh 三 、初始化 ./bin/conda init 四、配置镜像源 conda config --show channels conda config --show conda config --set 配置项=值 conda config --remove-key 配置项 conda config --remove channels https://xxxxxxxxxxxxxxx conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes 五、使用 1.创建环境 conda create --name env310 python=3.10 numpy matplotlib conda info --env conda env list conda create --name new_env_name --clone old_env_name 2.删除环境 conda remove --name env310 --all #删除所有不再需要的文件和缓存 conda clean --all 3.激活/退出环境 conda activate env310 conda deactivate 4.安装/卸载库 conda list conda install scipy conda remove scipy conda update scipy conda search scipy 六、其他 #关闭自动激活状态 conda config --set auto_activate_base false #开启自动激活状态 conda config --set auto_activate_base true 卸载 conda #1.直接删除安装目录 rm -rf /data/weisx/conda #2.撤消对shell初始化脚本的更改 conda init --reverse --all #3.删除可能已在主目录中创建的以下隐藏文件和文件夹 rm -rf ~/.condarc ~/.conda ~/.continuum "},"notes/linux/minio.html":{"url":"notes/linux/minio.html","title":"minio","keywords":"","body":"Minio https://github.com/minio/minio.git 一、安装 https://github.com/minio/minio docker run -p 9090:9000 --name minio \\ -v /etc/localtime:/etc/localtime \\ -v /data/minio/data:/data \\ -v /data/minio/config:/root/.minio \\ -d minio/minio server /data http://127.0.0.1:9090/ AccessKey: minioadmin SecretKey: minioadmin 二、使用 mc alias set local https://test.com:59000 username password mc admin info local mc ls local/aaa mc cp local/aaa/file1 . mc put ./test local/aaa mc get local/aaa/file1 ./ curl https://test.com:59000/aaa/file1 -o file1 curl -k -C- -O --retry 3 https://test.com:59000/aaa/file1 curl -X PUT -u \":\" -T \"example.txt\" \\ http://:/mybucket/example.txt curl -X GET -u \":\" \\ http://:/mybucket/example.txt -o \"example.txt\" curl -X DELETE -u \":\" \\ http://:/mybucket/example.txt curl -X GET -u \":\" \\ http://:/mybucket?list-type=2 mc 使用 命令 作用 ls 列出文件和文件夹 mb 创建一个存储桶或一个文件夹 cat 显示文件和对象内容 pipe 将一个STDIN重定向到一个对象或者文件或者STDOUT share 生成用于共享的URL cp 拷贝文件和对象 mirror 给存储桶和文件夹做镜像 find 基于参数查找文件 diff 对两个文件夹或者存储桶比较差异 rm 删除文件和对象 events 管理对象通知 watch 监听文件和对象的事件 policy 管理访问策略 session 为cp命令管理保存的会话 config 管理mc配置文件 update 检查软件更新 version 输出版本信息 运行 MinIO Client docker run -it --entrypoint=/bin/sh minio/mc mc config host add minio http://172.20.32.232:9090 minioadmin minioadmin --api s3v4 mc ls minio //查看存储桶 mc ls minio/test //查看存储桶test中存在的文件 mc mb minio/dnps //创建一个名为dnps的存储桶 mc share download minio/test/small.jpg //共享test桶下small.jpg文件的下载路径 mc find minio/test --name \"*.jpg\" //查找test存储桶中的png文件 mc policy set download minio/dnps/ //设置权限：none, download, upload, public mc policy list minio/dnps/ //查看存储桶当前权限 mc cp minio/test/small.jpg minio/dnps/ //拷贝文件和对象 "},"notes/linux/prometheus.html":{"url":"notes/linux/prometheus.html","title":"prometheus","keywords":"","body":"Prometheus https://prometheus.io/docs/prometheus/latest/querying/basics/ https://prometheus.io/docs/prometheus/latest/querying/api/ 一、PromQL 1、选择器 = 与字符串匹配 != 与字符串不匹配 =~ 与字符串正则匹配 !~ 与字符串正则不匹配 http_requests_total{handler=~\"/api/v1/.*\"} 2、范围查询 ms, s, m, h, d, w, y http_requests_total[5m] 3、时间位移 查询的时间范围分别前移 5分钟 node_filesystem_free_bytes{mountpoint=\"/data00\"} offset 5m 4、操作符 https://prometheus.io/docs/prometheus/latest/querying/operators/ +, -, *, /, %, ^ ==, !=, >, =, and, or, unless on, ignoring roup_left, roup_right 5、聚合操作 函数 说明 sum 求和 count 计数 count_values 对value计数 min 最小值 max 最大值 avg 平均值 stddev 标准差 stdvar 标准方差 bottomk 后n条时序 topk 前n条时序 quantile 分位数 @ 更改查询中各个即时和范围向量的计算时间 atan2 弧度计算 without用于从计算结果中移除列举的标签，而保留其它标签。 by则正好相反，结果向量中只保留列出的标签，其余标签则移除。 sum(http_requests_total) without (instance) 等价于 sum(http_requests_total) by (code,handler,job,method) # 获取HTTP请求数前5位的时序样本数据 topk(5, http_requests_total) # quantile用于计算当前样本数据值的分布情况quantile(φ, express)其中0 ≤ φ ≤ 1。 例如，当φ为0.5时，即表示找到当前样本数据中的中位数： quantile(0.5, http_requests_total) count by (namespace) (kube_pod_container_resource_limits: {ressource=\"cpu\"}) sum by (node) (node_memory_MemTotal_bytes — node_memory_MemAvailable_bytes) sum(http_requests_total{method=\"GET\"} @ 1609746000) rate(http_requests_total[5m])[30m:1m] sum(apisix_http_requests_total)[1h:] 6、常用方法 https://prometheus.io/docs/prometheus/latest/querying/functions/ 1.increase 获取区间向量中第一个样本和最后一个样本，并返回其增长量。 获取节点存储5分钟内的变化量 increase(node_filesystem_free_bytes{mountpoint=\"/data\"}[5m]) 2.rate、irate rate 计算区间向量在时间窗口内平均增长速率，会在单调性发生变化时自动中断。 irate 计算区间向量的增长率，但其反应出的是瞬时增长率。 rate 与 irate 函数仅适用于 Counter 类型的 Metrics。 获取 HTTP Request 请求5 分钟内的变化率。 rate(http_request_total{status=\"200\", method=\"GET\"}[5m]) irate(http_request_total{status=\"200\", method=\"GET\"}[5m]) 3.delta、idelta delta 计算一个区间向量的第一个元素和最后一个元素之间的差值。 idelta与delta函数类似，不同的是它计算最新的2个样本之间的差值。 获取最近两个小时 CPU 的温度差值。 delta(cpu_temp_celsius{host=\"zeus\"}[2h]) 4.histogram_quantile 计算分位数 计算延迟的 P99 值。 histogram_quantile(0.99 , rate(prometheus_tsdb_compaction_chunk_range_bucket[5m])) 5.absent 一般用于验证 样本是否存在，如果存在则返回空，如果不存在，则返回 1。 确认节点中 node exporter 是否存在。 absent(up{job=\"node-exporter\", instance=\"127.0.0.1:9100\"}) 6.abs、ceil、floor abs() 绝对值 ceil() 向上取整 floor() 向下取整 7.label_replace 动态标签替换 label_replace(node_boot_time_seconds{instance=\"10.13.1.10:9100\"},\"node\",\"$1\",\"instance\",\"(.*):9100\") label_replace(up, \"host\", \"$1\", \"instance\", \"(.*):.*\") label_replace(apisix_http_status,\"path\",\"$0\",\"matched_uri\",\".*\") 8.label_join label_join(up{job=\"api-server\",src1=\"a\",src2=\"b\",src3=\"c\"}, \"foo\", \",\", \"src1\", \"src2\", \"src3\") 9.time 当前时间戳 (time()-node_boot_time_seconds)/60/60/24 10.group_right/group_left a * on (foo, bar) b a * ignoring (baz) b a * on (foo, bar) group_left(baz) b kube_node_info * on (node) group_right() kube_node_status_condition{condition=\"Ready\",status=\"true\"} * on (node) group_right() label_replace(node_boot_time_seconds,\"node\",\"$1\",\"instance\",\"(.*):9100\") (sum(label_replace(label_replace(apisix_http_status,\"host\",\"$0\",\"matched_host\",\".*\"),\"path\",\"$0\",\"matched_uri\",\".*\")) by (code,path,host)) * on (host,path) group_left(service_name,ingress,namespace,service_port) (kube_ingress_path) (sum(label_replace(label_replace(apisix_http_status,\"host\",\"$0\",\"matched_host\",\".*\"),\"path\",\"$1\",\"matched_uri\",\"(/[^/.]*).*\")) by (code,path,host)) * on (host,path) group_left(service_name,ingress,namespace,service_port) (kube_ingress_path) (sum(label_replace(apisix_http_status{code!=\"200\"}, \"host\", \"$0\", \"matched_host\", \".*\")) by (host) * on (host) group_left(ingress,namespace,path,service_name,service_port) kube_ingress_path{service_name=\"httpbin\"}) - (sum(label_replace(apisix_http_status{code!=\"200\"} offset 15s, \"host\", \"$0\", \"matched_host\", \".*\")) by (host) * on (host) group_left(ingress,namespace,path,service_name,service_port) kube_ingress_path{service_name=\"httpbin\"}) (sum(label_replace(increase(apisix_http_status[30s]), \"host\", \"$0\", \"matched_host\", \".*\")) by (host) * on (host) group_left(ingress,namespace,path,service_name,service_port) kube_ingress_path{service_name=\"httpbin\"}) (sum(label_replace(apisix_http_status{code!=\"200\"}, \"host\", \"$0\", \"matched_host\", \".*\")) by (host) * on (host) group_left(service_name) kube_ingress_path{service_name=\"default-svc-infer01\"} or sum(kube_ingress_path{service_name=\"default-svc-infer01\"}*0) by (host,service_name)) - (sum(label_replace(apisix_http_status{code!=\"200\"} offset 1d, \"host\", \"$0\", \"matched_host\", \".*\")) by (host) * on (host) group_left(service_name) kube_ingress_path{service_name=\"default-svc-infer01\"} or sum(kube_ingress_path{service_name=\"default-svc-infer01\"}*0) by (host,service_name)) 11.increase 增长量 increase(http_requests_total{job=\"apiserver\"}[5m]) 12.vector apisix_http_status or vector(0) 7、常用指标 1.QPS，Queries Per Second 每秒查询率 sum(label_replace(rate(apisix_http_status[30s]), \"host\", \"$0\", \"matched_host\", \".*\")) by (host) * on (host) group_left(ingress,namespace,path,service_name,service_port) kube_ingress_path{service_name=\"httpbin\"} 2.RT，Response Time 响应时间 rate 计算区间向量在时间窗口内平均增长速率，会在单调性发生变化时自动中断。 过去5分钟内第90个百分位数的请求延迟 过去5分钟内90%请求的平均响应时间 histogram_quantile(0.90, sum(rate(apisix_http_latency_bucket{type=\"request\"}[5m])) by (le)) {le=\"+Inf\"} 是一个特殊的标签选择器，用于选择直方图中所有桶（bucket） le less than or equal to, +Inf 正无穷大的“溢出”桶 二、HTTP API https://prometheus.io/docs/prometheus/latest/querying/api/ curl -G 'http://192.168.0.127:32070/api/v1/query_range' \\ --data-urlencode 'query=sum(apisix_http_requests_total)' \\ --data-urlencode 'start=2024-07-15T20:10:30.781Z' \\ --data-urlencode 'end=2024-07-15T20:10:30.781Z' \\ --data-urlencode 'step=15s' 三、自定义metric go get github.com/prometheus/client_golang/prometheus go get github.com/prometheus/client_golang/prometheus/promhttp package main import ( \"net/http\" \"github.com/prometheus/client_golang/prometheus\" \"github.com/prometheus/client_golang/prometheus/promhttp\" ) // 定义计数器 var requestTotal = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"http_requests_total\", Help: \"Number of get requests.\", }, []string{\"code\"}, ) func main() { // 注册指标 prometheus.MustRegister(requestTotal) // 使用http.HandleFunc来处理metrics请求 http.Handle(\"/metrics\", promhttp.Handler()) // 你的业务逻辑 // ... // 启动HTTP服务器 http.ListenAndServe(\":8080\", nil) } vim prometheus-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: prometheus-config data: prometheus.yml: | global: scrape_interval: 15s scrape_configs: - job_name: 'my-custom-metrics' static_configs: - targets: ['localhost:8080'] 四、kube-state-metrics https://github.com/kubernetes/kube-state-metrics/blob/main/docs/metrics/cluster/node-metrics.md 1、允许labels vim kubeasz/roles/kube-prometheus-stack/files/kube-prometheus-stack/charts/kube-state-metrics/values.yaml ... extraArgs: [\"--metric-labels-allowlist=nodes=[*]\", \"--metric-annotations-allowlist=nodes=[*]\"] ... --metric-labels-allowlist=pods=[*] --metric-labels-allowlist=nodes=[*],pods=[*],persistentvolumeclaims=[*],deployments=[*],statefulsets=[*],configmaps=[*],secrets=[*],services=[*],replicasets=[*] --metric-labels-allowlist=*=[*] --metric-annotations-allowlist=pods=[*] --metric-labels-allowlist=pods=[*],nodes=[node,failure-domain.beta.kubernetes.io/zone,topology.kubernetes.io/zone] 2、查询指标 curl 'http://127.0.0.1:9090/metrics' curl -G 'http://127.0.0.1:9090/api/v1/query' \\ --data-urlencode 'query=count(kube_node_status_condition{condition=\"Ready\", status=\"false\"})/count(kubelet_node_name)*100' curl -G 'http://127.0.0.1:30127/api/v1/query_range' \\ --data-urlencode 'query=sum(apisix_http_requests_total)' \\ --data-urlencode 'start=1721062917' \\ --data-urlencode 'end=1721116919' \\ --data-urlencode 'step=15s' 3、其他 --metric-denylist=kube_deployment_spec_.* "},"notes/linux/test_tools.html":{"url":"notes/linux/test_tools.html","title":"test_tools","keywords":"","body":"测试工具 一、ab（Apache Benchmark） apt -y install apache2-utils ab -n 1000 -c 100 http://你的网站地址/ -n 1000：表示总共要发送1000个请求。 -c 100：表示模拟100个并发用户，也就是同时有100个用户在访问网站。 命令执行后，ab会输出一些测试结果，包括每秒处理的请求数（RPS）、平均响应时间等。 Server Software: nginx/1.10.2 (服务器软件名称及版本信息) Server Hostname: 192.168.1.106(服务器主机名) Server Port: 80 (服务器端口) Document Path: /index1.html. (供测试的URL路径) Document Length: 3721 bytes (供测试的URL返回的文档大小) Concurrency Level: 1000 (并发数) Time taken for tests: 2.327 seconds (压力测试消耗的总时间) Complete requests: 5000 (的总次数) Failed requests: 688 (失败的请求数) Write errors: 0 (网络连接写入错误数) Total transferred: 17402975 bytes (传输的总数据量) HTML transferred: 16275725 bytes (HTML文档的总数据量) Requests per second: 2148.98 [#/sec] (mean) (平均每秒的请求数) 这个是非常重要的参数数值，服务器的吞吐量 Time per request: 465.338 [ms] (mean) (所有并发用户(这里是1000)都请求一次的平均时间) Time request: 0.247 [ms] (mean, across all concurrent requests) (单个用户请求一次的平均时间) Transfer rate: 7304.41 [Kbytes/sec] received 每秒获取的数据长度 (传输速率，单位：KB/s) ... Percentage of the requests served within a certain time (ms) 50% 347 ## 50%的请求在347ms内返回 66% 401 ## 60%的请求在401ms内返回 75% 431 80% 516 90% 600 95% 846 98% 1571 99% 1593 100% 1619 (longest request) 其他Linux系统的性能指标，如CPU负载、内存使用、网络带宽等。 "},"notes/linux/gitlab.html":{"url":"notes/linux/gitlab.html","title":"gitlab","keywords":"","body":"GitLab https://docs.gitlab.com/ee/ 一、安装 1、通过docker安装 mkdir -p config logs data docker run --detach \\ --hostname 192.168.0.10 \\ --env GITLAB_OMNIBUS_CONFIG=\"external_url 'http://192.168.0.10:30'; gitlab_rails['lfs_enabled'] = true; gitlab_rails['gitlab_shell_ssh_port'] = 22\" \\ --publish 16443:443 --publish 30:30 --publish 1622:22 \\ --name gitlab \\ --restart always \\ --volume ${PWD}/config:/etc/gitlab:Z \\ --volume ${PWD}/logs:/var/log/gitlab:Z \\ --volume ${PWD}/data:/var/opt/gitlab:Z \\ --shm-size 256m \\ gitlab/gitlab-ce:16.6.2-ce.0 2、获取root密码 docker exec -it gitlab grep 'Password:' /etc/gitlab/initial_root_password 3、其他 docker exec -it gitlab editor /etc/gitlab/gitlab.rb docker exec gitlab gitlab-ctl reconfigure docker exec gitlab update-permissions 二、配置runner 1、通过虚拟机 安装 gitlab-runner curl -L --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64 gitlab-runner register Enter the GitLab instance URL (for example, https://gitlab.com/): http://192.168.0.10:30/ Enter the registration token: (从gitlab管理界面获取token) GR1348951A12mTBjNFzr9FXcH8sR1 Enter a description for the runner: [runner]: vm Enter tags for the runner (comma-separated): vm Enter optional maintenance note for the runner: Enter an executor: shell, ssh, docker+machine, instance, kubernetes, custom, docker, docker-windows, parallels, virtualbox, docker-autoscaler: shell Runner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded! Configuration (with the authentication token) was saved in \"/etc/gitlab-runner/config.toml\" cat /etc/gitlab-runner/config.toml concurrent = 1 check_interval = 0 shutdown_timeout = 0 [session_server] session_timeout = 1800 [[runners]] name = \"vm22\" url = \"http://192.168.0.10:30/\" id = 4 token = \"asdsad\" token_obtained_at = 2023-12-15T03:23:09Z token_expires_at = 0001-01-01T00:00:00Z executor = \"ssh\" [runners.cache] MaxUploadedArchiveSize = 0 [runners.ssh] user = \"root\" password = \"123\" host = \"192.168.0.20\" port = \"22\" 2、runner 依赖安装 安装 release-cli curl --location --output /usr/local/bin/release-cli \"https://gitlab.com/api/v4/projects/gitlab-org%2Frelease-cli/packages/generic/release-cli/latest/release-cli-linux-amd64\" chmod +x /usr/local/bin/release-cli 安装 docker 环境 安装 golang 环境 安装 buildx 环境 3、其他配置 gitlab 管理界面 CI/CD -> Runners -> 编辑（铅笔图标） -> Run untagged jobs （勾选） 三、编写 .gitlab-ci.yml stages: - build - upload - release variables: PACKAGE_VERSION: ${CI_COMMIT_BRANCH}-${CI_COMMIT_SHORT_SHA} REGISTRY: \"192.168.0.10:3000\" AMD64_TAR: \"aaa-amd64.tar.gz\" ARM64_TAR: \"aaa-arm64.tar.gz\" INSTALL: \"install.sh\" PACKAGE_REGISTRY_URL: \"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/Release/${PACKAGE_VERSION}\" build-job: image: ${REGISTRY}/cicd/build-base:v1.0.1 stage: build except: - tags # rules: # - if: $CI_COMMIT_TAG # Do not run this job when a tag is created manually # when: never # - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH # Run this job when commits are pushed or merged to the default branch script: - echo \"running build-job for ${PACKAGE_VERSION}\" - make artifacts: untracked: false when: on_success expire_in: 30 days paths: - bin/${AMD64_TAR} - bin/${ARM64_TAR} - bin/${INSTALL} upload-job: stage: upload image: ${REGISTRY}/cicd/curl:latest needs: - job: build-job artifacts: true script: - echo \"running upload-job for ${PACKAGE_VERSION}\" - | curl --header \"JOB-TOKEN: ${CI_JOB_TOKEN}\" --upload-file bin/${AMD64_TAR} \"${PACKAGE_REGISTRY_URL}/${AMD64_TAR}\" curl --header \"JOB-TOKEN: ${CI_JOB_TOKEN}\" --upload-file bin/${ARM64_TAR} \"${PACKAGE_REGISTRY_URL}/${ARM64_TAR}\" curl --header \"JOB-TOKEN: ${CI_JOB_TOKEN}\" --upload-file bin/install.sh \"${PACKAGE_REGISTRY_URL}/${INSTALL}\" - echo ${PACKAGE_REGISTRY_URL}/${AMD64_TAR} - echo ${PACKAGE_REGISTRY_URL}/${ARM64_TAR} - echo ${PACKAGE_REGISTRY_URL}/${INSTALL} release-job: stage: release image: ${REGISTRY}/cicd/release-cli:latest needs: - job: upload-job script: - echo \"running release-job for ${PACKAGE_VERSION}\" release: name: ${PACKAGE_VERSION} tag_name: ${PACKAGE_VERSION} description: ${PACKAGE_VERSION} assets: links: - name: ${AMD64_TAR} url: \"${PACKAGE_REGISTRY_URL}/${AMD64_TAR}\" filepath: \"/${AMD64_TAR}\" link_type: package - name: ${ARM64_TAR} url: \"${PACKAGE_REGISTRY_URL}/${ARM64_TAR}\" filepath: \"/${ARM64_TAR}\" link_type: package - name: ${INSTALL} url: \"${PACKAGE_REGISTRY_URL}/${INSTALL}\" filepath: \"/${INSTALL}\" link_type: package 四、删除pipeline cat > ./remove_pipeline.sh 五、FAQ 1、”\\ No newline at end of file“ vim :set binary noeol "},"notes/linux/etcd.html":{"url":"notes/linux/etcd.html","title":"etcd","keywords":"","body":"ETCD 相关 一、搭建 1、创建单独的网络 docker network create etcd --subnet 172.19.0.0/16 2、运行etcd0 docker run -d --name etcd0 --network etcd --ip 172.19.1.10 quay.io/coreos/etcd etcd \\ -name etcd0 \\ -advertise-client-urls http://172.19.1.10:2379,http://172.19.1.10:4001 \\ -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \\ -initial-advertise-peer-urls http://172.19.1.10:2380 \\ -listen-peer-urls http://0.0.0.0:2380 \\ -initial-cluster-token etcd-cluster-1 \\ -initial-cluster etcd0=http://172.19.1.10:2380,etcd1=http://172.19.1.11:2380,etcd2=http://172.19.1.12:2380 \\ -initial-cluster-state new 3、运行etcd1 docker run -d --name etcd1 --network etcd --ip 172.19.1.11 quay.io/coreos/etcd etcd \\ -name etcd1 \\ -advertise-client-urls http://172.19.1.11:2379,http://172.19.1.11:4001 \\ -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \\ -initial-advertise-peer-urls http://172.19.1.11:2380 \\ -listen-peer-urls http://0.0.0.0:2380 \\ -initial-cluster-token etcd-cluster-1 \\ -initial-cluster etcd0=http://172.19.1.10:2380,etcd1=http://172.19.1.11:2380,etcd2=http://172.19.1.12:2380 \\ -initial-cluster-state new 4、运行etcd2 docker run -d --name etcd2 --network etcd --ip 172.19.1.12 quay.io/coreos/etcd etcd \\ -name etcd2 \\ -advertise-client-urls http://172.19.1.12:2379,http://172.19.1.12:4001 \\ -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \\ -initial-advertise-peer-urls http://172.19.1.12:2380 \\ -listen-peer-urls http://0.0.0.0:2380 \\ -initial-cluster-token etcd-cluster-1 \\ -initial-cluster etcd0=http://172.19.1.10:2380,etcd1=http://172.19.1.11:2380,etcd2=http://172.19.1.12:2380 \\ -initial-cluster-state new 二、使用 1、进入容器 docker run -it --name client --network etcd quay.io/coreos/etcd sh 2、etcd2 etcdctl --endpoints http://172.19.1.10:2379,http://172.19.1.11:2379,http://172.19.1.12:2379 set /foo bar etcdctl --endpoints http://172.19.1.10:2379,http://172.19.1.11:2379,http://172.19.1.12:2379 get /foo 3、etcd3 ETCDCTL_API=3 etcdctl --endpoints http://172.19.1.10:2379,http://172.19.1.11:2379,http://172.19.1.12:2379 put foo bar ETCDCTL_API=3 etcdctl --endpoints http://172.19.1.10:2379,http://172.19.1.11:2379,http://172.19.1.12:2379 get foo 4、查看状态 etcdctl --endpoints http://172.19.1.10:2379 endpoint status etcdctl --endpoints http://172.19.1.10:2379 endpoint status -w table etcdctl --endpoints http://172.19.1.10:2379 endpoint health etcdctl --endpoints http://172.19.1.10:2379 endpoint health -w table etcdctl --endpoints http://172.19.1.10:2379 member list -w table "},"notes/linux/frp.html":{"url":"notes/linux/frp.html","title":"frp","keywords":"","body":"frp 相关 1、容器镜像 docker pull docker.io/snowdreamtech/frps:latest 2、服务端 cat > ./frps.toml docker run --restart=always --network host -d -v $PWD/frps.toml:/etc/frp/frps.toml --name frps snowdreamtech/frps 3、客户端 cat > /etc/frp/frpc.toml docker run --restart=always --network host -d -v $PWD/frpc.toml:/etc/frp/frpc.toml --name frpc snowdreamtech/frpc "},"notes/linux/synology.html":{"url":"notes/linux/synology.html","title":"synology","keywords":"","body":"Synology 1、安装 教程 教程2 DS918+ 7.2.1-69057 安装镜像 操作系统pat文件 ChipGenius 群晖搜索助手 1、安装镜像修改 通过Diskgenius修改安装镜像 grub.cfg RPCB1(0)->boot->grub->grub.cfg 修改 vid、pid、sn、mac1、mac2 通过ChipGenius获取U盘vid、pid 2、制作U盘系统 Rufus imageUSB 3、安装 进入Bios设置U盘为第一启动 进入启动，等待安装 通过群晖搜索助手 扫描synology IP地址 打开浏览器，IP:5000 安装 DiskStation Manager，选择 DSM_DS918+_69057.pat 根据步骤安装 4、使用m.2硬盘 winhex 参考 NVME硬盘群晖无法识别，因为群晖提亲设定了各个机型NVME所在的PCI位置，保存在/lib64/libsynonvme.so.1文件中。 1、查看nvme信息 ls /dev/nvme* udevadm info /dev/nvme0n1 P: /devices/pci0000:00/0000:00:1c.0/0000:01:00.0/nvme/nvme0/nvme0n1 需要使用上述 0000:00:1c.0，第三个字段是硬盘ID 2、修改/lib64/libsynonvme.so.1 备份 cp /lib64/libsynonvme.so.1 /lib64/libsynonvme.so.1.bak 复制到windows上 使用winhex修改 使用winhex打开libsynonvme.so.1 ctrl+F 搜索 DS918+ 在右侧找到原数据为0000:00:13.0和0000:00:13.1的字段，根据第一步查到的本机NVME所在的PCI位置，修改为0000:00:1d.0，顺便把另外一个nvme插槽也修改为0000:00:1d.1，修改后保存。 3、上传到群晖，替换 /lib64/libsynonvme.so.1 chmod 755 /lib64/libsynonvme.so.1 4、重启群晖。 5、其他 fdisk -l /dev/nvme0n1 cat /proc/mdstat 分区 synopartition --part /dev/nvme0n1 7 mdadm --create /dev/md7 --level=1 --raid-devices=1 --force /dev/nvme0n1p3 格式化 mkfs.ext4 -F /dev/md7 mkfs.btrfs -f /dev/md7 参考 工具集合 Tailscale "},"notes/linux/netns.html":{"url":"notes/linux/netns.html","title":"netns","keywords":"","body":"网络命名空间 netns 1、基本命令 ip netns list ip netns add test1 ip netns delete test1 ip netns exec ip a ip link add veth-test1 type veth peer name veth-test2 ip link set veth-test1 netns test1 ip link set veth-test2 netns test2 ip netns exec test1 ip addr add 192.168.1.1/24 dev veth-test1 ip netns exec test2 ip addr add 192.168.1.2/24 dev veth-test2 ip netns exec test1 ip link set dev veth-test1 up ip netns exec test2 ip link set dev veth-test2 up 2、示例 # 创建test1,test2 ip netns add test1 ip netns add test2 # 开启test1,test2 ip netns exec test1 ip link set dev lo up ip netns exec test2 ip link set dev lo up # 创建一对veth ip link add veth-test1 type veth peer name veth-test2 # 分配给test1，test2 ip link set veth-test1 netns test1 ip link set veth-test2 netns test2 # 给veth分配ip地址 ip netns exec test1 ip addr add 192.168.1.1/24 dev veth-test1 ip netns exec test2 ip addr add 192.168.1.2/24 dev veth-test2 # 启动veth ip netns exec test1 ip link set dev veth-test1 up ip netns exec test2 ip link set dev veth-test2 up # result # 在test1命名空间中可以ping通 192.168.1.2 # 在test2命名空间中可以ping通 192.168.1.1 ip netns exec test1 ping 192.168.1.2 ip netns exec test2 ping 192.168.1.1 "},"notes/linux/uboot.html":{"url":"notes/linux/uboot.html","title":"uboot","keywords":"","body":"uboot 一、环境变量配置 setenv console 'console=ttyAMA1,115200' setenv earlycon 'earlycon=pl011,0x28001000' setenv rootfs 'root=/dev/sda3' setenv ramdisk_size 'ramdisk_size=0x2000000' setenv swapcnt 'swapaccount=1' setenv cgrp_en 'cgroup_enable=memory' setenv mmap3 'memmap=103M$0x90000000' setenv mmap4 'memmap=7M$0x96000000' setenv mamp5 'memmap=1024M$0xc0000000' setenv boot_fdt 'booti 80100000 83000000 85000000' setenv load_kernel fatload scsi 0:1 0x80100000 Image setenv load_fdt fatload scsi 0:1 0x85000000 ft2004-devboard-d4-dsk.dtb setenv load_ramdisk fatload scsi 0:1 83000000 initramfs-4.19.115.img.arm64-uboot setenv bootargs ${console} ${earlycon} ${rootfs} rw quiet ${ramdisk_size} ${swapcnt} ${cgrp_en} nohugeiomap ${mmap3} ${mmap4} ${mamp5} setenv distro_bootcmd 'run load_kernel; run load_fdt; run load_ramdisk; run boot_fdt' saveenv boot 二、常用命令 scsi nvme usb scsi scan scsi part fatls scsi 0:1 22784512 Image 81836 initramfs-4.19.115.img.arm64-uboot 9190 ft2004-devboard-d4-dsk.dtb 3 file(s), 0 dir(s) "},"notes/linux/linux_network.html":{"url":"notes/linux/linux_network.html","title":"linux 网络配置","keywords":"","body":"linux 网络配置 一、netplan vim /etc/netplan/01-netcfg.yaml network: ethernets: enp1s0: dhcp4: no addresses: [192.168.0.84/24] nameservers: addresses: [8.8.8.8] routes: - to: default via: 192.168.0.1 netplan try netplan apply netplan --debug apply netplan -d apply networkctl status network: version: 2 renderer: networkd # renderer: NetworkManager ethernets: enp0s3: dhcp4: no # dhcp6: no enp4s0: dhcp4: true bonds: bond0: dhcp4: yes interfaces: - enp3s0 - enp4s0 parameters: mode: active-backup primary: enp3s0 bridges: br0: dhcp4: yes interfaces: - enp3s0 vlans: vdev: id: 101 link: net1 addresses: - 10.0.1.10/24 vprod: id: 102 link: net2 addresses: - 10.0.2.10/24 vtest: id: 103 link: net3 addresses: - 10.0.3.10/24 vmgmt: id: 104 link: net4 addresses: - 10.0.4.10/24 二、network vim /etc/network/interfaces auto enp10s0 iface enp10s0 inet static address 192.168.1.162 netmask 255.255.255.0 gateway 192.168.1.100 dns-nameservers 1.0.0.1,1.1.1.1 systemctl restart networking vim /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 DEVICE=ens33 ONBOOT=yes IPADDR=192.168.1.22 NETMASK=255.255.255.0 GATEWAY=192.168.1.1 DNS1=114.114.114.114 systemctl restart network 三、NetworkManager nmcli nmtui cat /etc/NetworkManager/system-connections/eth1 [connection] id=eth1 uuid=31b84fda-dc81-4871-a5e3-da34a74d8cc3 type=ethernet interface-name=eth1 permissions= timestamp=1701162383 [ethernet] mac-address-blacklist= [ipv4] address1=192.168.0.75/24,192.168.0.1 dns=8.8.8.8;114.114.114.114; dns-search= method=manual [ipv6] addr-gen-mode=stable-privacy dns-search= ip6-privacy=0 method=auto 四、systemd-networkd .network 文件，为匹配的设备提供一个网络配置 .netdev 文件，为匹配的环境创建一个虚拟网络设备 .link 文件，当网络设备出现时，udev 将查找第一个匹配的.link文件 [Match] Host= 主机名 Virtualization= 检查是否运行于虚拟机中 [NetDev] Name= 接口名称。必须提供 Kind= 例如：bridge, bond, vlan, veth, sit，等等。必须提供 cat 100-bind.network [Match] Name=eth0 [Network] Bridge=br0 cat 100-br0.netdev [NetDev] Name=br0 Kind=bridge cat 100-br0.network [Match] Name=br0 [Network] Address=192.168.0.21/24 Gateway=192.168.0.1 DNS=114.114.114.114 /etc/systemd/network/MyDhcp.network [DHCPv4] UseHostname=false /etc/systemd/network/MyDhcp.network [Match] Name=en* [Network] DHCP=ipv4 /etc/systemd/network/10-dhcp.network [Match] Name=enp* [Network] DHCP=yes 静态网络可以配置成 /etc/systemd/network/20-static.network [Match] Name=enp0s3 [Network] Address=192.168.0.22/24 Gateway=192.168.0.1 DNS=192.168.0.1 systemctl restart systemd-networkd mac地址冲突 persistent random none vim /lib/systemd/network/99-default.link [Match] OriginalName=* [Link] NamePolicy=keep kernel database onboard slot path AlternativeNamesPolicy=database onboard slot path MACAddressPolicy=persistent 五、网络扫描 apt install -y nmap nmap -sP 192.168.0.0/24 nmap -sS 192.168.0.0/24 六、ip命令 ip link add eth0 type dummy ip link add eth1 type dummy ip link set eth0 up ip link set eth1 up ip addr add 192.168.0.1/24 dev eth0 ip addr add 192.168.0.2/24 dev eth1 brctl addbr br0 ip link set br0 up brctl addif br0 eth0 eth1 ip addr add 192.168.100.100 dev br0 ip route add default via 192.168.0.10 dev eth0 ip rule add from all to 192.168.0.10 dev eth0 table 1 ip rule add dev eth1 table 2 ip addr del 192.168.0.1/24 dev eth0 "},"notes/linux/qemu_img.html":{"url":"notes/linux/qemu_img.html","title":"qemu-img","keywords":"","body":"qemu-img 一、常用命令 支持磁盘格式：blkdebug、blklogwrites、blkverify、bochs、cloop、compress、copy-on-read、dmg、file、host_cdrom、host_device、luks、nbd、null-aio、null-co、nvme、qcow、qcow2、qed、quorum、raw、replication、throttle、vdi、vhdx、vmdk、vpc、vvfat '-p' show progress of command 1、create qmeu-img 创建的镜像是一个稀疏文件，也就是说刚创建出来的文件并没有10G，它会随着数据的增多慢慢增加，直到10G copy-on-write qemu-img create -f raw -o /home/image/source.raw size=10G 2、convert qemu-img convert -f 源类型 -O 目标类型 源映像 输出映像 -f: 指定原镜像的格式，会自动检查可以省略 -O: 指定目标镜像格式 qemu-img convert -f raw -O vdi source.raw out.vdi 3、check 对磁盘镜像文件进行一致性检查，查找镜像文件中的错误，目前仅支持：qcow2、qed、vdi格式文件的检查 qemu-img check -f qcow2 test.qcow2 qemu-img check qcow2 test.qcow2 4、info qemu-img info test.qcow2 5、snapshot snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename “-l” 选项是查询并列出镜像文件中的所有快照 “-a snapshot”是让镜像文件使用某个快照 “-c snapshot”是创建一个快照 “-d”是删除一个快照。 *ps: raw不支持快照，只有qcow2支持快照 qemu-img snapshot -c snap1 ubuntu20.10.qcow2 qemu-img snapshot -a sanp1 ubuntu20.10.qcow2 qemu-img snapshot -l ubuntu20.10.qcow2 qemu-img snapshot -d sanp1 ubuntu20.10.qcow2 6、resize “+”和“-”分别表示增加和减少镜像文件的大小 size支持K、M、G、T等单位 缩小镜像的大小之前，需要在客户机中保证里面的文件系统有空余空间，否则会数据丢失 qcow2格式文件不支持缩小镜像的操作 qemu-img resize test.raw +2G qemu-img resize test.raw -1G parted /dev/vdb (parted) resizepart 3 End? [42.9GB]? 60GB 同步文件系统 如果使用xfs文件系统 xfs_growfs /dev/root_vg/root 如果使用ext4文件系统 resize2fs /dev/root_vg/root "},"notes/linux/systemd.html":{"url":"notes/linux/systemd.html","title":"systemd","keywords":"","body":"systemd type含义 Type=oneshot 这一选项适用于只执行一项任务、随后立即退出的服务。可能需要同时设置 RemainAfterExit=yes 使得 systemd 在服务进程退出之后仍然认为服务处于激活状态。 Type=notify 与 Type=simple 相同，但约定服务会在就绪后向 systemd 发送一个信号。这一通知的实现由 libsystemd-daemon.so 提供。 Type=dbus 若以此方式启动，当指定的 BusName 出现在DBus系统总线上时，systemd认为服务就绪。 Type=idle systemd 会等待所有任务处理完成后，才开始执行 idle 类型的单元。其他行为与 Type=simple 类似。 Type=forking systemd 认为当该服务进程fork，且父进程退出后服务启动成功。对于常规的守护进程（daemon），除非你确定此启动方式无法满足需求，使用此类型启动即可。使用此启动类型应同时指定 PIDFile=，以便 systemd 能够跟踪服务的主进程 Type=simple （默认值） systemd认为该服务将立即启动。服务进程不会 fork 。如果该服务要启动其他服务，不要使用此类型启动，除非该服务是socket 激活型。 "},"notes/linux/dd.html":{"url":"notes/linux/dd.html","title":"dd 相关","keywords":"","body":"dd 相关 一、参数 dd：用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换 if=文件名，input file of=文件名，output file ibs=bytes，一次读入bytes个字节，即指定一个块大小为bytes个字节 obs=bytes，一次输出bytes个字节，即指定一个块大小为bytes个字节 bs=bytes，同时设置读入、输出的块大小为bytes个字节 cbs=bytes，一次转换bytes个字节，即指定转换缓冲区大小 skip=blocks，从输入文件开头跳过blocks个块后再开始复制 seek=blocks，从输出文件开头跳出blocks个块后再开始复制 count=blocks，仅拷贝blocks个块，块大小等于lbs指定的字节数 conv=conversion，用指定的参数转换文件 ascii，转换ebcdic为ascii ebcdic，转换ascii为 ebcdic ibm，转换ascii为alternate ebcdic block，把每一行转换为长度为cbs，不足部分为空格填充 unblock，使每一行的长度都为cbs，不足部分用空格填充 lcase，把大写字符转换为小写字符 ucase，把小写字符转换为大写字符 swab，交换输入的每对字符 noerror，出错时不停止 notrunc，不截短输出文件 sync，将每个输入块填充到ibs个字节，不足部分用空（NUL）字符补齐 /dev/null，无底洞，可以向它输入任何数据。是一个空设备，也成为位桶（bit bucket），任何写入它的输出都会被抛弃，如果不想让消息以标准输出显示或写入文件，可以将消息重定向到位桶。 /dev/zero，是一个输入设备，可以用来初始化文件。该设备无穷尽的提供0。 二、示例 1、将本地/dev/sda整盘备份到/dev/sdb dd if=/dev/sda of=/dev/sdb 2、将/dev/sda全盘数据备份到指定路径的image文件 dd if=/dev/sda of=/root/image dd if=/dev/sda of=/root/image bs=2M status='progress' 3、将备份文件恢复到指定盘 dd if=/root/image of=/dev/sda 4、备份/dev/sda全盘数据，并利用gzip工具进行压缩，保存到指定路径 dd if=/dev/sda | gzip > /root/image.gz 5、将压缩的备份文件恢复到指定盘 gzip -dc /root/image.gz | dd of=/dev/sda 6、拷贝内存内容到硬盘 dd if=/dev/mem of=/root/mem.bin bs=1024(指定块大小为1K) 7、拷贝光盘内容到指定文件夹，并保存为cd.iso文件 dd if=/dev/cdrom(hdc) of=/root/cd.iso 8、增加swap分区文件大小 第一步：创建一个大小为256M的文件 dd if=/dev/zero of=/swapfile bs=1024 count=262144 第二步：把这个文件变成swap文件 mkswap /swapfile 第三步：启用这个swap文件 swapon /swapfile 第四步：编辑/etc/fstab文件 /swapfile swap swap default 0 0 9、销毁磁盘数据 dd if=/dev/urandom of=/dev/sda 10、测试磁盘的读写速度 dd if=/dev/zero bs=1024 count=1000000 of=/root/1GB.file dd if=/dev/1GB.file bs=64K | dd of=/dev/null 11、确定磁盘的最佳块大小 dd if=/dev/zero bs=1024 count=1000000 of=/root/1GB.file dd if=/dev/zero bs=2048 count=500000 of=/root/1GB.file dd if=/dev/zero bs=4096 count=250000 of=/root/1GB.file dd if=/dev/zero bs=8192 count=125000 of=/root/1GB.file 12、修复磁盘 当硬盘较长时间(一年以上)放置不使用后，磁盘上会产生magnetic flux point，当磁头读到这些区域时会遇到困难，并可能导致I/O错误。当这种情况影响到硬盘的第一个扇区时，可能导致硬盘报废。上边的命令有可能使这些数 据起死回生。并且这个过程是安全、高效的。 dd if=/dev/sda of=/dev/sda 13、禁止标准输出 cat $filename > /dev/null 14、禁止标准错误 rm $badname 2>/dev/null 15、禁止标准输出和标准错误输出 cat $filename 2>/dev/null >/dev/null #cat $filename &>/dev/null 16、自动清空日志文件内容 cat /dev/null > /var/log/messages # :>/var/log/messages 有同样的效果，但不会产生新的进程（因为:是内建的） "},"notes/linux/kernel_make.html":{"url":"notes/linux/kernel_make.html","title":"kernel 内核编译","keywords":"","body":"kernel 内核编译 一、构建交叉编译容器 x86_64_aarch64 Dockerfile From ubuntu:22.04 ARG http_proxy=http://192.168.0.55:1080 ARG https_proxy=$http_proxy ARG no_proxy=$no_proxy ENV http_proxy=$http_proxy ENV https_proxy=$https_proxy ENV no_proxy=$no_proxy ENV PATH=$PATH:/opt/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu/bin ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu/lib WORKDIR /opt RUN sed -i s@/archive.ubuntu.com/@/mirrors.ustc.edu.cn/@g /etc/apt/sources.list && \\ apt update && \\ DEBIAN_FRONTEND=noninteractive apt install --no-install-recommends --no-install-suggests -y dracut git make libncurses-dev libelf-dev bison flex libssl-dev bc u-boot-tools vim wget xz-utils && \\ rm -rf /var/lib/apt/lists/* && \\ ln -s /usr/bin/python3 /usr/bin/python && \\ wget --no-check-certificate https://releases.linaro.org/components/toolchain/binaries/latest-7/aarch64-linux-gnu/gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu.tar.xz && \\ tar -xf gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu.tar.xz && \\ rm -rf gcc-linaro-7.5.0-2019.12-x86_64_aarch64-linux-gnu.tar.xz && \\ alias make='make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu-' docker build -t gcc-linaro-7.5.0:1.0.0 . 二、容器内编译内核 1、直接编译 docker run -it --rm --privileged -v $PWD/../:/kernel -w /kernel /gcc-linaro-7.5.0:1.0.0 /bin/sh -c \"sh 2、编译boot.img make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- rockchip_linux_defconfig cp arch/arm64/configs/rockchip_linux_defconfig .config make menuconfig make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- rk3399-firefly-linux.img -j $(nproc) mkdir -p ../out/ cp boot.img ../out/ 3、检查内核是否支持容器相关功能 wget https://raw.githubusercontent.com/moby/moby/master/contrib/check-config.sh chmod +x check-config.sh ./check-config.sh .config 三、安装依赖 yum install -y make ncurses-devel elfutils-libelf-devel bison flex openssl-devel bc gcc-c++ gcc 1、gcc 源码编译 wget https://mirrors.ustc.edu.cn/gnu/gcc/gcc-7.3.0/gcc-7.3.0.tar.gz mkdir -p /usr/local/gcc tar -xvf /hl/gcc-7.3.0.tar.gz -C /usr/local/gcc/ cd /usr/local/gcc/gcc-7.3.0/ ./contrib/download_prerequisites HTTPS_PROXY=192.168.0.103:1080 ./contrib/download_prerequisites mkdir gcc-build-7.3.0 cd gcc-build-7.3.0/ yum install -y gmp-devel mpfr-devel libmpc-devel ../configure --enable-languages=c,c++ --disable-multilib make -j8 make install -j8 rpm -q gcc rpm -q gcc-c++ rpm -e gcc-4.8.5-28.el7_5.1.aarch64 rpm -e gcc-c++-4.8.5-28.el7_5.1.aarch64 2、下载源码 yumdownloader --source kernel apt install linux-source-4.18.0 cd /usr/src/linux-source-4.18.0 四、进入内核源码并编译 wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.17.11.tar.xz wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.4.50.tar.xz cd /linux-4.19.37-rt/linux-4.19.37-rt/ #清理环境 #make mrproper #make clean cp /boot/config-5.4.0-66-generic ./.config make menuconfig make -j8 make modules_install -j8 make install -j8 #cp /linux-4.19.37-rt/linux-4.19.37-rt/arch/arm64/boot/Image.gz /boot/vmlinuz-4.19.37-rt19.arm64 #cd /lib/modules # 分析可加载模块的依赖性, 更新模块依赖 # depmod -a #initramfs-4.19.37-rt19.img #cd /boot #dracut --kver 4.19.37-rt19 #lsinitrd initramfs-4.19.37-rt19.img | grep nvme #dracut --kver 4.19.37-rt19 --add-drivers \"nvme nvme_core\" #cp ./kernel_resource/config-4.19.37-rt19.arm64 /boot/config-4.19.37-rt19 #update-initramfs -c -k 5.4.50 #update-grub vim /boot/efi/EFI/centos/grub.cfg menuentry 'DeltaLinux LFS 3.19.37 (aarch64)' --class centos --class gnu-linux --class gnu --class os --unrestricted { load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod xfs set root='hd0,gpt2' if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root --hint-ieee1275='ieee1275//disk@0,gpt2' --hint-bios=hd0,gpt2 --hint-efi=hd0,gpt2 --hint-baremetal=ahci0,gpt2 ccd31679-aeab-4dcf-a6bf-f2128b8b37d6 else search --no-floppy --fs-uuid --set=root ccd31679-aeab-4dcf-a6bf-f2128b8b37d6 fi linux /vmlinuz-4.19.37 root=/dev/mapper/centos-root ro crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap LANG=en_US.UTF-8 initrd /initramfs-4.19.37.img } update-ca-trust force-enable update-ca-trust extract openssl s_client -showcerts -connect github.com:443 /dev/null|openssl x509 -outform PEM >mycertfile.pem cp mycertfile.pem /etc/pki/ca-trust/source/anchors/ update-ca-trust extract objdump -tT libatlasutil.so make config 基于文本模式的交互配置 make menuconfig 基于文本模式的菜单模式 make oldconfig make defconfig make localmodconfig make clean make distclean 五、内核操作 lsmod modprobe vxlan insmod ./kernel/drivers/net/vxlan.ko modinfo ./kernel/drivers/net/vxlan.ko 六、内核镜像操作 mv initramfs-4.19.37-rt19.img.arm64 initramfs-4.19.37-rt19.img.arm64.gz gzip -d initramfs-4.19.37-rt19.img.arm64.gz cpio -id initrd.img mksquashfs /mnt /root/install.img –all-root -noF unsquashfs squashfs.img 七、FAQ chromium Kernel Features -> Page size -> 4KB atlas VDEV ARM64_VA_BITS_48 > CONFIG_ARM64_PAGE_SHIFT=16 > CONFIG_ARM64_CONT_SHIFT=5 > CONFIG_ARCH_MMAP_RND_BITS_MIN=14 > CONFIG_ARCH_MMAP_RND_BITS_MAX=29 > CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MIN=7 > CONFIG_ARCH_MMAP_RND_COMPAT_BITS_MAX=16 vim arch/arm64/Kconfig │ Symbol: ARM64_VA_BITS_48 [=y] │ │ Type : bool │ │ Prompt: 48-bit │ │ Location: │ │ -> Kernel Features │ │ (1) -> Virtual address space size ( [=y]) │ │ Defined at arch/arm64/Kconfig:658 │ │ Depends on: "},"notes/nodejs.html":{"url":"notes/nodejs.html","title":"nodejs 相关","keywords":"","body":"nodejs 相关 download 1、安装nodejs curl -LO https://nodejs.org/dist/v21.7.3/node-v21.7.3-linux-x64.tar.xz or wget https://nodejs.org/dist/v21.7.3/node-v21.7.3-linux-x64.tar.xz tar -xvf node-v21.7.3-linux-x64.tar.xz -C /usr/local/ ln -s /usr/local/node-v21.7.3-linux-x64/bin/npm /usr/local/bin/ ln -s /usr/local/node-v21.7.31-linux-x64/bin/node /usr/local/bin/ 2、查看版本 node --version 3、使用淘宝npm源 npm install -g cnpm --registry=https://registry.npmmirror.com cnpm install SOMEPAKAGE 4、默认安装目录 /usr/local/node/0.10.24/lib/node_modules/ 5、使用国内镜像加速npm npm config set registry=https://registry.npmmirror.com 6、使用国内镜像加速yarn yarn config set registry https://registry.npmmirror.com 7、使用国内源安装 npm install -g cnpm --registry=https://registry.npm.taobao.org 8、安装 apidoc npm install apidoc -g echo -e \"export PATH=$(npm prefix -g)/bin:$PATH\" >> ~/.bashrc && source ~/.bashrc 9、其他 npm config list npm config set registry https://registry.npmjs.org npm config delete registry nvm install 10.14.1 nvm use 10.14.1 npm cache clean --force "},"notes/linux/unixbench.html":{"url":"notes/linux/unixbench.html","title":"UnixBench 虚拟机性能测试工具","keywords":"","body":"UnixBench 虚拟机性能测试工具 unixbench是一个用于测试unix系统性能的工具，也是一个比较通用的benchmark， 此测试的目的是对类Unix 系统提供一个基本的性能指示，很多测试用于系统性能的不同方面，这些测试的结果是一个指数值（index value，如520），这个值是测试系统的测试结果与一个基线系统测试结果比较得到的指数值，这样比原始值更容易得到参考价值，测试集合里面所有的测试得到的指数值结合起来得到整个系统的指数值。 各项的测试有得分，然后有一个综合的得分，这样可以很方便的通过分数去比较。 1、下载 wget http://soft.laozuo.org/scripts/UnixBench5.1.3.tgz # wget https://s3.amazonaws.com/cloudbench/software/UnixBench5.1.3.tgz 2、解压 tar -zxvf UnixBench5.1.3.tgz 3、编译 cd UnixBench/ make 4、运行 ./Run 5、测试项 测试项目 项目说明 基准线 Dhrystone 2 using register variables 测试 string handling 116700.0lps Double-Precision Whetstone 测试浮点数操作的速度和效率 55.0MWIPS Execl Throughput 此测试考察每秒钟可以执行的 execl 系统调用的次数 43.0lps File Copy 1024 bufsize 2000 maxblocks 测试从一个文件向另外一个文件传输数据的速率 3960.0KBps File Copy 256 bufsize 500 maxblocks 测试从一个文件向另外一个文件传输数据的速率。 1655.0KBps File Read 4096 bufsize 8000 maxblocks 测试从一个文件向另外一个文件传输数据的速率。 5800.0KBps Pipe-based Context Switching 测试两个进程（每秒钟）通过一个管道交换一个不断增长的整数的次数 12440.0lps Pipe Throughput 一秒钟内一个进程可以向一个管道写 512 字节数据然后再读回的次数 4000.0lps Process Creation 测试每秒钟一个进程可以创建子进程然后收回子进程的次数（子进程一定立即退出）。 126.0lps Shell Scripts (8 concurrent) 测试一秒钟内一个进程可以并发地开始一个shell 脚本的 n 个拷贝的次数，n 一般取值1，2，4，8. 42.4lpm System Call Overhead 测试进入和离开操作系统内核的代价，即一次系统调用的代价。 6.0lpm "},"notes/linux/pypi_local_source.html":{"url":"notes/linux/pypi_local_source.html","title":"pip 本地源","keywords":"","body":"pip 本地源 1、安装 pip2pi 工具 pip install pip2pi 2、建立索引，会创建 simple 文件夹 dir2pi path 3、更新索引 多个包: pip2acmeco -r requirements.txt 单个包: pip2acmeco package==1.0.0 4、同步软件包 1、创建目录 mkdir /work/pypi/Packages/ 2、同步单个软件包 pip2tgz /work/pypi/Packages requests 3、批量同步 pip2tgz /work/pypi/Packages -r ./requirements.txt 4、创建索引 dir2pi /work/pypi/Packages/ "},"notes/linux/lvm.html":{"url":"notes/linux/lvm.html","title":"LVM 逻辑卷相关","keywords":"","body":"LVM 逻辑卷相关 一、常用命令 1、查看 pvdisplay vgdisplay lvdisplay pvscan/pvs vgscan/vgs lvscan/lvs 2、创建 pvcreate /dev/xvdb4 vgcreate myVG /dev/xvdb4 lvcreate -l 100%FREE -n myLV myVG lvcreate -l 100%free -n myLV myVG lvcreate -L +120G -n myLV myVG 3、删除 lvremove /dev/myVG/myLV vgremove /dev/myVG pvremove /dev/xvdb4 4、增加 vg 大小 vgextend myVG /dev/xvdb5 5、减小 vg 大小 vgreduce myVG /dev/xvdb5 6、增加逻辑卷的大小 lvextend -l 100%FREE /dev/myVG/myLV lvextend -L +120G /dev/mapper/centos-root 7、减小逻辑卷的大小 lvreduce -L 1G /dev/myVG/myLV lvreduce -l -256 /dev/myVG/myLV 8、调整卷大小，可增可减 lvresize -L 40G /dev/myVG/myLV 9、刷新挂载点 resize2fs /dev/myVG/myLV 10、链接目录 ln -sf /dev/myVG/myLV /home/hl 11、查看逻辑卷 id blkid /dev/myVG/myLV 12、格式化逻辑卷 mkfs -t ext4 /dev/myVG/myLV 13、扩展根目录卷 pvcreate /dev/sdb vgextend cenots /dev/sdb lvextend -l +100%FREE /dev/centos/root xfs_growfs /dev/centos/root 14、将 home 目录空间扩展到根目录 umount /home/ lvremove /dev/mapper/centos-home lvextend -L +120G /dev/mapper/centos-root # lvextend -l 100%FREE /dev/mapper/centos-root xfs_growfs /dev/mapper/centos-root lvcreate -L 47G -n home centos mkfs.xfs /dev/mapper/centos-home mount /dev/mapper/centos-home /home 二、LVM 简介 LVM是 Logical Volume Manager(逻辑卷管理)的简写。LVM将一个或多个硬盘的分区在逻辑上集合，相当于一个大硬盘来使用，当硬盘的空间不够使用的时候，可以继续将其它的硬盘的分区加入其中，这样可以实现磁盘空间的动态管理，相对于普通的磁盘分区有很大的灵活性。 LVM是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。 三、 LVM 基本术语 物理存储介质（The physical media）：这里指系统的存储设备：硬盘，如：/dev/hda1、/dev/sda等等，是存储系统最低层的存储单元。 物理卷（physical volume）：物理卷就是指硬盘分区或从逻辑上与磁盘分区具有同样功能的设备(如RAID)，是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数。 卷组（Volume Group）：LVM卷组类似于非LVM系统中的物理硬盘，其由物理卷组成。可以在卷组上创建一个或多个“LVM分区”（逻辑卷），LVM卷组由一个或多个物理卷组成。 逻辑卷（logical volume）：LVM的逻辑卷类似于非LVM系统中的硬盘分区，在逻辑卷之上可以建立文件系统(比如/home或者/usr等)。 PE（physical extent）：每一个物理卷被划分为称为PE(Physical Extents)的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可配置的，默认为4MB。 LE（logical extent）：逻辑卷也被划分为被称为LE(Logical Extents) 的可被寻址的基本单位。在同一个卷组中，LE的大小和PE是相同的，并且一一对应。 简单来说就是： PV：是物理的磁盘分区 VG：LVM中的物理的磁盘分区，也就是PV，必须加入VG，可以将VG理解为一个仓库或者是几个大的硬盘。 LV：也就是从VG中划分的逻辑分区 四、 安装 LVM 首先确定系统中是否安装了lvm工具：rpm –qa|grep lvm 1、 创建分区 使用分区工具（如：fdisk等）创建LVM分区，方法和创建其他一般分区的方式是一样的，区别仅仅是LVM的分区类型为8e。 fdisk /dev/xvdb n 新建分区 p 分区类型为主分区 4 enter enter t 修改分区格式 8e 类型改为8e LVM p 查看当前分区 w 写入分区 partprobe 使分区表生效，无需重启 pvcreate /dev/xvdb4 使用pvcreate转换 pvdisplay 查看已经存在的pv vgcreate myVG /dev/xvdb4 创建VG，可利用已经存在的VG名（myVG），同一VG名下的一组PV构成一个VG vgdisplay 查看VG 创建完成VG之后，才能从VG中划分一个LV lvcreate -l 100%FREE -n myLV myVG 创建LV，并把VG所有剩余空间分给LV lvdisplay 显示LV的信息 mkfs -t ext3 /dev/myVG/myLV 对LV进行格式化（使用mksf进行格式化操作），然后LV才能存储资料 mkdir /home/hl mount /dev/myVG/myLV /home/hl df -h 检查linux服务器的文件系统的磁盘空间占用情况 vim /etc/fstab blkid /dev/myVG/myLV echo 'UUID=a5d3a67a-aad4-4eea-91ce-1b1fc56ffe9f /home/hl ext4 defaults 0 0' >/etc/fstab mount -a 2、开机挂载及 /etc/fstab 格式 当我们在挂载磁盘的时候，除了利用磁盘的代号之外 (/dev/hdxx) 也可以直接利用磁盘的 label 来作为挂载的磁盘挂载点喔！基本上， 就是那个 /etc/fstab 档案的设定,Label 来做为磁盘挂载的依据， 这样有好有坏： 优点：不论硬盘代号怎么变，不论您将硬盘插在那个 IDE 接口 (IDE1 或 IDE2 或 master 或 slave 等)，由于系统是透过 Label ，所以，磁盘插在那个接口将不会有影响。 缺点：如果插了两颗硬盘，刚好两颗硬盘的 Label 有重复的，那就惨了～ 因为系统会无法判断那个磁盘分割槽才是正确的！ 开机挂载 /etc/fstab 及 /etc/mtab： 系统挂载的一些限制： 根目录 / 是必须挂载的,而且一定要先于其它 mount point 被挂载进来。 其它 mount point 必须为已建立的目录,可任意指定,但一定要遵守必须的系统目录架构原则 所有 mount point 在同一时间之内,只能挂载一次。 所有 partition 在同一时间之内,只能挂载一次。 如若进行卸载,您必须先将工作目录移到 mount point(及其子目录) 之外。 [root@linux ~]# cat /etc/fstab # Device Mount_point filesystem parameters dump fsck LABEL=/ / ext3 defaults 1 1 （以标头名称挂载） /dev/hda5 /home ext3 defaults 1 2 /dev/hda3 swap swap defaults 0 0 /dev/hdc /media/cdrom auto pamconsole,exec,noauto,managed 0 0 /dev/devpts /dev/pts devpts gid=5,mode=620 0 0 /dev/shm /dev/shm tmpfs defaults 0 0 /dev/proc /proc proc defaults 0 0 /dev/sys /sys sysfs defaults 0 0 其实这个 /etc/fstab 就是将我们使用 mount 来挂载一个装置到系统的某个挂载点， 所需要下达的指令内容，将这些内容通通写到 /etc/fstab 里面去，而让系统一开机就主动挂载。 那么 mount 下达指令时，需要哪些参数？不就是『装置代号、挂载点、档案系统类别、参数』等等， 而我们的 /etc/fstab 则加入了两项额外的功能，分别是备份指令 dump 的执行与否， 与是否开机进行 fsck 扫瞄磁盘。 前面的4个已经很熟悉了，每个档案系统还有很多参数可以加入的，例如中文编码的 iocharset=big5,codepage=950 之类的，当然还有很多常见的参数，具体可以看mount中的详细介绍，具体说一下后2个：dump和fsck。 能否被 dump 备份指令作用： 在 Linux 当中，可以利用 dump 这个指令来进行系统的备份的。而 dump 指令则会针对 /etc/fstab 的设定值，去选择是否要将该 partition 进行备份的动作呢！ 0 代表不要做 dump 备份， 1 代表要进行 dump 的动作。 2 也代表要做 dump 备份动作， 不过，该 partition 重要度比 1 小。 是否以 fsck 检验扇区： 开机的过程中，系统预设会以 fsck 检验我们的 partition 内的 filesystem 是否完整 (clean)。 不过，某些 filesystem 是不需要检验的，例如虚拟内存 swap ，或者是特殊档案系统， 例如 /proc 与 /sys 等等。所以，在这个字段中，我们可以设定是否要以 fsck 检验该 filesystem 喔。 0 是不要检验， 1 是要检验， 2 也是要检验，不过 1 会比较早被检验啦！ 一般来说，根目录设定为 1 ，其它的要检验的 filesystem 都设定为 2 就好了。 3、其他操作 1、增加逻辑卷的大小 lvextend -l 100%FREE /dev/myVG/myLV 2、减小逻辑卷的大小 lvreduce -L 1G /dev/myVG/myLV lvreduce -l -256 /dev/myVG/myLV 3、删除逻辑卷 lvremove /dev/myVG/myLV 4、删除物理卷 vgremove /dev/myVG 5、链接目录 ln -s /dev/myVG/myLV /home/hl 6、取消文件夹的绑定 umount /home/hl 4、扩展根目录逻辑卷 pvcreate /dev/sdb vgextend cenots /dev/sdb lvextend -l +100%FREE /dev/centos/root xfs_growfs /dev/centos/root "},"notes/linux/tcpdump.html":{"url":"notes/linux/tcpdump.html","title":"tcpdump 相关","keywords":"","body":"tcpdump 相关 一、基础概念 tcpdump [option] [proto] [direction] [type] proto: ip, ip6, arp, rarp, atalk, aarp, decnet, sca, lat, mopdl, moprc, iso, stp, ipx, netbeui direction：src, dst type：host, net, port, portrange 1、类型 host 192.168.201.128 net 128.3 port 20 portrange 6000-6008 2、目标 src dst src or dst src and dst 3、协议 tcp udp icmp 4、操作符 and / && or / || not / ! = == != 5、关键字接口 if：表示网卡接口名、 proc：表示进程名 pid：表示进程 id svc：表示 service class dir：表示方向，in 和 out eproc：表示 effective process name epid：表示 effective process ID tcpdump \"( if=en0 and proc =nc ) || (if != en0 and dir=in)\" 6、输出内容结构 21:26:49.013621 IP 172.20.20.1.15605 > 172.20.20.2.5920: Flags [P.], seq 49:97, ack 106048, win 4723, length 48 时分秒毫秒 21:26:49.013621 网络协议 IP 发送方的ip地址+端口号，其中172.20.20.1是 ip，而15605 是端口号 箭头 >， 表示数据流向 接收方的ip地址+端口号，其中 172.20.20.2 是 ip，而5920 是端口号 冒号 数据包内容，包括Flags 标识符，seq 号，ack 号，win 窗口，数据长度 length，其中 [P.] 表示 PUSH 标志位为 1 7、Flags标识符 [S] : SYN（开始连接） [P] : PSH（推送数据） [F] : FIN （结束连接） [R] : RST（重置连接） [.] : 没有 Flag，由于除了 SYN 包外所有的数据包都有ACK，所以一般这个标志也可表示 ACK 8、可选参数 1、设置不解析域名提升速度 -n：不把ip转化成域名，直接显示 ip，避免执行 DNS lookups 的过程，速度会快很多 -nn：不把协议和端口号转化成名字，速度也会快很多。 -N：不打印出host 的域名部分.。比如,，如果设置了此选现，tcpdump 将会打印'nic' 而不是 'nic.ddn.mil'. 2、过滤结果输出到文件 tcpdump icmp -w icmp.pcap 3、从文件中读取数据包 tcpdump icmp -r all.pcap 4、详细输出 -v：产生详细的输出. 比如包的TTL，id标识，数据包长度，以及IP包的一些选项。同时它还会打开一些附加的包完整性检测，比如对IP或ICMP包头部的校验和。 -vv：产生比-v更详细的输出. 比如NFS回应包中的附加域将会被打印, SMB数据包也会被完全解码。 -vvv：产生比-vv更详细的输出。比如 telent 时所使用的SB, SE 选项将会被打印, 如果telnet同时使用的是图形界面，其相应的图形选项将会以16进制的方式打印出来。 5、时间显示 -t：在每行的输出中不输出时间 -tt：在每行的输出中会输出时间戳 -ttt：输出每两行打印的时间间隔(以毫秒为单位) -tttt：在每行打印的时间戳之前添加日期的打印（此种选项，输出的时间最直观） 6、显示数据包的头部 -x：以16进制的形式打印每个包的头部数据（但不包括数据链路层的头部） -xx：以16进制的形式打印每个包的头部数据（包括数据链路层的头部） -X：以16进制和 ASCII码形式打印出每个包的数据(但不包括连接层的头部)，这在分析一些新协议的数据包很方便。 -XX：以16进制和 ASCII码形式打印出每个包的数据(包括连接层的头部)，这在分析一些新协议的数据包很方便。 7、过滤特定流向的数据包 -Q：选择是入方向还是出方向的数据包，可选项有：in, out, inout，也可以使用 --direction=[direction] 这种写法 8、对输出内容进行控制 -D : 显示所有可用网络接口的列表 -e : 每行的打印输出中将包括数据包的数据链路层头部信息 -E : 揭秘IPSEC数据 -L ：列出指定网络接口所支持的数据链路层的类型后退出 -Z：后接用户名，在抓包时会受到权限的限制。如果以root用户启动tcpdump，tcpdump将会有超级用户权限。 -d：打印出易读的包匹配码 -dd：以C语言的形式打印出包匹配码. -ddd：以十进制数的形式打印出包匹配码 9、其他 -A：以ASCII码方式显示每一个数据包(不显示链路层头部信息). 在抓取包含网页数据的数据包时, 可方便查看数据 -l : 基于行的输出，便于你保存查看，或者交给其它工具分析 -q : 简洁地打印输出。即打印很少的协议相关信息, 从而输出行都比较简短. -c : 捕获 count 个包 tcpdump 就退出 -s : tcpdump 默认只会截取前 96 字节的内容，要想截取所有的报文内容，可以使用 -s number， number 就是你要截取的报文字节数，如果是 0 的话，表示截取报文全部内容。 -S : 使用绝对序列号，而不是相对序列号 -C：file-size，tcpdump 在把原始数据包直接保存到文件中之前, 检查此文件大小是否超过file-size. 如果超过了, 将关闭此文件,另创一个文件继续用于原始数据包的记录. 新创建的文件名与-w 选项指定的文件名一致, 但文件名后多了一个数字.该数字会从1开始随着新创建文件的增多而增加. file-size的单位是百万字节(nt: 这里指1,000,000个字节,并非1,048,576个字节, 后者是以1024字节为1k, 1024k字节为1M计算所得, 即1M=1024 ＊ 1024 ＝ 1,048,576) -F：使用file 文件作为过滤条件表达式的输入, 此时命令行上的输入将被忽略. 二、示例 1、基于网卡、源IP地址过滤 tcpdump -i eth2 src 192.168.1.127 所有网卡可用 -i any 2、基于源、网段过滤 tcpdump net 192.168.1.0/24 tcpdump src net 192.168 3、基于源、端口过滤 tcpdump src port 80 or 8088 tcpdump src portrange 8000-8080 4、基于协议过滤 ip, ip6, arp, rarp, atalk, aarp, decnet, sca, lat, mopdl, moprc, iso, stp, ipx, netbeui tcpdump icmp 5、基于IP协议的版本进行过滤 6 表示tcp在ip报文中的编号 ipv4 tcpdump 'ip proto tcp' tcpdump ip proto 6 tcpdump 'ip protochain tcp' tcpdump ip protochain 6 ipv6 tcpdump 'ip6 proto tcp' tcpdump ip6 proto 6 tcpdump 'ip6 protochain tcp' tcpdump ip6 protochain 6 6、基于包大小过滤 tcpdump less 32 tcpdump greater 64 tcpdump 7、基于mac地址过滤 tcpdump ether host [ehost] tcpdump ether dst [ehost] tcpdump ether src [ehost] ehost为记录在/etc/ethers里的name 8、过滤通过指定网关的数据包 tcpdump gateway [host] 9、过滤广播/多播数据包 tcpdump ether broadcast tcpdump ether multicast tcpdump ip broadcast tcpdump ip multicast tcpdump ip6 multicast 10、其他 tcpdump -i ens33 port 8080 and host node1 只抓192.168网段 10个包 tcpdump -i ens33 -c 10 net 192.168 tcpdump -c 5 -nn -i eth0 icmp and src 192.168.100.62 解析包数据 tcpdump -c 2 -q -XX -vvv -nn -i ens33 tcp dst port 22 tcpdump -i eth0 tcp port 80 and host 192.168.0.129 "},"notes/linux/libvirt.html":{"url":"notes/linux/libvirt.html","title":"libvirt 相关","keywords":"","body":"libvirt 相关 一、安装相关 1、安装libvrit-python yum install -y libvirt-devel gcc-c++ qemu-kvm pip install libvirt-python # 测试 python -c 'import libvirt' 2、安装libvirt客户端 yum install -y libvirt-client # 测试 virsh list 二、libvirt python api连接 libvirt connections 1、本地连接 import sys import libvirt conn = libvirt.open('qemu:///system') if conn == None: print('Failed to open connection to qemu:///system', file=sys.stderr) exit(1) conn.close() exit(0) 2、远程tcp免密连接 1、修改 /etc/libvirt/libvirtd.conf listen_tls = 0 listen_tcp = 1 listen_addr = \"0.0.0.0\" auth_tcp = \"none\" 2、修改/etc/sysconfig/libvirtd，注释掉LIBVIRTD_ARGS=\"--listen\" #LIBVIRTD_ARGS=\"--listen\" 3、重启libvirtd systemctl restart libvirtd 4、查看进程和端口 ps aux | grep libvirtd root 17419 0.0 0.0 2494136 50312 ? Ssl Oct22 0:01 /usr/sbin/libvirtd --listen netstat -lntp | grep libvirtd tcp 0 0 0.0.0.0:16509 0.0.0.0:* LISTEN 17419/libvirtd 5、连接测试 python import libvirt conn = libvirt.open(\"qemu+tcp://192.168.0.1/system\") conn.listDefinedDomains() conn.listDomainsID() 三、隔离 CPU（isolcpus） isolcpus=2,3，禁止普通进程运行在编号为2,3的cpu上 1、修改配置 /etc/default/grub GRUB_CMDLINE_LINUX=\"... isolcpus=2,3\" # 使生效 grub2-mkconfig -o /boot/grub2/grub.cfg 或 df -h ... /dev/sdb2 1014M 72M 943M 8% /boot /dev/sdb1 200M 12M 189M 6% /boot/efi ... lscpu ... CPU(s): 6 On-line CPU(s) list: 0-5 ... cat /boot/efi/EFI/centos/grub.cfg ... linuxefi /vmlinuz-4.19.31-rt18 root=/dev/mapper/deltaos-root ro crashkernel=auto rd.lvm.lv=deltaos/root rd.lvm.lv=deltaos/swap rhgb quiet LANG=en_US.UTF-8 isolcpus=2,3 ... 2、查看 cat /boot/grub2/grub.cfg |grep isolcpus 或 cat /boot/efi/EFI/centos/grub.cfg |grep isolcpus 3、重启服务器，并查看 [root@dao8 ~]# ps -eLo psr|grep 0|wc -l 74 [root@dao8 ~]# ps -eLo psr|grep 3|wc -l 12 具体查看这12个进程 [root@dao8 ~]# ps -eLo psr,ruser,pid,lwp,args|awk '{if($1==2)print $0}' 2 root 15 15 [kswork] 2 root 28 28 [cpuhp/2] 2 root 29 29 [migration/2] 2 root 30 30 [posixcputmr/2] 2 root 31 31 [rcuc/2] 2 root 32 32 [ktimersoftd/2] 2 root 33 33 [ksoftirqd/2] 2 root 35 35 [kworker/2:0H-xf] 2 root 62 62 [rcu_tasks_kthre] 2 root 63 63 [kauditd] 2 root 67 67 [writeback] 2 root 78 78 [ata_sff] 2 root 82 82 [kworker/u13:0] 2 root 85 85 [nfsiod] 2 root 96 96 [user_dlm] 2 root 135 135 [irq/120-aerdrv] 2 root 137 137 [i915/signal:0] 2 root 138 138 [i915/signal:1] 2 root 139 139 [i915/signal:2] 2 root 140 140 [i915/signal:6] 2 root 141 141 [drbd-reissue] 2 root 146 146 [fc_rport_eq] 2 root 148 148 [fnic_fip_q] 2 root 152 152 [bnx2i_thread/2] 2 root 157 157 [scsi_eh_0] 2 root 158 158 [scsi_tmf_0] 2 root 159 159 [nvme-wq] 2 root 167 167 [scsi_eh_2] 2 root 169 169 [scsi_eh_3] 2 root 170 170 [scsi_tmf_3] 2 root 171 171 [scsi_eh_4] 其中：migration用于进程在不通cpu间迁移,两个kworker用户处理workqueues，ksoftirqd用户调度CPU软中断的进程，也就说没有普通进程，说明CPU隔离生效了 4、把某线程固定到某CPU上 taskset -p 0x4 3497 注：0x4二进制是0100，可知是CPU2，3497是线程号lwp 如果是0x3，即0011，那么可能绑定到cpu0或cpu1 "},"notes/linux/linux.html":{"url":"notes/linux/linux.html","title":"Linux 相关","keywords":"","body":"Linux 相关 usual ip addr add 192.168.1.100/24 dev eth0 ip addr del 192.168.1.100/24 dev eth0 ssh-keygen -R 192.168.1.100 echo \"root:123\" | chpasswd kubectl completion bash # 使用 openssl 命令来验证 ca.crt 是否有效 openssl x509 -in /etc/nginx/ssl/server.crt -noout -dates pgrep -a python ps -efww|grep python ss -tunl netstat -ntlp base64 -d /hl/ca.base64 > /hl/ca.crt cat /hl/token|tr -d '\\n' > token1 Skills # 清空文件 > access.log # 生成大文件 dd if=/dev/zero of=file.img bs=1M count=1024 # 安全擦除硬盘 dd if=/dev/urandom of=/dev/sda # 快速制作系统盘，/dev/sdb U盘 dd if=ubuntu-server-amd64.iso of=/dev/sdb # 查看某个进程运行时间 ps -p 10167 -o etimes,etime # 获取进程内存信息 ps -p 10167 -o rss # 时间戳快速转换 date -d@1234567890 +\"%Y-%m-%d %H:%M:%S\" # 查看当前时间戳 date +%s # 删除乱码文件 $ ls -i 138957 a.txt 138959 T.txt 132395 ڹ��.txt $ find . -inum 132395 -exec rm {} \\; # 获取公网IP curl ip.sb curl ifconfig.me # 批量下载网页资源 wget -r -nd -np --accept=pdf http://fast.dpdk.org/doc/pdf-guides/ wget -r -np -nH -R index.html http://url/including/files/you/want/to/download/ -r 遍历所有子目录 -np 不到上一层子目录去 -nH 不要讲文件保存到主机名文件 -R index.html 不下载index.html文件 # 开机启动 1. /etc/rc.local chmod +xxx /etc/rc.local 2. # Crontab 可以使用 @reboot 来执行主机启动之后的命令。 crontab -e @reboot /root/script/restart.sh 3. vim /lib/systemd/system/restart.service [Unit] Description=restart After=default.target [Service] ExecStart=/root/script/restart.sh [Install] WantedBy=default.target 命令 一、pigz 1、安装 apt-get -y install pigz 2、压缩 tar cvf - 目录名 | pigz -9 -p 24 > file.tgz #pigz：用法-9是压缩比率比较大，-p是指定cpu的核数。 tar cvf - /opt/hadoop-2.7.3.tar.gz | pigz -9 -p 4 >hadoop-2.7.3.tgz 3、解压 pigz -d file.tgz tar -xf file #tar -xf --format=posix file 二 、sed 1、ip地址匹配替换 sed -i \"s/presenter_server_ip=[0-9.]*/presenter_server_ip=${presenter_connect_ip}/g\" ${cur_path}/presenterserver/${app_name}/config/config.conf 2、多段替换 sed -i -e \"s#apk -U#sed -i 's+https://dl-cdn.alpinelinux.org+${NGINX}/alpine+g' /etc/apk/repositories \\&\\& apk -U#g\" \\ -e \"s#https://github.com/aquasecurity/trivy/releases/download#${NGINX}/kymoc_build/trivy#g\" Dockerfile.dapper 3、匹配行前插入 sed -i '/# Usage:/i\\\\\\cp -rf kymoc /usr/local/bin/kymoc' test 4、匹配行后追加 sed -i '/# Usage:/a\\INSTALL_KYMOC_SKIP_DOWNLOAD=true' test 5、最后一行后添加 sed '$ahello' 1.txt 6、最后一行前插入 sed '$ihello' 1.txt 7、删除文中行 sed '4d' 1.txt #删除第四行 8、删除匹配行 sed '/123/d' 1.txt #删除匹配123的行 9、删除空行 sed '/^$/d' 1.txt #删除空行 10、获取磁盘大小 lsblk -ndb -o SIZE /dev/sda blockdev --getsize64 /dev/sda 11、shell 编译成二进制 apt install -y shc gcc shc -f a.sh gcc -static a.sh.x.c 12、sshpass apk add openssh sshpass sshpass -p qwe ssh -l root 192.168.0.33 \"cat /etc/hosts\" 13、gcc 打包静态库，去除静态库中符号、调试和debug信息 gcc hello.c -o hello -static strip hello 14、每三小时执行一次定时任务 crontab -l crontab -e 0 */3 * * * docker restart samba 三、dd命令 1、参数 if=文件名：输入文件名，缺省为标准输入。即指定源文件。 of=文件名：输出文件名，缺省为标准输出。即指定目的文件。 ibs=bytes：一次读入bytes个字节，即指定一个块大小为bytes个字节。 obs=bytes：一次输出bytes个字节，即指定一个块大小为bytes个字节。 bs=bytes：同时设置读入/输出的块大小为bytes个字节。 cbs=bytes：一次转换bytes个字节，即指定转换缓冲区大小。 skip=blocks：从输入文件开头跳过blocks个块后再开始复制。 seek=blocks：从输出文件开头跳过blocks个块后再开始复制。 count=blocks：仅拷贝blocks个块，块大小等于ibs指定的字节数。 conv=，关键字可以有以下11种： conversion：用指定的参数转换文件。 ascii：转换ebcdic为ascii ebcdic：转换ascii为ebcdic ibm：转换ascii为alternate ebcdic block：把每一行转换为长度为cbs，不足部分用空格填充 unblock：使每一行的长度都为cbs，不足部分用空格填充 lcase：把大写字符转换为小写字符 ucase：把小写字符转换为大写字符 swab：交换输入的每对字节 noerror：出错时不停止 notrunc：不截短输出文件 sync：将每个输入块填充到ibs个字节，不足部分用空（NUL）字符补齐。 --help：显示帮助信息 --version：显示版本信息 1、将本地的/dev/hdb整盘备份到/dev/hdd dd if=/dev/hdb of=/dev/hdd 2、将/dev/hdb全盘数据备份到指定路径image文件 dd if=/dev/hdb of=/root/image 3、见备份文件恢复到指定盘 dd if=/root/image of=/dev/hdb 4、备份/dev/hdb全盘数据，并利用gzip工具进行压缩。保存到指定路径 dd if=/dev/hdb | gzip > /root/image.gz 5、将压缩的备份文件恢复到指定盘 gzip -dc /root/image.gz | dd of=/dev/hdb 6、备份与恢复MBR 备份磁盘开始的512个字节大小的MBR信息到指定文件 dd if=/dev/hda of=/root/image count=1 bs=512 count=1指仅拷贝一个块，bs=512指块大小为512字节。 恢复，将备份的MBR信息写到磁盘的开始部分 dd if=/root/image of=/dev/hda 7、备份软盘 dd if=/dev/fd0 of=disk.img count=1 bs=1440k 8、拷贝内存内容到硬盘 dd if=/dev/mem of=/root/mem.bin bs=1024(块大小1k) 9、拷贝光盘内容到指定文件夹，并保存为cd.iso文件 dd if=/dev/cdrom of=/root/cs.iso 10、增加swap分区文件大小 1、创建一个大小为256M的文件 dd if=/dev/zero of=/swapfile bs=1024 count=262144 2、把这个文件变成swap文件 mkswap /swapfile 3、启用这个swap文件 swapon /swapfile 4、编辑/etc/fstab文件，使在每次开机时自动加载swap文件 /swapfile swap swap default 0 0 11、销毁磁盘数据(利用随机数据填充硬盘) dd if=/dev/urandom of=/dev/hda1 12、测试硬盘读写速度 通过以下两个命令输出的命令执行时间，可以计算出硬盘的读写速度。 dd if=/dev/zero bs=1024 count=1000000 of=/root/1Gb.file dd if=/root/1Gb.file bs=64k | dd of=/dev/null 13、/dev/null和/dev/zero的区别 /dev/null：无底洞，是空设备，可以向它输入任何数据，通吃。 /dev/zero：是一个输入设备，可以用它来初始化文件。该设备无穷尽的提供0，可以使用任何需要的数目。可以用于向设备或文件写入字符串0。 dd if=/dev/zero of=./test.txt bs=1k count=1 #禁止标准输出 rm -rf /proc/cpuinfo >/dev/null #禁止标准错误 rm -rf /proc/cpuinfo 2>/dev/null #禁止标准输出和标准错误的输出 rm -rf /proc/cpuinfo 2>/dev/null >/dev/null rm -rf /proc/cpuinfo >/dev/null 2>&1 14、dd测试磁盘io https://www.xiaomastack.com/2015/01/09/dd%E5%91%BD%E4%BB%A4%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98io/ dd if=/dev/zero of=out.file bs=512K count=2048 dd if=/dev/zero of=out.file bs=512K count=2048 oflag=direct dd if=/dev/zero of=out.file bs=512K count=2048 oflag=dsync dd if=/dev/zero of=out.file bs=512K count=2048 oflag=sync dd if=/dev/zero of=out.file bs=512K count=2048 conv=fdatasync dd if=/dev/zero of=out.file bs=512K count=2048 conv=fsync ​ 1、大多数文件系统的默认I/O操作都是缓存I/O，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。dd命令不指定oflag或conv参数时默认使用缓存I/O ​ 2、用oflag=direct指定采用直接I/O操作。直接I/O的优点是通过减少操作系统内核缓冲区和应用程序地址空间的数据拷贝次数，降低了对文件读取和写入时所带来的CPU的使用以及内存带宽的占用。很明显，直接I/O会增加磁盘读写的次数 ​ 3、用oflag=dsyns参数，dd在执行时每次都会进行同步写入操作，可能是最慢的一种方式了，因为基本上没有用到写缓存(write cache) ​ 4、用oflag=syns参数，与上者dsyns类似，但同时也对元数据（描述一个文件特征的系统数据，如访问权限、文件拥有者及文件数据块的分布信息等。）生效，dd在执行时每次都会进行数据和元数据的同步写入操作 ​ 5、使用conv=fdatasync参数，dd命令执行到最后会真正执行一次“同步(sync)”操作，将所有还没有写入到磁盘的缓存内容写入磁盘，这样算出来的数度才是比较符合实际的 ​ 6、使用conv=fsync参数，与上者fdatasync类似，但同时元数据也一同写入。 以上可以看出，因为测试数据还和当前的系统环境、系统负载、磁盘测试时磁盘已有的读写任务等有关，用dd命令测试磁盘时不一定是磁盘真实的读写速度。但是可以大致判断机器目前的磁盘读写速度 15、磁盘修复 fsck.ext4 -y /dev/sda1 xfs_repair /dev/sda1 16、扩容 xfs_growfs /dev/mapper/centos-root resize2fs /dev/myVG/myLV 四、别名 alias aming='pwd' unalias aming 五、\";\",\"&&\",\"||\" 使用 ”;” 时，不管command1是否执行成功都会执行command2； 使用 “&&” 时，只有command1执行成功后，command2才会执行，否则command2不执行； 使用 “||” 时，command1执行成功后command2 不执行，否则去执行command2，总之command1和command2总有一条命令会执行。 六、/proc /proc 文件系统是一个伪的文件系统，就是说它是一个实际上不存在的目录，因而这是一 个非常特殊的目录。它并不存在于某个磁盘上，而是由核心在内存中产生。这个目录用于提 供关于系统的信息。下面说明一些最重要的文件和目录(/proc 文件系统在proc man页中有更详 细的说明)。 /proc/x 关于进程x的信息目录，这一x是这一进程的标识号。每个进程在/proc 下有一个名为自 己进程号的目录。 /proc/cpuinfo 存放处理器( c p u )的信息，如c p u的类型、制造商、型号和性能等。 /proc/devices 当前运行的核心配置的设备驱动的列表。 /proc/dma 显示当前使用的d m a通道。 /proc/filesystems 核心配置的文件系统信息。 /proc/interrupts 显示被占用的中断信息和占用者的信息，以及被占用的数量。 /proc/ioports 当前使用的i / o端口。 /proc/kcore 系统物理内存映像。与物理内存大小完全一样，然而实际上没有占用这么多内存；它仅仅是在程序访问它时才被创建。(注意：除非你把它拷贝到什么地方，否则/proc 下没有任何东西占用任何磁盘空间。) /proc/kmsg 核心输出的消息。也会被送到s y s l o g。 /proc/ksyms 核心符号表。 /proc/loadavg 系统“平均负载”； 3个没有意义的指示器指出系统当前的工作量。 /proc/meminfo 各种存储器使用信息，包括物理内存和交换分区( s w a p )。 /proc/modules 存放当前加载了哪些核心模块信息。 /proc/net 网络协议状态信息。 /proc/self 存放到查看/proc 的程序的进程目录的符号连接。当2个进程查看/proc 时，这将会是不同的连接。这主要便于程序得到它自己的进程目录。 /proc/stat 系统的不同状态，例如，系统启动后页面发生错误的次数。 /proc/uptime 系统启动的时间长度。 /proc/version 核心版本 七、grep -E ：开启扩展（Extend）的正则表达式。 -i ：忽略大小写（ignore case）。 -v ：反过来（invert），只打印没有匹配的，而匹配的反而不打印。 -n ：显示行号 -w ：被匹配的文本只能是单词，而不能是单词中的某一部分，如文本中有liker，而我搜寻的只是like，就可以使用-w选项来避免匹配liker -c ：显示总共有多少行被匹配到了，而不是显示被匹配到的内容，注意如果同时使用-cv选项是显示有多少行没有被匹配到。 -o ：只显示被模式匹配到的字符串。 --color :将匹配到的内容以颜色高亮显示。 -A n：显示匹配到的字符串所在的行及其后n行，after -B n：显示匹配到的字符串所在的行及其前n行，before -C n：显示匹配到的字符串所在的行及其前后各n行，context 1、-A 显示匹配到的字符串所在的行及其后n行，after grep -A 2 \"core id\" /proc/cpuinfo 2、-B 显示匹配到的字符串所在的行及其前n行，before grep -B 2 \"core id\" /proc/cpuinfo 3、-C 显示匹配到的字符串所在的行及其前后n行，context grep -C 2 \"core id\" /proc/cpuinfo 4、grep -E \"word1|word2|word3\" file.txt 5、egrep '^root|bash$' passwd 八、tar 分卷压缩 举例说明： 要将目录logs打包压缩并分割成多个1M的文件，可以用下面的命令： tar cjf - logs/ |split -b 1m - logs.tar.bz2. 完成后会产生下列文件： logs.tar.bz2.aa, logs.tar.bz2.ab, logs.tar.bz2.ac 要解压的时候只要执行下面的命令就可以了： cat logs.tar.bz2.a* | tar xj 再举例： 要将文件test.pdf分包压缩成500 bytes的文件： tar czf - test.pdf | split -b 500 - test.tar.gz 最后要提醒但是那两个\"-\"不要漏了，那是tar的ouput和split的input的参数。 tar cjf - logs/ |split -b 1m - logs.tar.bz2. 完成后会产生下列文件： logs.tar.bz2.aa, logs.tar.bz2.ab, logs.tar.bz2.ac 要解压的时候只要执行下面的命令就可以了： cat logs.tar.bz2.a* | tar xj 九、修改用户名、密码 vim /etc/hostname vim /etc/hosts vim /etc/passwd vim /etc/shadow passwd {your_name} 十、linux快捷键 ctrl + a 光标跳到行首 ctrl + e 光标跳到行尾 ctrl + b 光标左移一个字母 ctrl + f 光标右移一个字母 ctrl + u 清除光标前内容 ctrl + k 清除光标后内容 ctrl + w 清除光标前一个单词 ctrl + y 恢复上次删除 ctrl + x 光标移到行首，再按下移动到原处 ctrl + h 同backspace ctrl + d 删除光标所在字母 esc + f 往右跳一个单词 esc + b 往左跳一个单词 十一、网络吞吐量测试 1、安装 yum -y install iperf3 2、Server执行(192.168.21.2) 持续测试1个小时，每10秒打印一次输出结果 -J 以Json格式输出测试结果 -P 指定同时连接测试的数量，默认一条连接 -u 使用UDP --sctp 使用SCTP 缺省iperf3使用上传模式：Client负责发送数据，Server负责接收；如果需要测试下载速度，则在Client侧使用-R参数即可 iperf3 -s -p 5201 -i 10 -t 3600 3、Client执行 iperf3 -c 192.168.21.2 -p 5201 十二、Python 环境切换 update-alternatives --install /usr/bin/python python /usr/bin/python2 100 update-alternatives --install /usr/bin/python python /usr/bin/python3 150 update-alternatives --config python 十三、永久修改 DNS vim /etc/systemd/resolved.conf [Resolve] DNS=119.29.29.29 systemctl restart systemd-resolved 十四、内核模块自动加载 echo \"test\" > /etc/modules-load.d/test.conf cp test.ko /lib/modules/4.19.37-rt19/kernel/ 十五、缺少stdio.h头文件 apt install -y build-essential 十六、C 多线性 -g gdb 调试 gcc server.c -g -o server -lpthread gdb ./server 十七、tar 拆分 -b b、k、m 单位 yolov3.om 可以是文件，也可以是目录 tar -zcvf - yolov3.om | split -b 100M - yolov3.om_ cat yolov3.om_a* | tar -zxvf - 十八、获取当前时间 `date '+%Y%m%d%H%M%S'` 十九、tmux tmux ls tmux attach -t 0 # 后台 Ctrl+b d # 选则窗口 Ctrl+b s # 水平拆分 tmux split-window Ctrl+b % # 垂直拆分 tmux split-window -h Ctrl+b \" #切换窗口 Ctrl+b 上下左右 # 关闭窗口 Ctrl+b x # 全屏 Ctrl+b z 二十、 shell set set -o errexit/set -e 脚本在遇到任何非零退出状态的命令时立即退出。有助于快速检测到错误，并且能够避免严重的错误导致脚本继续执行下去。 set -o nounset/set -u 脚本在使用未设置的变量时立即退出。 set -o pipefail 使整个管道命令的退出状态与其中的最后一个命令的退出状态保持一致。 set -o noglob 禁用通配符的展开。 二十一、卸载硬盘 umount: /hl: target is busy. 1、强制卸载 mount -lf /hl/ 2、使用fuser fuser -mv /hl/ fuser -kv /hl/ kill -9 PID 二十二 、指定目录压缩，并删除源文件 tar --remove-files -zcf dist/aaa-${arch}.tar.gz -C bbb/ . 二十三 、curl curl -LO http://xx.tar --progress-bar 二十四、bzip2 docker save aaa:v1.0.0 aaa_v1.0.0_image_arm64.tar bzip2 -z aaa_v1.0.0_image_arm64.tar.bz2 ls aaa_v1.0.0_image_arm64.tar.bz2|xargs bzip2 -cd|docker load 二十五、获取窗口大小 stty size 二十六、jq wget https://github.com/jqlang/jq/releases/download/jq-1.7.1/jq-linux-amd64 chmod +x jq-linux-amd64 mv jq-linux-amd64 /usr/bin/jq jq = python -m json.tool 二十七、查看端口 ss -tuln iptables -t nat -L -n -v netstat -tuln 二十八、查看证书是否过期 openssl x509 -in /etc/nginx/ssl/server.crt -noout -dates notBefore 表示证书的生效时间。 notAfter 表示证书的过期时间。 openssl s_client -connect your-domain.com:443 -servername your-domain.com /dev/null | openssl x509 -noout -dates 二十九、进程 ID 及其启动命令 pgrep -a python -a 参数会显示完整的命令行。 ps -efww 列出所有系统进程及其详细信息。 -e：显示所有进程。 -f：以完整格式显示。 -w：宽格式显示，确保显示完整的命令行。 UID PID PPID C STIME TTY TIME CMD root 1 0 0 08:00 ? 00:00:01 /sbin/init user 12345 1 0 08:01 ? 00:00:05 python3 my_script.py --arg1 value1 user 67890 12345 1 08:02 ? 00:00:10 python3 another_script.py UID：用户 ID，显示进程的拥有者。 PID：进程 ID。 PPID：父进程 ID。 C：CPU 使用率。 STIME：进程启动时间。 TTY：终端信息（? 表示非交互式）。 TIME：CPU 使用时间。 CMD：启动命令及其参数。 htop 在界面中按 F4，然后输入 python，即可筛选出与 Python 相关的进程 top -c 输入 o 然后输入 COMMAND=python，可以筛选与 Python 相关的进程。 三十、 sublime 替换重复行 Ctrl+H ^(.+)$[\\r\\n](^\\1$[\\r\\n]{0, 1})+ \\1\\n 三十一、 清理内存缓存 sync echo 3 > /proc/sys/vm/drop_caches echo 3 意味着向这个文件写入数字3，这将会执行以下三种缓存清理操作： 页面缓存（pagecache）清理：这包括用于文件内容的缓存。 dentries和inodes清理：dentries是目录项缓存，用于加快文件路径名的解析；inodes是文件系统用来存储文件和目录元数据的结构体。 slab对象（如目录项缓存和inode缓存）回收：slab分配器是Linux内核中用于管理内存的一种机制，用于缓存经常分配和释放的对象。 三十二、nohup local cmd=\"docker exec -t kubeasz ezctl add-node ${CLUSTER} ${ip}\" local log=\"/tmp/add-node.log\" nohup $cmd > $log 2>&1 & tail -n +1 -f $log 三十三、dos2unix LF 或 CRLF dos2unix filename 三十四、kill kill -HUP $(pgrep -f gunicorn) kill -USR2 1 pkill -f gunicorn kill -9 $(pgrep python) 三十五、base64 echo -n 'qwe'|base64 -w0|openssl rsautl -encrypt -pubin -inkey public.pem|base64 -w0 Linux系统启动过程 Linux系统的启动过程：内核引导、运行init、系统初始化、建立终端、用户登录系统 1、内核引导 当计算机打开电源后，首先是BIOS开机自检，按照BIOS中设置的设备的启动设备（通常是硬盘）来启动。操作系统接管软件后，首先读入/boot目录下的内核文件。 操作系统 -> /boot 2、运行 init init进程是系统所有进程的起点，没有这个进程，系统中任何进程都不会启动。init程序首先是需要读取配置文件/etc/inintab。 操作系统 -> /boot -> init进程 运行级别 许多程序需要开机启动。windows叫做“服务”（service）在linux叫做“守护进程”（daemon）。 init进程的一大任务，就是去运行这些开机启动程序。 但，不同场合需要启动不同的程序，比如用作服务器时，需要启动apache，用作桌面就不需要。 linux允许为不同的场合，分配不同的开机启动程序，这叫做“运行级别”（runlevel）。也就是，启动时根据“运行级别”，确定要运行哪些程序。 操作系统 -> /boot -> init进程 -> 运行级别 linux系统有7个运行级别（runlevel）： 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用户系统维护，禁止远程登录 运行级别2：多用户状态（没有NFS） 运行级别3：完全的多用户状态（有NFS），登录后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：x11控制台，登录后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 3、系统初始化 在init的配置文件中有这么一行： si::sysinit:/etc/rc.d/rc.sysinit 它调用执行了/etc/rc.d/rc.sysinit，而rc.sysinit是一个bash shell的脚本，它主要是完成一些系统初始化的工作，rc.sysinit是每一个运行级别都要首先运行的终于脚本。 它主要完成的工作有：激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行的任务。 15:5:wait:/etc/rc.d/rc 5 这一行表示以5为参数运行/etc/rc.d/rc,/etc/rc.d/rc是一个shell脚本，它接受5作为参数，去执行/etc/rc.d/rc5.d/目录下的所有的rc启动脚本，/etc/rc.d/rc5.d/目录中的这些启动脚本实际上都是一些连接文件，而不是真正的rc启动脚本，真正的rc启动脚本实际上都是放在/etc/rc.d/init.d/目录下。而这些rc启动搅拌有着类似的用法，它们接受start、stop、restart、status等参数。 操作系统 -> /boot -> init进程 -> 运行级别 ->/etc/init.d 4、建立终端 rc执行完毕后，返回init。这时基本系统环境已经设置好了，各种守护进程也已经启动了。init接下来会打开6个终端，以便用户登录系统。 5、用户登录系统 一般来说，用户的登录方式有三种： （1）命令行登录 （2）ssh登录 （3）图形界面登录 操作系统 -> /boot -> init进程 -> 运行级别 -> /etc/init.d -> 用户登录 6、图形模式与文字模式的切换方式 操作系统 -> /boot -> init进程 -> 运行级别 -> /etc/init.d -> 用户登录 -> Login shell 7、Linux 关机 正确的关机流程为：sync > shutdown > reboot > halt "},"notes/linux/change_centos_boot_kernel.html":{"url":"notes/linux/change_centos_boot_kernel.html","title":"CentOS 修改默认启动内核","keywords":"","body":"CentOS 修改默认启动内核 1、查看当前内核 uname -r 2、显示已经安装的内核 rpm -qa | grep kernel 3、安装指定内核 rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-3.10.0-229.1.2.el7.x86_64.rpm --force 4、卸载内核 yum remove kernel-3.10.0-229.1.2.el7.x86_64.rpm 5、启动内核修改 1.查看启动项 cat /boot/grub2/grub.cfg /查看启动项 cat /boot/grub2/grubenv 2. 设置默认启动项 grub2-set-default \"CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core)\" 3.10.0-693.17.1.el7.x86_64 3. 查看默认启动项 grub2-editenv list 4. 生成配置 grub2-mkconfig -o /boot/grub2/grub.cfg #备注： 在生成grub.cfg之前，最好先备份原始的grub.cfg文件 6、重新安装内核即可 yum -y update "},"notes/linux/vim.html":{"url":"notes/linux/vim.html","title":"Vim 常用命令","keywords":"","body":"Vim 常用命令 1、set 忽略大小写查找 :set ignorecase :set noignorecase 高亮搜索结果 :set hlsearch :set nohlsearch :e ++enc=utf8 filename, 让vim用utf-8的编码打开这个文件。 :w ++enc=gbk，不管当前文件什么编码，把它转存成gbk编码。 :set fenc或:set fileencoding，查看当前文件的编码。 2、快捷键 w 光标移动向后一个单词，单词首，2w向后两个 e 光标移动向后一个单词，单词尾,2e ge 和e相反，向前 b 光标移动向前一个单词，单词首，2b ^ 移动到本行第一个非空白字符上 0、HOME 移动到本行第一个字符上 $ 移动到行尾， 3$ 移动到下面3行的行尾 gg 文件头 G 文件尾 f fx找到下一个为x的字符 :20 相当于 20G ，跳到指定行 Ctrl + e 向下滚动一行 Ctrl + y 向上滚动一行 Ctrl + d 向下滚动半屏 Ctrl + u 向上滚动半屏 Ctrl + f 向下滚动一屏 Ctrl + b 向上滚动一屏 u 撤销（undo） U 撤销对整行的操作 Ctrl + r 重做（redo) 3x 删除当前光标开始向后的三个字符 dd 删除当前行 dj 删除上一行 dk 删除下一行 3、执行shell :!command :!ls 列出当前目录下文件 :!perl -c script.pl 检查perl脚本语法，可以不用退出vim，非常方便。 :!perl script.pl 执行perl脚本，可以不用退出vim，非常方便。 :suspend或Ctrl - Z 挂起vim，回到shell，按fg可以返回vim。 4、注释命令 3,5 s/^/#/g 注释第3-5行 3,5 s/^#//g 解除3-5行的注释 1,$ s/^/#/g 注释整个文档。 :%s/^/#/g 注释整个文档，此法更快。 5、宏 . 重复上一个编辑动作 qa 开始录制宏a（键盘操作记录） q 停止录制 @a 播放宏a 6、文件加解密 vim -x file 开始编辑一个加密的文件。 :X 为当前文件设置密码。 :set key= 去除文件的密码。 7、常用配置 vim ~/.vimrc set nobackup set noswapfile set nowritebackup "},"notes/linux/nested.html":{"url":"notes/linux/nested.html","title":"嵌套虚拟化","keywords":"","body":"嵌套虚拟化 1、查看内核版本 [root@kvm ~]# uname -r 3.10.0-327.el7.x86_64 2、查看netsted引导参数，默认不开启，需要重新加载模块，并修改它的引导参数 [root@kvm ~]# cat /sys/module/kvm_intel/parameters/nested N 3、移除kvm_intel模块，移除前先关闭所有虚拟机，不然会报错 [root@kvm ~]# rmmod kvm_intel 或 modprobe -r kvm_intel 4、重新加载并开启nested功能 [root@kvm ~]# modprobe kvm_intel nested=1 5、再次查看netsted引导参数 [root@kvm ~]# cat /sys/module/kvm_intel/parameters/nested Y 6、以上开启nested的方式在重启的时候失效，如果需要永久生效可以通过如下方式： [root@kvm ~]# echo \"options kvm-intel nested=1\" >> /etc/modprobe.d/kvm_intel.conf "},"notes/linux/net_bridge.html":{"url":"notes/linux/net_bridge.html","title":"创建网桥","keywords":"","body":"创建网桥 一、CentOS 1、安装网桥相关依赖 yum -y install tunctl bridge-utils 2、创建网桥配置文件 cat /etc/sysconfig/network-scripts/ifcfg-br0 TYPE=Bridge DEVICE=br0 ONBOOT=yes BOOTPROTO=static IPADDR=192.168.0.10 NETMASK=255.255.255.0 GATEWAY=192.168.0.1 DNS1=114.114.114.114 EOF 3、修改原有网卡配置文件 cat /etc/sysconfig/network-scripts/ifcfg-eth0 TYPE=Ethernet DEVICE=eth0 ONBOOT=yes BRIDGE=br0 EOF 4、重启网络 systemctl restart network 5、网桥显示 brctl show 二、Ubuntu 1、安装依赖 apt -y install tunctl bridge-utils 2、配置 vim /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: eno1: dhcp4: no dhcp6: no # addresses: # - 192.168.0.241/24 # gateway4: 192.168.0.1 # nameservers: # addresses: # - 114.114.114.114 eno2: dhcp4: no dhcp6: no version: 2 bridges: br0: interfaces: [eno1] addresses: [192.168.0.241/24] gateway4: 192.168.0.1 nameservers: addresses: [114.114.114.114] 3、应用 netplan apply 4、查看状态 networkctl status br0 "},"notes/linux/nic_bond.html":{"url":"notes/linux/nic_bond.html","title":"网络端口聚合绑定","keywords":"","body":"网络端口聚合绑定 mode=0 (balance-rr) mode=1 (active-backup) mode=2 (balance-xor) mode=3 (broadcast) mode=4 (802.3ad) lacp mode=5 (balance-tlb) mode=6 (balance-alb) 一、bind 配置 1、netplan 网卡 bind 配置 cat /etc/netplan/00-installer-config.yaml network: ethernets: enp65s0f0: addresses: [] enp65s0f1: addresses: [] bonds: eth0: addresses: [192.168.0.61/24] gateway4: 192.168.0.1 nameservers: addresses: [114.114.114.114] interfaces: - enp65s0f0 - enp65s0f1 parameters: mode: 802.3ad mii-monitor-interval: 100 lacp-rate: fast transmit-hash-policy: layer3+4 2、交换机配置 sys display current-configuration interface GigabitEthernet 0/0/3 undo port link-type undo port default vlan nterface GigabitEthernet 0/0/4 undo port link-type undo port default vlan interface Eth-Trunk 1 mode lacp port link-type access port default vlan 20 max active-linknumber 2 interface GigabitEthernet 0/0/3 eth-trunk 1 lacp priority 100 nterface GigabitEthernet 0/0/4 eth-trunk 1 lacp priority 100 display trunkmembership eth-trunk 1 display eth-trunk 1 3、查看 bond 网卡 cat /proc/net/bonding/eth0 三、其他 1、centos网卡绑定 /etc/sysconfig/network-scripts/ifcfg-eth0 TYPE=Ethernet BOOTPROTO=none NAME=eth0 DEVICE=eth0 ONBOOT=yes MASTER=bond0 SLAVE=yes /etc/sysconfig/network-scripts/ifcfg-eth1 TYPE=Ethernet BOOTPROTO=none NAME=eth1 DEVICE=eth1 ONBOOT=yes MASTER=bond0 SLAVE=yes /etc/sysconfig/network-scripts/ifcfg-bond0 TYPE=Bond BOOTPROTO=none NAME=bond0 DEVICE=bond0 ONBOOT=yes BONDING_MASTER=yes BONDING_OPTS=\"mode=1 miimon=100\" IPADDR=10.10.10.12 NETMASK=255.255.255.0 GATEWAY=10.10.10.1 DNS1=114.114.114.114 systemctl restart network 其他方式 配置 bond0 绑定模式 创建 /etc/modprobe.d/bonding.conf，加入以下内容 alias bond0 bonding options bond0 miimon=100 mode=1 载入 bonding 模块，重启 network 服务 modprob bonding systemctl restart network 2、ubuntu网卡绑定 network: ethernets: enp1s0: addresses: [] enp6s0: addresses: [] bonds: eth0: addresses: [192.168.0.127/24] gateway4: 192.168.0.1 nameservers: addresses: [114.114.114.114] interfaces: - enp1s0 - enp6s0 apt-get install ifenslave vim /etc/module 加bonding source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface auto enp2s0f0 iface enp2s0f0 inet manual bond-master bond0 auto enp2s0f1 iface enp2s0f1 inet manual bond-master bond0 auto bond0 iface bond0 inet static address 172.20.31.1 netmask 255.255.255.0 network 172.20.31.0 broadcast 172.20.31.255 gateway 172.20.31.254 dns-nameservers 61.139.2.69 bond-mode 4 bond-miimon 100 bond-lacp-rate 1 bond-slaves enp2s0f0 enp2s0f1 auto lo iface lo inet loopback # The primary network interface #auto p1p1 #iface p1p1 inet manual auto p1p2 iface p1p2 inet manual #auto em1 #iface em1 inet manual #up ip link set $IFACE promisc on #down ip link set $IFACE promisc off #mtu 9000 auto p1p1 iface p1p1 inet static up ip link set $IFACE promisc on #post-up ifenslave bond0 p1p1 p1p2 down ip link set $IFACE promisc off #post-down ifenslave -d bond0 p1p1 p1p2 address 172.18.20.1 netmask 255.255.255.0 gateway 172.18.20.254 dns-nameservers 61.139.2.69 "},"notes/linux/ramdisk.html":{"url":"notes/linux/ramdisk.html","title":"虚拟内存盘","keywords":"","body":"虚拟内存盘 1、创建文件夹，并将文件夹作为内存盘 mkdir /mnt/ramdisk mount -t tmpfs tmpfs /mnt/ramdisk -o size=2G,defaults,noatime,mode=777 2、设置开机挂载 vim /etc/rc.local mount -t tmpfs tmpfs /mnt/ramdisk -o size=2G,defaults,noatime,mode=777 3、查看 df -h 凡是标注着tmpfs的都是虚拟硬盘，例如建立的 /mnt/ramdisk "},"notes/linux/ssh_interval.html":{"url":"notes/linux/ssh_interval.html","title":"ssh 自动断开","keywords":"","body":"ssh 自动断开 ssh 连接长时间不操作自动断开 修改服务器端参数 /etc/ssh/sshd_config 1、在其中添加一行内容，意思是向客户端每60秒发一次保持连接的信号 ClientAliveInterval 60 2、如果仍要设置断开时间,还有一个参数可以添加，意思是如果客户端60次未响应就断开连接,依据你期望的时间来设定 ClientAliveCountMax 60 修改本地参数 1、在连接前使用 -o 可以设置相应的参数 ssh -o ServerAliveInterval=30 root@192.168.1.1 "},"notes/linux/ssh_forwading.html":{"url":"notes/linux/ssh_forwading.html","title":"ssh 转发","keywords":"","body":"ssh 转发 C表示压缩数据传输 f表示后台用户验证,这个选项很有用,没有shell的不可登陆账号也能使用. N表示不执行脚本或命令 g表示允许远程主机连接转发端口 -L 本地转发 -R 远程转发 -D port 指定一个本地机器 “动态的'’ 应用程序端口转发. 工作原理是这样的, 本地机器上分配了一个 socket 侦听 port 端口, 一旦这个端口上有了连接, 该连接就经过安全通道转发出去, 根据应用程序的协议可以判断出远程主机将和哪里连接. 目前支持 SOCKS4 协议, 将充当 SOCKS4 服务器. 只有 root 才能转发特权端口. 可以在配置文件中指定动态端口的转发. 1、将本机6300端口转发到远端1521端口 本机(运行这条命令的主机)打开6300端口, 通过加密隧道映射到远程主机172.16.1.164的1521端口(使用远程主机oracle用户). 在本机上用netstat -an|grep 6300可看到. 简单说,本机的6300端口就是远程主机172.16.1.164的1521端口. ssh -CfNg -L 6300:127.0.0.1:1521 oracle@172.16.1.164 2、将本机81端口转发到远端4443端口 例：（192.168.21.31执行）使用192.168.21.31的81端口，转发到192.168.21.33的4443端口 ssh -CfNg -L 81:127.0.0.1:4443 root@192.168.21.33 3、将远端的1521端口转发到本机的6300端口 作用同上, 只是在远程主机172.16.1.164上打开1521端口, 来映射本机的6300端口. ssh -CfNg -R 1521:127.0.0.1:6300 oracle@172.16.1.164 4、将远端的81端口转发带本机4443端口 例：（192.168.21.33执行）让远程（192.168.21.31）的81端口，转发到（192.168.21.33）4443端口 ssh -CfNg -R 81:127.0.0.1:4443 root@192.168.21.31 5、指定一个本地机器 “动态的'’ 应用程序端口转发 ssh -C -f -N -g -D listen_port user@Tunnel_Host https://www.zfl9.com/ssh-port-forwarding.html "},"notes/linux/samba.html":{"url":"notes/linux/samba.html","title":"Samba 相关","keywords":"","body":"Samba 相关 1、安装 yum install -y samba samba-client samba-common 2、创建文件作为共享文件夹 mkdir /hl 3、编辑配置文件（/etc/samba/smb.conf） [global] workgroup = WORKGROUP server string = %h server (Samba, Ubuntu) dns proxy = no log file = /var/log/samba/log.%m max log size = 1000 syslog = 0 panic action = /usr/share/samba/panic-action %d server role = standalone server passdb backend = tdbsam obey pam restrictions = yes unix password sync = yes passwd program = /usr/bin/passwd %u passwd chat = *Enter\\snew\\s*\\spassword:* %n\\n *Retype\\snew\\s*\\spassword:* %n\\n *password\\supdated\\ssuccessfully* . pam password change = yes map to guest = bad user usershare allow guests = yes [hl] comment = hl path = /hl browseable = yes writable = yes create mode = 0777 directory mode = 0777 guest ok = yes locking = no 4、重启服务 systemctl restart smb "},"notes/linux/ftp.html":{"url":"notes/linux/ftp.html","title":"ftp 相关","keywords":"","body":"ftp 相关 1、安装ftp apt-get update apt-get install vsftpd service vsftpd restart 2、新建\"/home/uftp\"目录作为用户主目录 mkdir /home/uftp 3、新建用户uftp并设置密码 useradd -d /home/uftp -s /bin/bash uftp passwd uftp 4、编辑配置文件（/etc/vsftpd.conf） 添加: userlist_deny=NO //指定一个userlist，放允许ftp登陆的本地用户 userlist_enable=YES userlist_file=/etc/allowed_users //记录允许本地登陆用户名的文件 eccomp_sandbox=NO 改： local_enable=YES write_enable=YES 5、编辑其他文件 vim /etc/allowed_users 加入：uftp vim /etc/ftpusers //记录不能访问FTP服务器的用户清单 删uftp 6、重启服务 service vsftpd restart 7、其他问题 * 用户名：uftp 密码： 端口21 * 553 Could not create file. setsebool -P ftpd_disable_trans 1 service vsftpd restart "},"notes/linux/iscsi.html":{"url":"notes/linux/iscsi.html","title":"iscsi 相关","keywords":"","body":"iscsi 相关 一、服务端 1、安装 yum -y install targetcli systemctl start target systemctl enable target 2、Create a Backstore targetcli /backstores/fileio create file1 /tmp/disk1.img 200M write_back=false /backstores/block create name=block_backend dev=/dev/sdb /backstores/pscsi/ create name=pscsi_backend dev=/dev/sr0 /backstores/ramdisk/ create name=rd_backend size=1GB 3、Create an iSCSI Target targetcli iscsi/ create //create iqn.2006-04.com.example:444 4、Configure an iSCSI Portal iqn.2006-04.example:444/tpg1/ portals/ create portals/ create 192.168.122.137 5、 Configure ACLs /iscsi/iqn.20...mple:444/tpg1> acls/ create iqn.2006-04.com.example.foo:888 6、Configure LUNs luns/ create /backstores/ramdisk/rd_backend /iscsi/iqn.2016-02.local.itzgeek.server:disk1/tpg1/ set attribute generate_node_acls=1 set attribute authentication=0 clearconfig confirm=True 二、客户端 1、安装客户端（在需要连接iscsi的主机中操作） yum -y install iscsi-initiator-utils 2、设置授权客户端的iqn，编辑/etc/iscsi/initiatorname.iscsi要和服务端（提供iscsi的服务器）acls配置一致 vim /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.2003-01.org.linux-iscsi.hl.x8664:sn.18da323b85e7 systemctl restart iscsid 3、发现并登陆iscsi服务器-t senbtargets”表示发布的target（我这上面缩写为“-t st”），选项“-p ”ip：port 用来指定服务器IP地址。“-m node”选项表示管理目标节点，选项“-l”表示登录连接（--login 也可以） iscsiadm -m discovery -t st -p 192.168.21.33 -l lsblk blkid iscsiadm -m node -L all iscsiadm -m node -U all Optionally, set auto_cd_after_create to false to prevent targetcli from automatically changing object contexts after the creation of a new object: set global auto_add_mapped_luns=false 4、设置开机自动登录 sudo iscsiadm -m node -o update -n node.startup -v automatic "},"notes/linux/guestfish.html":{"url":"notes/linux/guestfish.html","title":"guestfish 相关","keywords":"","body":"guestfish 相关 1、安装相关软件 yum install -y libguestfs-tools 注意：默认安装是不安装windows系统支持的，如果需要修改windows系统镜像，需要再运行如下命令。 yum install libguestfs-winsupport 2、查看镜像中的文件 virt-cat tmp-CentOS7.4_x86_64-20171128.img /etc/hosts 3、镜像磁盘空间使用查看 virt-df 4、列出指定目录内文件 virt-ls 5、显示指定文件内容 virt-cat 6、编辑指定文件 virt-edit 7、将文件拷贝到虚拟机内部 virt-copy-in 8、将虚拟机内部文件拷贝出来 virt-copy-out 9、tar压缩文件拷贝进虚拟机并解压 virt-tar-in 10、镜像内指定目录文件拷贝并压缩 virt-tar-out 11、解压或者上传文件到虚拟机 virt-tar 12、交互的shell guestfish --rw -a CentOS-6-x86_64-GenericCloud.qcow2 > run > list-filesystems >/dev/sda1: ext4 > mount /dev/sda1 / > edit /etc/fstab > umount / > exit 13、guestfish修改镜像格式和大小，修改镜像格式和大小主要使用以下命令 virt-convert - convert virtual machines between formats 14、转化虚拟机镜像格式 virt-resize - Resize a virtual machine disk 15、修改虚拟机镜像磁盘 raw转qcow2格式 需要先用qemu-img命令创建一个一样大小的空qcow2格式镜像文件，然后使用virt-convert命令，原始镜像可以是 vmware镜像vmx，kvm进行，ovf的镜像。 virt-convert -i raw -o qcow2 old.img new.qcow2 将指定的分区扩大5G，创建一个新的镜像，比原来大5G，然后扩展 virt-resize --expand /dev/sda2 olddisk newdisk 将boot增加200M，剩下的空间扩充给/dev/sda2 virt-resize --resize /dev/sda1=+200M --expand /dev/sda2 \\ olddisk newdisk lv扩展 virt-resize --expand /dev/sda2 --LV-expand /dev/vg_guest/lv_root \\ olddisk newdisk 扩展分区，并将raw格式转化成qcow2格式 qemu-img create -f qcow2 newdisk.qcow2 15G virt-resize --expand /dev/sda2 olddisk newdisk.qcow2 注意： 1、如果是扩展分区，目标磁盘文件必须大于原生磁盘； 2、磁盘缩小比较复杂，一般要求缩小到的空间远大于文件系统的大小。 "},"notes/linux/nginx.html":{"url":"notes/linux/nginx.html","title":"nginx 相关","keywords":"","body":"nginx 相关 一、介绍 正向代理 反向代理 由于不能直接访问google，那么可以借助NPV来实现，这就是一个简单的正向代理的例子。可以发现，正向代理“代理”的是客户端，而且客户端是知道目标的，而目标是不知道客户端是通过NPV访问的。 当我们在外网访问百度的时候，其实会进行一个转发，代理到内网去，这就是所谓的反向代理，即反向代理“代理”的是服务器端，而且这一个过程对于客户端而言是透明的。 1、启动ngnx docker run -d -p 192.168.21.32:80:80 --name nginx nginx 2、下面四种情况分别用http://192.168.1.4/proxy/test.html 进行访问。 第一种： location /proxy/ { proxy_pass http://127.0.0.1:81/; } 会被代理到http://127.0.0.1:81/test.html 这个url 第二种(相对于第一种，最后少一个 /) location /proxy/ { proxy_pass http://127.0.0.1:81; } 会被代理到http://127.0.0.1:81/proxy/test.html 这个url 第三种： location /proxy/ { proxy_pass http://127.0.0.1:81/ftlynx/; } 会被代理到http://127.0.0.1:81/ftlynx/test.html 这个url。 第四种情况(相对于第三种，最后少一个 / )： location /proxy/ { proxy_pass http://127.0.0.1:81/ftlynx; } 会被代理到http://127.0.0.1:81/ftlynxtest.html 这个url 3、配置实例nginx server { listen 80; server_name 192.168.21.33; #access_log logs/quancha.access.log main; #error_log logs/quancha.error.log; #root html; #index index.html index.htm index.php; ## send request back to apache ## location / { proxy_pass http://192.168.21.100:80; #Proxy Settings proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_max_temp_file_size 0; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; } } server { listen 80; server_name xxx123.tk; location / { proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://192.168.10.38:3000; } access_log logs/xxx123.tk_access.log; } server { listen 80; server_name xxx456.tk; location / { proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://192.168.10.40:80; } access_log logs/xxx456.tk_access.log; } 二、使用 1、常用配置 autoindex on;# 显示目录 autoindex_exact_size on;# 显示文件大小 autoindex_localtime on;# 显示文件时间 2、rewrite location /aa { rewrite \"^/aa/(.*)$\" /$1 break; proxy_pass http://127.0.0.1:1027; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-scheme $scheme; proxy_buffering off; } 三、yaml cat apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: nodeName: master containers: - name: nginx image: nginx:1.21.3 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-service spec: ports: - name: nginx-service port: 80 targetPort: 80 nodePort: 30080 type: NodePort selector: app: nginx "},"notes/linux/haproxy.html":{"url":"notes/linux/haproxy.html","title":"haproxy 相关","keywords":"","body":"haproxy 相关 1、检查配置文件语法 haproxy -c -f /etc/haproxy/haproxy.cfg 2、以daemon模式启动，以systemd管理的daemon模式启动 haproxy -D -f /etc/haproxy/haproxy.cfg [-p /var/run/haproxy.pid] haproxy -Ds -f /etc/haproxy/haproxy.cfg [-p /var/run/haproxy.pid] 3、启动调试功能，将显示所有连接和处理信息在屏幕 haproxy -d -f /etc/haproxy/haproxy.cfg 4、restart，需要使用st选项指定pid列表 haproxy -f /etc/haproxy.cfg [-p /var/run/haproxy.pid] -st `cat /var/run/haproxy.pid 5、graceful restart，即reload。需要使用sf选项指定pid列表 haproxy -f /etc/haproxy.cfg [-p /var/run/haproxy.pid] -sf `cat /var/run/haproxy.pid 6、显示haproxy编译和启动信息 haproxy -vv 7、配置实例 yum install haproxy -y # 然后启动Haproxy haproxy -f /usr/local/haproxy/etc/haproxy.cfg # 停止Haproxy killall haproxy vim /etc/haproxy/haproxy.cfg global chroot /var/lib/haproxy user haproxy group haproxy daemon log 127.0.0.1 local0 info maxconn 4000 stats socket /var/lib/haproxy/haproxy.sock defaults log global mode http option redispatch option httplog #option forwardfor retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout check 10s frontend stratos bind *:1234 mode tcp option tcplog default_backend stratos_backend frontend openstack bind *:80 default_backend openstack_backend backend stratos_backend mode tcp option tcplog stick-table type ip size 200k expire 30m stick on src option ssl-hello-chk server stratos 192.168.21.31:1234 backend openstack_backend http-request del-header X-Forwarded-Proto if { ssl_fc } http-request set-header X-Forwarded-Proto https if { ssl_fc } server kolla 192.168.21.100:80 check inter 2000 rise 2 fall 5 "},"notes/linux/build_pypi.html":{"url":"notes/linux/build_pypi.html","title":"发布pypi软件包","keywords":"","body":"发布pypi软件包 发布pypi python包，打包软件包时，主要依赖setuptools。 pip install setuptools 一、注册pypi账号 pypi register 1、点击register 2、填写名字，密码，邮件 记住自己的用户名和密码，后面上传的时候需要输入 二、准备自己的python源码 略 三、准备setup文件（全部放在源码根目录） 1、准备setup.cfg [bdist_wheel] universal = 1 2、准备README.rst，具体语法可以参考http://rest-sphinx-memo.readthedocs.io/en/latest/ReST.html ======== autopep8 ======== .. image:: https://img.shields.io/pypi/v/autopep8.svg :target: https://pypi.org/project/autopep8/ :alt: PyPI Version .. image:: https://travis-ci.org/hhatto/autopep8.svg?branch=master :target: https://travis-ci.org/hhatto/autopep8 :alt: Build status .. contents:: Installation ============ From pip:: $ pip install --upgrade autopep8 Consider using the ``--user`` option_. .. _option: https://pip.pypa.io/en/latest/user_guide/#user-installs Requirements ============ autopep8 requires pycodestyle_. .. _pycodestyle: https://github.com/PyCQA/pycodestyle 3、准备setup.py version：包版本，更新包时更新版本号 name：包名称 long_description：必须是rst（reStructuredText）格式，这个内容会显示在pypi包首页，具体语法可以参考http://rest-sphinx-memo.readthedocs.io/en/latest/ReST.html install_requires：申明依赖包。安装包是pip会自动安装 packages = findpackages()，这个参数是导入目录下的所有_init.py包 #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\"Setup for autopep8.\"\"\" import ast import io from setuptools import setup INSTALL_REQUIRES = ( ['pycodestyle >= 2.5.0'] ) def version(): \"\"\"Return version string.\"\"\" with io.open('autopep8.py') as input_file: for line in input_file: if line.startswith('__version__'): return ast.parse(line).body[0].value.s with io.open('README.rst') as readme: setup( name='hlautopep8', version=version(), description='A tool that automatically formats Python code to conform ' 'to the PEP 8 style guide', long_description=readme.read(), license='Expat License', author='tmp', author_email='tmp@gmail.com', url='https://tmp/autopep8', classifiers=[ 'Development Status :: 5 - Production/Stable', 'Environment :: Console', 'Intended Audience :: Developers', 'License :: OSI Approved :: MIT License', 'Operating System :: OS Independent', 'Programming Language :: Python', 'Programming Language :: Python :: 2', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: 3.7', 'Topic :: Software Development :: Libraries :: Python Modules', 'Topic :: Software Development :: Quality Assurance', ], keywords='automation, pep8, format, pycodestyle', install_requires=INSTALL_REQUIRES, test_suite='test.test_autopep8', py_modules=['autopep8'], zip_safe=False, entry_points={'console_scripts': ['autopep8 = autopep8:main']}, ) 四、开始打包（到源码根目录打包，成功执行打包命令会生成一个dist文件夹） 1、将源码打包为wheels(whl)格式 python setup.py bdist_wheel --universal 可以使用pip命令进行安装 pip install xx.whl 2、将源码打包为tar.gz格式 python setup.py sdist build 3、将源码打包为egg格式 python setup.py bdist_egg 五、上传生成的包 1、使用twine上传，先安装twine pip install twine 2、上传，会提示输入用户名和密码 twine upload dist/* 3、上传成功可以到自己的pypi仓库查看 六、使用 pip search xx pip install xx 七、其他 #!/usr/bin/env python # coding: utf-8 from setuptools import setup setup( name='jujube_pill', version='0.0.1', author='xlzd', author_email='what@the.f*ck', url='https://zhuanlan.zhihu.com/p/26159930', description=u'吃枣药丸', packages=['jujube_pill'], install_requires=[], entry_points={ 'console_scripts': [ 'jujube=jujube_pill:jujube', 'pill=jujube_pill:pill' ] } ) install_requires 是这个库所依赖的其它库，当别人使用 pip 等工具安装你的包时，会自动安装你所依赖的包。consolescripts 是这个包所提供的终端命令，比如我希望在安装这个包后可以使用「 jujube 」和「 pill 」两个命令，则按照 setup 文件的写法，当我在终端输入「 jujube 」的时候，将会执行 jujubepill 包下（__init 中）的 jujube 函数。 init.py 文件如下： #!/usr/bin/env python # encoding=utf-8 def jujube(): print u'吃枣' def pill(): print u'药丸' "},"notes/linux/mysql.html":{"url":"notes/linux/mysql.html","title":"mysql 相关","keywords":"","body":"mysql 相关 1、允许远程访问 use mysql; update db set host = '%' where user = 'root'; flush privileges; grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option; #修改ip vim /etc/mysql/my.cnf #重启服务 service mysql restart 2、修改密码 1、连接数据库 2、use mysql: 3、update user set password=password(\"0127\") where user=\"root\"; 4、flush privileges; 忘记roo密码 1、关闭正在运行的MySQL服务。 2、打开DOS窗口，转到mysql\\bin目录。 3、 输入mysqld --skip-grant-tables 回车。--skip-grant-tables 的意思是启动MySQL服务的时候跳过权限表认证。 4、再开一个DOS窗口（因为刚才那个DOS窗口已经不能动了），输入mysql回车，如果成功，将出现MySQL提示符 >。 6、连接权限数据库： use mysql; 。 7、改密码：update user set password=password(\"root\") where user=\"root\";（别忘了最后加分号） 。 8、刷新权限（必须步骤）：flush privileges;　。 9、退出 quit。 3、使用docker创建mysql服务 docker run -itd --name mariadb --restart=always -v /opt/mysql:/etc/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mariadb GRANT ALL PRIVILEGES ON *.* TO 'root'@'%'IDENTIFIED BY '123456' WITH GRANT OPTION; flush privileges; mysql -h 127.0.0.1 -u root -p123456 4、my.cnf常用配置 [mysqld] expire_logs_days=7 max_connection=1000 max_user_connections=500 wait_timeout=30 interactive_timeout=30 5、数据库修复 ib_logfile0和ib_logfile1PPH innodb_force_recovery=6 "},"notes/linux/ssr_privoxy.html":{"url":"notes/linux/ssr_privoxy.html","title":"ssr privoxy 相关","keywords":"","body":"ssr privoxy 相关 一、ssr代理服务 1、下载 git clone https://github.com/SAMZONG/gfwlist2privoxy.git cd gfwlist2privoxy/ cp ssr /usr/local/bin chmod +x /usr/local/bin/ssr 2、安装 ssr install ssr config 3、 配置文件路径 /usr/local/share/shadowsocksr/config.json { \"server\": \"0..0.0.0\", // ssr服务器ip \"server_ipv6\": \"::\", \"server_port\": 8080, // ssr服务器端口 \"local_address\": \"127.0.0.1\", \"local_port\": 1080, \"password\": \"123456\", // 对应password \"method\": \"none\", // 这里对应SSGlobal配置中的Encryption \"protocol\": \"auth_chain_a\", //对应protocl \"protocol_param\": \"\", \"obfs\": \"http_simple\", //对应obfs \"obfs_param\": \"hello.world\", //对应obfs_param \"speed_limit_per_con\": 0, \"speed_limit_per_user\": 0, \"additional_ports\" : {}, // only works under multi-user mode \"additional_ports_only\" : false, // only works under multi-user mode \"timeout\": 120, \"udp_timeout\": 60, \"dns_ipv6\": false, \"connect_verbose_info\": 0, \"redirect\": \"\", \"fast_open\": false } 4、启动/关闭 ssr start ssr stop 5、卸载 ssr uninstall 6、这里操作会删除/usr/local/share/shadowsocksr 以上，本地监听服务已经配置完成了，在填写的过程中，要注意你的本地监听地址和监听端口，默认是127.0.0.1:1080，如果你修改了设置，那么在后续配置中也要配合修改。 二、Privoxy 配置 1、安装 CentOS yum install -y epel-release privoxy Ubuntu apt install -y privoxy 2、全局模式 代理模式同其他平台上方式，将所有http/https请求走代理服务，如果需要全局代理的话按照如下操作即可，如果要使用PAC模式，请跳过此部分。 # 添加本地ssr服务到配置文件 echo 'forward-socks5 / 127.0.0.1:1080 .' >> /etc/privoxy/config # Privoxy 默认监听端口是是8118 export http_proxy=http://127.0.0.1:8118 export https_proxy=http://127.0.0.1:8118 export ftp_proxy=http://127.0.0.1:8118 export no_proxy=localhost 启动服务 systemctl start privoxy.service 3、PAC模式 使用GFWList是由AutoProxy官方维护，由众多网民收集整理的一个中国大陆防火长城的屏蔽列表。 cd gfwlist2privoxy/ bash gfwlist2privoxy proxy(socks5): 127.0.0.1:1080 # 注意，如果你修改了ssr本地监听端口是需要设置对应的 {+forward-override{forward-socks5 127.0.0.1:1080 .}} cp -af gfw.action /etc/privoxy/ echo 'actionsfile gfw.action' >> /etc/privoxy/config # Privoxy 默认监听端口是是8118 export http_proxy=http://127.0.0.1:8118 export https_proxy=http://127.0.0.1:8118 export no_proxy=localhost 4、启动服务 systemctl start privoxy.service 5、proxy 环境变量 # privoxy默认监听端口为8118 export http_proxy=http://127.0.0.1:8118 export https_proxy=http://127.0.0.1:8118 export no_proxy=localhost # no_proxy是不经过privoxy代理的地址 # 只能填写具体的ip、域名后缀，多个条目之间使用','逗号隔开 # 比如: export no_proxy=\"localhost, 192.168.1.1, ip.cn, chinaz.com\" # 访问 localhost、192.168.1.1、ip.cn、*.ip.cn、chinaz.com、*.chinaz.com 将不使用代理 6、代理测试 curl -sL www.google.com # 获取当前 IP 地址 # 如果使用 privoxy 全局模式，则应该显示 ss 服务器的 IP # 如果使用 privoxy gfwlist模式，则应该显示本地公网 IP 7、管理脚本 在以上部署操作完成后，应该已经可以正常科学上网了，但是如果需要进行管理时，需要分别管理ssr和privoxy，为了方便管理，这里写了一个shell脚本方便管理: ssr_manager vim ssr_manager #!/bin/bash case $1 in start) ssr start &> /var/log/ssr-local.log systemctl start privoxy.service export http_proxy=http://127.0.0.1:8118 export https_proxy=http://127.0.0.1:8118 export no_proxy=\"localhost, ip.cn, chinaz.com\" ;; stop) unset http_proxy https_proxy no_proxy systemctl stop privoxy.service ssr stop &> /var/log/ssr-log.log ;; autostart) echo \"ssr start\" >> /etc/rc.local systemctl enable privoxy.service echo \"http_proxy=http://127.0.0.1:8118\" >> /etc/bashrc echo \"https_proxy=http://127.0.0.1:8118\" >> /etc/bashrc echo \"no_proxy='localhost, ip.cn, chinaz.com'\" >> /etc/bashrc ;; *) echo \"usage: source $0 start|stop|autostart\" exit 1 ;; esac 8、使用 mv ssr_manager /usr/local/bin chmod +x ssr_manager 9、启动服务 ssr_manager start 10、关闭服务 ssr_manager stop 11、 添加开机自启动 ssr_manager autostart 12、参考链接 https://www.zfl9.com/ss-local.html 13、sockes5 代理 export http_proxy=socks5://127.0.0.1:1080 export https_proxy=socks5://127.0.0.1:1080 # 强制终端中的 wget、curl 等都走 SOCKS5 代理 export ALL_PROXY=socks5://127.0.0.1:1080 14、git 代理 git config --global http.proxy http://127.0.0.1:8118 git config --global https.proxy http://127.0.0.1:8118 git config --global http.proxy socks5://127.0.0.1:1080 git config --global https.proxy socks5://127.0.0.1:1080 "},"notes/linux/proxychains_ng.html":{"url":"notes/linux/proxychains_ng.html","title":"proxychains-ng 代理工具","keywords":"","body":"proxychains-ng 代理工具 1、安装git、gcc yum -y install git gcc 2、下载源码 git clone https://github.com/rofl0r/proxychains-ng 3、生成配置文件 cd /root/proxychains-ng ./configure --prefix=/usr --sysconfdir=/etc 4、编译 make make install make install-config 5、修改配置文件 /etc/proxychains.conf 修改 socks4 127.0.0.1 9050 为 socks5 192.168.21.81 1080 6、使用、测试 proxychains4 curl www.google.com "},"notes/linux/virt_manager.html":{"url":"notes/linux/virt_manager.html","title":"virt-manager 相关","keywords":"","body":"virt-manager 相关 一、CentOS 1、安装相关软件 yum install -y virt-manager xorg-x11-xauth yum install -y qemu-kvm qemu-img virt-manager libvirt libvirt-python libvirt-client virt-install virt-viewer bridge-utils 二、Ubuntu 1、安装软件 apt install -y qemu qemu-kvm qemu-system-arm qemu-efi-aarch64 qemu-utils libvirt-daemon libvirt-clients bridge-utils virt-manager 2、重启 libvirt systemctl restart libvirtd 3、配置网桥 vim /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: eno1: dhcp4: no dhcp6: no # addresses: # - 192.168.0.241/24 # gateway4: 192.168.0.1 # nameservers: # addresses: # - 114.114.114.114 eno2: dhcp4: no dhcp6: no version: 2 bridges: br0: interfaces: [eno1] addresses: [192.168.0.241/24] gateway4: 192.168.0.1 nameservers: addresses: [114.114.114.114] 4、重启网络、查看网桥状态 netplan apply networkctl status br0 5、使用 virt-manager 三、常见问题 1.virt-manager方格乱码的问题 解决CentOS和Ubuntu下virt-manager方格乱码的问题，virt-manager显示乱码的解决方法，安装相应字体： ubuntu下： apt install font-manager apt install fonts-arphic-ukai apt install ttf-wqy-zenhei xfonts-wqy ttf-wqy-microhei apt install fonts-cwtex-fs apt install ttf-hanazono apt install ttf-mscorefonts-installer CentOS下安装： yum install -y dejavu-lgc-sans-fonts 2.打印运行日志 virt-manager --no-fork 3.virt-installERROR internal error process exited while connecting to monitor ERROR internal error process exited while connecting to monitor: char device redirected to /dev/pts/2 kvm: -drive file=/home/muge0913/workstation/kvm/test.img,if=none,id=drive-ide0-0-0,format=raw: could not open disk image /home/muge0913/workstation/kvm/test.img: Permission denied 解决:编辑/etc/libvirt/qemu.conf添加内容如下，这样root就有操作的权限了。 user = “root” # The group for QEMU processes run by the system instance. It can be # specified in a similar way to user. group = “root” # Whether libvirt should dynamically change file ownership # to match the configured user/group above. Defaults to 1. # Set to 0 to disable file ownership changes. dynamic_ownership = 0 service libvirt-bin restart 4、virt-manager 输入字符响应两次的问题 1、打开Xbrowser.exe之后，点击最上面的工具。 2、点击Xconfig开始。 3、右键点击Default Profile,选择属性 4、在打开的界面选择高级。取消XKEYBOARD的勾选。 5、删除默认网桥 virbr0 virbr0 virsh net-list net-autostart --disable default net-destroy default systemctl restart libvirtd 6、docker0与libvirt br0网桥冲突问题 touch /etc/docker/daemon.json vim /etc/docker/daemon.json { \"bridge\": \"br0\", \"default-gateway\": \"192.168.0.1\" } systemctl reload-daemon systemctl restart docker "},"notes/linux/linux_source.html":{"url":"notes/linux/linux_source.html","title":"Linux 源制作","keywords":"","body":"Linux 源制作 一、国内常用镜像站地址 网址 官方镜像站 https://developer.aliyun.com/mirror/ 阿里云 https://mirrors.huaweicloud.com/home 华为 http://mirrors.163.com/ 网易 https://mirrors.cloud.tencent.com/ 腾讯 https://mirrors.tuna.tsinghua.edu.cn/ 清华大学开 http://mirrors.pku.edu.cn/Mirrors 北京大学 http://mirrors.hust.edu.cn/ 华中科技大学 https://chinanet.mirrors.ustc.edu.cn/ 中国科技技术大学 二、apt 下载软件做离线源 1、拷贝所需安装软件包 通过apt-get安装的软件都在/var/cache/apt/archives目录下 cp /var/cache/apt/archives/* /home/package 2、生成软件包信息（含有重要的包的依赖关系） apt-get install dpkg-dev dpkg-scanpackages package /dev/null | gzip > packs/Packages.gz dpkg-scanpackages pools override > dists/trusty/main/binary-i386/Packages dpkg-scanpackages pools override > dists/trusty/main/binary-amd64/Packages 注：/dev/null位置的参数是指定一个文件，文件名不限，该文件的作用是用来重写覆盖deb软件包中控制文件的某些定义，它的第一行的格式，一行对应一个软件包： package priority section package指定你所要修改的软件包 priority 有low,medium,high三个值 section 用来指定软件包属于哪个section 如果不需要对deb软件包做任何修改你就可以像例子中那样直接指定一个/dev/null文件。 3、添加本地源 apt命令每次都会读取/etc/apt/sources.list源列表(这个源列表可以添加好多源,每次都选中开头的有效源)，因此我们编辑该文件，在第一行添加我们自己的本地源，如： deb http://172.18.20.161/ packs/ # deb file:///home packs/ 要注意中间的空格 4、打包本地源 将/etc/apt/sources.list文件拷贝到packages目录下，将packages文件夹打包、备份，以便使用。 5、如何使用本地源 将packages压缩包放到/目录(该目录只要和添加的本地源路径一致即可，以便apt能找到源）下解压，备份本机的sources.list，将packages目录下的sources.list拷贝到/etc/apt/目录下。修改/etc/apt/sources.list 之后一般会运行下面两个命令进行更新升级： sudo apt-get update sudo apt-get dist-upgrade 其中 ： update - 取回更新的软件包列表信息 dist-upgrade - 发布版升级 然后就可以离线安装了：apt-get install xxxx ## deb file:///opt/chuandge /packs/ 三、使用 apt-mirror 建立本地 ubuntu 源 1、安装apt-mirror apt-get install apt-mirror 2、创建放镜像的文件夹 例如将镜像等文件放在 /service/Ubuntu文件夹下： 并新建以下文件夹（mirror.list里面提示要事先新建以下文件夹的）： /service/ubuntu /service/ubuntu/mirror /service/ubuntu/skel /service/ubuntu/var 3、配置apt-mirror： vim /etc/apt/mirror.list ############# config ################## # # set base_path /var/spool/apt-mirror # # if you change the base path you must create the directories below with write privlages # # set mirror_path $base_path/mirror # set skel_path $base_path/skel # set var_path $base_path/var # set cleanscript $var_path/clean.sh # set defaultarch #我们添加或更改如下内容： set base_path /service/ubuntu set mirror_path $base_path/mirror set skel_path $base_path/skel set var_path $base_path/var set cleanscript $var_path/clean.sh set nthreads 20 set _tilde 0 # ############# end config ############## #我们把常用的软件同步过来就够用了 deb-i386 http://archive.Ubuntu.com/ubuntu hardy main restricted universe multiverse deb-i386 http://archive.ubuntu.com/ubuntu hardy-updates main restricted universe multiverse deb-i386 http://archive.ubuntu.com/ubuntu hardy-backports main restricted universe multiverse deb-i386 http://archive.ubuntu.com/ubuntu hardy-security main restricted universe multiverse deb-i386 http://archive.ubuntu.com/ubuntu hardy-proposed main restricted universe multiverse #当某些软件包在服务器端进行了升级，或者服务器端不再需要这些软件包时，我们使用了 apt-mirror与服务器同步后，会在本地的$var_path/下生成一个clean.sh的脚本，列出了遗留在本地的旧版本和无用的软件包，你可 以手动运行这个脚本来删除遗留在本地的且不需要用的软件包 clean http://archive.ubuntu.com/ubuntu 如果用amd64位架构下的包，可以加上deb-amd64的标记 如果什么都不加，直接使用deb http.....这种格式，则在同步时，只同步当前系统所使用的架构下的软件包。比如一个64位系统，直接debhttp....只同步64位的软件 包。如果还嫌麻烦，直接去改set defaultarch 这个参数就好，比如改成set defaultarch i386，这样就使用debhttp.....这种格式，则在同步时，只同步i386的软件包了。 如果你还想要源码，可以把源码也加到mirror.list里面同步过来，比如加上deb-src这样的标记。想要其他的东西也可以追加相应的标记来完成。 4、配置好后指定的镜像进行同步 apt-mirror 如果是第一次同步，官方镜像可能需要几天时间才能同步完整，如果与国内源进行同步，只同步常用软件，平均1秒钟网速1MB（Byte）要同步30G左右的数据，大概需要5-8小时的时间才能同步完整。 5、清理无用软件包，同步完成后，我们可以利用clean.sh清理无用软件包（本文档以set base_path /server/ubuntu为例）： bash /service/ubuntu/var/clean.sh 6、更新完毕后，使用apache发布源镜像了。 1.配置apache2 vim httpd.conf # # AllowOverride none # Require all denied # #改成 Options FollowSymLinks AllowOverride None Order deny,allow allow from all #在 节点中增加虚拟目录 Alias /ubuntu /workspace/ubuntu/mirror/mirrors.163.com/ubuntu #添加虚拟目录的权限 Options Indexes FollowSymLinks AllowOverride None Require all granted #由于有opengrok 做了个tomcat的端口转发 ProxyPass /source http://localhost:8081/source ProxyPassReverse /source http://localhost:8081/source 2.重启apache service apache2 restart 3.客户端配置source.list vim /etc/source.list deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty main restricted deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty-updates main restricted deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty universe deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty-updates universe deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty multiverse deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty-updates multiverse deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty-backports main restricted universe multiverse deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty-security main restricted deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty-security universe deb [arch=amd64] http://192.168.19.184/ubuntu/ trusty-security multiverse 4.更新源 apt-get update "},"notes/linux/rpm_build.html":{"url":"notes/linux/rpm_build.html","title":"Python rpm编译","keywords":"","body":"Python rpm编译 1、编写程序，首先将实现python源代码,包括如下目录和文件： Filebackup-1.0.0 ├── Filebackup │ ├── backup.py │ ├── Filebackup.py │ ├── file_remove.py │ ├── file_sync.py │ └── __init__.py ├── MANIFEST.in ├── README.txt ├── requirements.txt └── setup.py 2、查看setup.py文件内容 cat setup.py from setuptools import setup, find_packages setup(name='Filebackup', version='1.0', description='File backup for gitlab,redmine and wiki', author='Leon Zhang', author_email='test@123.com', url='http://123.com', packages=find_packages(), entry_points={ 'console_scripts': [ 'kyscripts = Filebackup.Filebackup:main', ], },) 3、将文件打包： tar -zcvf Filebackup-1.0.0.tar.gz Filebackup-1.0.0 4、创建systemd service文件 cat Filebackup.service [Unit] Description=File backup and synchronize application After=network-online.target docker.service Wants=network-online.target [Service] Type=simple ExecStart=/usr/bin/kyscripts ExecReload=/bin/kill -HUP $MAINPID KillMode=process Restart=on-failure RestartSec=42s [Install] WantedBy=multi-user.target 5、使用rpmbuild打包，首先安装rpmbuild,并生成打包目录： yum install -y rpm-build yum install -y rpmdevtools pcre-devel gcc make rpmdev-setuptree 此时会生成rpmbuild目录 6、编写*.spec文件： %define _unpackaged_files_terminate_build 0 %if 0%{?_version:1} %define _verstr %{_version} %else %define _verstr 1.0.0 %endif Name: Filebackup Version: %{_verstr} Release: 1%{?dist} Summary: consul-template watches a series of templates on the file system, writing new changes when Consul is updated. It runs until an interrupt is received unless the -once flag is specified. Group: System Environment/Daemons License: MPLv2.0 URL: http://www.tmp.com.cn Source0: %{name}-%{version}.tar.gz Source1: %{name}.service #BuildRoot: %(mktemp -ud %{_tmppath}/%{name}-%{version}-%{release}-XXXXXX) BuildRoot: %_topdir/BUILDROOT %if 0%{?fedora} >= 14 || 0%{?rhel} >= 7 BuildRequires: systemd-units Requires: systemd %endif Requires(pre): docker %description Filebackup is a service that bakcup remove and sync gitlab,wiki and redmine data files. %prep %setup -q %install rm -rf %{buildroot} mkdir -p %{buildroot}/%{_bindir}/ mkdir -p %{buildroot}/%{_sysconfdir}/%{name}.d mkdir -p %{buildroot}/%{_sharedstatedir}/%{name} %if 0%{?fedora} >= 14 || 0%{?rhel} >= 7 mkdir -p %{buildroot}/%{_unitdir} cp %{SOURCE1} %{buildroot}/%{_unitdir}/ python setup.py install --root=%{buildroot} --record=INSTALLED_FILES %endif %pre getent group Filebackup>/dev/null || groupadd -r Filebackup getent passwd Filebackup >/dev/null || \\ useradd -r -g Filebackup -d /var/lib/Filebackup -s /sbin/nologin \\ -c \"Filebackup user\" Filebackup exit 0 %if 0%{?fedora} >= 14 || 0%{?rhel} >= 7 %post %systemd_post %{name}.service %preun %systemd_preun %{name}.service %postun %systemd_postun_with_restart %{name}.service %else %post /sbin/chkconfig --add %{name} %preun if [ \"$1\" = 0 ] ; then /sbin/service %{name} stop >/dev/null 2>&1 /sbin/chkconfig --del %{name} fi %endif %clean rm -rf %{buildroot} %files -f INSTALLED_FILES %defattr(-,root,root,-) %dir %attr(750, root, Filebackup) %{_sysconfdir}/%{name}.d %dir %attr(750, Filebackup, Filebackup) %{_sharedstatedir}/%{name} %if 0%{?fedora} >= 14 || 0%{?rhel} >= 7 %{_unitdir}/%{name}.service %else %{_initrddir}/%{name} %{_sysconfdir}/Filebackup.d/%{name} %endif %doc %changelog * Tue Aug 22 2017 mh - 0.19.0-1 - Bumped version to 0.19.0 * Wed Apr 05 2017 mh - Bumped version to 0.18.2 - remove legacy location /etc/Filebackup/ * Wed Sep 28 2016 Andy Bohne - Bumped version to 0.16.0 * Thu Jun 30 2016 Paul Lussier - Created new spec file to build Filebackup for rhel7 7、将文件放入rpmbuild打包目录： [root@leon ~]# tree rpmbuild/ rpmbuild/ ├── BUILD ├── BUILDROOT ├── RPMS ├── SOURCES │ ├── Filebackup-1.0.0 │ │ ├── Filebackup │ │ │ ├── backup.py │ │ │ ├── Filebackup.py │ │ │ ├── file_remove.py │ │ │ ├── file_sync.py │ │ │ └── __init__.py │ │ ├── MANIFEST.in │ │ ├── README.txt │ │ ├── requirements.txt │ │ └── setup.py │ ├── Filebackup-1.0.0.tar.gz │ └── Filebackup.service ├── SPECS │ └── Filebackup.spec └── SRPMS 8、进入SPECS目录，并执行打包命令： cd /root/rpmbuild/SPECS rpmbuild -bb Filebackup.spec 执行完成以上命令以后，rpm生成并存放在/root/rpmbuild/RPMS目录下面。 9、其他specs文件 [root@delta SPECS]# cat grafana.spec %global debug_package %{nil} Name: grafana Version: 5.1.3 Release: 7%{?dist} Summary: Grafana is an open source, feature rich metrics dashboard and graph editor License: ASL 2.0 URL: https://github.com/grafana/grafana Source0: grafana-5.1.3.tar.gz ExclusiveArch: x86_64 BuildRequires: golang BuildRequires: systemd BuildRequires: phantomjs Requires(post): systemd Requires(preun): systemd Requires(postun): systemd %description Grafana is an open source, feature rich metrics dashboard and graph editor for Graphite, InfluxDB & OpenTSDB. %prep %setup -n grafana-5.1.3 %build export GOPATH=%{_builddir} mkdir -p %{_builddir}/src/github.com/grafana/grafana #cp -rf /root/rpmbuild/gopath/src/* %{_builddir}/src/ #cp -rf /root/rpmbuild/gopath/pkg/* %{_builddir}/pkg/ cd %{_builddir}/src/github.com/grafana/grafana go run build.go setup go run build.go build npm install -g yarn yarn install --pure-lockfile npm run dev %install cd %{_builddir}/src/github.com/grafana/grafana install -d -p %{buildroot}%{_datadir}/%{name} cp -pav *.md %{buildroot}%{_datadir}/%{name} cp -rpav docs %{buildroot}%{_datadir}/%{name} cp -rpav public %{buildroot}%{_datadir}/%{name} cp -rpav scripts %{buildroot}%{_datadir}/%{name} install -d -p %{buildroot}%{_sbindir} cp bin/%{name}-server %{buildroot}%{_sbindir}/ cp bin/%{name}-cli %{buildroot}%{_sbindir}/ install -d -p %{buildroot}%{_sysconfdir}/%{name} cp conf/defaults.ini %{buildroot}%{_sysconfdir}/%{name}/defaults.ini cp conf/sample.ini %{buildroot}%{_sysconfdir}/%{name}/sample.ini cp conf/ldap.toml %{buildroot}%{_sysconfdir}/%{name}/ldap.toml cp -rpav conf %{buildroot}%{_datadir}/%{name} mkdir -p %{buildroot}%{_unitdir} install -p -m 0644 packaging/rpm/systemd/grafana-server.service %{buildroot}%{_unitdir}/ mkdir -p %{buildroot}%{_sysconfdir}/sysconfig install -p -m 0644 packaging/rpm/sysconfig/grafana-server %{buildroot}%{_sysconfdir}/sysconfig install -d -p %{buildroot}%{_sharedstatedir}/%{name} install -d -p %{buildroot}/var/log/%{name} mkdir -p %{buildroot}%{_datadir}/%{name}/vendor/phantomjs install -p tools/phantomjs/render.js %{buildroot}%{_datadir}/%{name}/vendor/phantomjs ln -s /usr/bin/phantomjs %{buildroot}%{_datadir}/%{name}/vendor/phantomjs/phantomjs %files %attr(0755, root, root) %{_sbindir}/%{name}-server %attr(0755, root, root) %{_sbindir}/%{name}-cli %attr(0755, root, grafana) %dir %{_sysconfdir}/%{name} %attr(0640, root, grafana) %{_sysconfdir}/%{name}/defaults.ini %attr(0640, root, grafana) %{_sysconfdir}/%{name}/sample.ini %attr(0640, root, grafana) %{_sysconfdir}/%{name}/ldap.toml %attr(0755, grafana, grafana) %dir %{_sharedstatedir}/%{name} %attr(0755, grafana, grafana) %dir /var/log/%{name} %attr(-, root, root) %{_unitdir}/grafana-server.service %attr(-, root, root) %{_sysconfdir}/sysconfig/grafana-server %attr(-, root, root) %{_datadir}/%{name} %exclude %{_datadir}/%{name}/*.md %exclude %{_datadir}/%{name}/docs %attr(-, root, root) %doc %{_datadir}/%{name}/CHANGELOG.md %attr(-, root, root) %doc %{_datadir}/%{name}/LICENSE.md %attr(-, root, root) %doc %{_datadir}/%{name}/NOTICE.md %attr(-, root, root) %doc %{_datadir}/%{name}/README.md %attr(-, root, root) %doc %{_datadir}/%{name}/ROADMAP.md %attr(-, root, root) %doc %{_datadir}/%{name}/docs %pre getent group grafana >/dev/null || groupadd -r grafana getent passwd grafana >/dev/null || \\ useradd -r -g grafana -d /etc/grafana -s /sbin/nologin \\ -c \"Grafana Dashboard\" grafana exit 0 %post %systemd_post grafana.service %preun %systemd_preun grafana.service %postun %systemd_postun grafana.service %changelog * Wed May 24 2018 hl 5.1.3-7 - Bump 5.1.3-7 - change build step * Wed May 24 2018 hl 5.1.3-6 - Bump 5.1.3-6 - fix defaults.ini - add sample.ini * Wed May 23 2018 hl 5.1.3-5 - Bump 5.1.3-4 [root@delta SPECS]# cat openstack-kolla.spec %global milestone .0rc1 %{!?upstream_version: %global upstream_version %{version}%{?milestone}} %global common_desc \\ Templates and tools from the Kolla project to build OpenStack container images. Name: openstack-kolla Version: 6.0.0 Release: 0.1%{?milestone}%{?dist} Summary: Build OpenStack container images License: ASL 2.0 URL: http://pypi.python.org/pypi/kolla Source0: https://tarballs.openstack.org/kolla/kolla-%{upstream_version}.tar.gz # # patches_base=6.0.0.0rc1 # BuildArch: noarch BuildRequires: python2-setuptools BuildRequires: python2-devel BuildRequires: python2-pbr BuildRequires: python2-oslo-config BuildRequires: crudini Requires: python-gitdb Requires: python2-pbr >= 2.0.0 Requires: GitPython Requires: python2-jinja2 >= 2.8 Requires: python2-docker >= 2.4.2 Requires: python2-six >= 1.10.0 Requires: python2-oslo-config >= 2:5.1.0 Requires: python2-oslo-utils >= 3.33.0 Requires: python2-cryptography >= 1.7.2 Requires: python2-netaddr %description %{common_desc} %prep %setup -q -n kolla-%{upstream_version} %build PYTHONPATH=. oslo-config-generator --config-file=etc/oslo-config-generator/kolla-build.conf %py2_build %install %py2_install mkdir -p %{buildroot}%{_datadir}/kolla/docker cp -vr docker/ %{buildroot}%{_datadir}/kolla # setup.cfg required for kolla-build to discover the version install -p -D -m 644 setup.cfg %{buildroot}%{_datadir}/kolla/setup.cfg # remove tests rm -fr %{buildroot}%{python2_sitelib}/kolla/tests # remove tools rm -fr %{buildroot}%{_datadir}/kolla/tools install -d -m 755 %{buildroot}%{_sysconfdir}/kolla crudini --set %{buildroot}%{_datadir}/kolla/etc_examples/kolla/kolla-build.conf DEFAULT tag %{version}-%{release} cp -v %{buildroot}%{_datadir}/kolla/etc_examples/kolla/kolla-build.conf %{buildroot}%{_sysconfdir}/kolla rm -fr %{buildroot}%{_datadir}/kolla/etc_examples %files %doc README.rst %doc %{_datadir}/kolla/doc %license LICENSE %{_bindir}/kolla-build %{python2_sitelib}/kolla* %{_datadir}/kolla %{_sysconfdir}/kolla %changelog * Thu Mar 01 2018 RDO 6.0.0-0.1.0rc1 - Update to 6.0.0.0rc1 [root@delta SPECS]# cat deltaclient.spec %define userpath /etc/deltaclient %define binpath /usr/bin Name: deltaclient Version: 1.0.0 Release: 1%{?dist} Summary: deltaclient Group: tmp License: GPL URL: http://www.tmp.com Source0: %{name}-%{version}.tar.gz BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root %description deltaclient %prep %setup -c %install install -d $RPM_BUILD_ROOT%{userpath} install -d $RPM_BUILD_ROOT%{binpath} cp -r %{name}-%{version}/resources/* $RPM_BUILD_ROOT%{userpath} cp %{name}-%{version}/bin/* $RPM_BUILD_ROOT%{binpath} %files %defattr(-,root,root) %{userpath} %{binpath} %changelog * Tue May 8 2018 hl -1.0.0 - init %define userpath /usr/share/ Name: tmp Version: 1.0.0 Release: 1%{?dist} Summary: kystack tools BuildArch: noarch Group: tmp License: MIT URL: tmp.com Source0: kystack.tar.gz BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root %description %prep %setup -c %install install -d $RPM_BUILD_ROOT%{userpath} cp -a kystack $RPM_BUILD_ROOT%{userpath} %clean rm -rf $RPM_BUILD_ROOT rm -rf $RPM_BUILD_DIR/%{name}-%{version} %files %defattr(-,root,root) %{userpath} %changelog Name: deltaclient Version: 1.1.1.0 Release: 1%{?dist} Summary: deltaclient Group: tmp License: GPL URL: http://www.tmp.com Source0: %{name}-%{version}.tar.gz BuildRoot: %{_tmppath}/%{name}-%{version}-%{release}-root %description deltaclient %prep %setup -c %install mkdir -p %{buildroot}/etc/deltaclient/ mkdir -p %{buildroot}/usr/bin mkdir -p %{buildroot}/usr/share mkdir -p %{buildroot}/usr/share/applications/ mkdir -p %{buildroot}/var/log/ cp -ar %{name}-%{version}/resources/* %{buildroot}/etc/deltaclient/ cp -ar %{name}-%{version}/bin/deltaclient %{buildroot}/usr/bin/ cp -ar %{name}-%{version}/bin/tmp-remote-viewer %{buildroot}/usr/bin/ cp -ar %{name}-%{version}/virt-viewer %{buildroot}/usr/share/ cp -ar %{name}-%{version}/deltaclient.desktop %{buildroot}/usr/share/applications/ cp -ar %{name}-%{version}/deltaclient.log %{buildroot}/var/log/deltaclient.log chmod -R 777 %{buildroot}/etc/deltaclient/ chmod -R 777 %{buildroot}/var/log/deltaclient.log %files %defattr(-,root,root) /etc/deltaclient/ /usr/bin/deltaclient /usr/bin/tmp-remote-viewer /usr/share/virt-viewer /usr/share/applications/deltaclient.desktop /var/log/deltaclient.log %changelog * Tue May 8 2018 hl -1.1.1.0 - build for kylin 4 "},"notes/linux/crontabs.html":{"url":"notes/linux/crontabs.html","title":"crontabs 计划任务相关","keywords":"","body":"crontabs 计划任务相关 * * * * * - - - - - | | | | | | | | | +---- 星期几 (0 - 7, 其中 0 和 7 都表示周日) | | | +------ 月份 (1 - 12) | | +-------- 日期 (1 - 31) | +---------- 小时 (0 - 23) +------------ 分钟 (0 - 59) 1、安装 yum install crontabs systemctl enable crond systemctl start crond 2、配置 vim /etc/crontab 分钟(0-59) 小时(0-23) 日(1-31) 月(11-12) 星期(0-6,0表示周日) 用户名 要执行的命令 例如: 每天早上5点定时重启系统 0 5 * * * root reboot 每一小时重启smb * */1 * * * /etc/init.d/smb restart 每天早上6点 0 6 * * * echo \"Good morning.\" >> /tmp/test.txt //注意单纯echo，从屏幕上看不到任何输出，因为cron把任何输出都email到root的信箱了。 每两个小时 0 */2 * * * echo \"Have a break now.\" >> /tmp/test.txt 晚上11点到早上8点之间每两个小时和早上八点 0 23-7/2，8 * * * echo \"Have a good dream\" >> /tmp/test.txt 每个月的4号和每个礼拜的礼拜一到礼拜三的早上11点 0 11 4 * 1-3 command line 1月1日早上4点 0 4 1 1 * command line SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root //如果出现错误，或者有数据输出，数据作为邮件发给这个帐号 HOME=/ 每小时执行/etc/cron.hourly内的脚本 01 * * * * root run-parts /etc/cron.hourly 每天执行/etc/cron.daily内的脚本 02 4 * * * root run-parts /etc/cron.daily 每星期执行/etc/cron.weekly内的脚本 22 4 * * 0 root run-parts /etc/cron.weekly 每月去执行/etc/cron.monthly内的脚本 42 4 1 * * root run-parts /etc/cron.monthly 注意: \"run-parts\"这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是文件夹名。 　 每天的下午4点、5点、6点的5 min、15 min、25 min、35 min、45 min、55 min时执行命令。 5，15，25，35，45，55 16，17，18 * * * command 每周一，三，五的下午3：00系统进入维护状态，重新启动系统。 00 15 * * 1，3，5 shutdown -r +5 每小时的10分，40分执行用户目录下的innd/bbslin这个指令： 10，40 * * * * innd/bbslink 每小时的1分执行用户目录下的bin/account这个指令： 1 * * * * bin/account 每天早晨三点二十分执行用户目录下如下所示的两个指令（每个指令以;分隔）： 20 3 * * * （/bin/rm -f expire.ls logins.bad;bin/expire$#@62;expire.1st）　　 每年的一月和四月，4号到9号的3点12分和3点55分执行/bin/rm -f expire.1st这个指令，并把结果添加在mm.txt这个文件之后（mm.txt文件位于用户自己的目录位置）。 12,55 3 4-9 1,4 * /bin/rm -f expire.1st$#@62;$#@62;mm.txt 3、加载任务,使之生效 crontab /etc/crontab 4、查看任务 crontab -l "},"notes/linux/diy_centos_iso.html":{"url":"notes/linux/diy_centos_iso.html","title":"CentOS iso 镜像定制","keywords":"","body":"CentOS iso 镜像定制 内核 kernel=images/pxeboot/vmlinuz 根文件系统 initrd=images/pxeboot/initrd.img 开机引导系统安装的内核（vmlinuz）及RAM镜像(initrd.img)，在引导系统时会载入内存，给系统的安装提供一个Linux安装引导平台，文件夹中还有在不同模式下显示信息的boot.msg文件 Anaconda是RedHat、CentOS、Fedora等Linux的安装管理程序，使用Python编写。可以提供文本、图形等安装管理方式，并支持Kickstart等脚本提供自动安装的功能。 mksquashfs /mnt /root/install.img –all-root -noF usr/bin/anaconda 安装程序的主执行文件 usr/lib/anaconda/iw/ 图形安装模式的模块 usr/lib/anaconda/textw/ 文本安装模式的模块。 usr/share/anaconda/pixmaps/ 图形安装过程的图片 usr/share/anaconda/ui/ 安装过程中显示的文字 usr/lib/anaconda/dispatch.py 来控制整个安装的流程，当一个Next或Back按钮被单击时的跳转 usr/lib/anaconda/vnc.py 用于控制对VNC进行设置（当在安装过程中请求了VNC时） 1、下载并上传CentOS-7-x86_64-Minimal-1810.iso到虚拟机。 centos下载地址 2、创建目录media，并挂载iso。 mkdir /media mount CentOS-7-x86_64-Minimal-1810.iso /media 3、创建镜像目录，并拷贝/media下的所有文件。 mkdir /root/TMPOS cp -pRf /media/* /root/TMPOS/ cp /media/.discinfo /root/TMPOS/ cp /media/.treeinfo /root/TMPOS/ 4、修改centos-release（可选）。 iso 系统定制 %define product_family CentOS http://rpm.pbone.net/index.php3/stat/3/srodzaj/2/search/centos-release-7-3.1611.el7.centos.src.rpm 5、修改centos-logos（可选）。 tar -xf centos-logos-70.0.6.tar.xz tar -cvf centos-logos-70.0.6.tar.xz centos-logos-70.0.6/ cd centos-logos-70.0.6/pixmaps 6、修改isolinux（可选）。 mkdir tmp cp initrd.img tmp/ xz -dc initrd.img |cpio -id vim .buildstamp ... Product=CentOS ... cd usr/share/pixmaps find . |cpio -c -o |xz -9 --format=lzma > initrd.img 7、修改LiveOS（可选）。 cd LiveOS/ unsquashfs squashfs.img cd squashfs-root mkdir t mount -o rw rootfs.img t cd usr/share/anaconda/pixmaps/ cd usr/share/anaconda/pixmaps/rnotes/en/ vim etc/os-release vim etc/redhat-release vim etc/system-release vim etc/centos-release vim usr/lib64/python2.7/site-packages/pyanaconda/ui/gui/hubs/progress.py #self.set_warning(_(\"Use of this product is subject to the license agreement found at %s\") % eulaLocation) umoun t rm -rf t rm -rf squashfs.img mksquashfs squashfs-root/ squashfs.img -all-root -noF rm -rf squashfs-root 8、准备ks.cfg文件 isolinux/ks.cfg。 安装前运行自定义脚本 %pre表示系统安装前，此时ISO镜像文件被挂载到内存中Linux的/mnt/source 安装后运行自定义脚本 %post 在系统安装后执行 –不带参数，其实就是在真实的操作系统里操作。 –nochroot 已安装的真实操作系统被挂载到内存虚拟操作系统中的/mnt/sysimage目录。这个参数的用途主要是配合%pre使用的。先将光盘里的文件copy到内存运行的虚拟操作系统，再从内存虚拟操作系统copy到已安装的真实操作系统操作。 %post --nochroot mkdir /media mount /dev/cdrom /media/ cp /media/test1.txt /mnt/sysimage/root/ %end [说明]：上面命令实现了从ISO镜像中拷贝文本文件到安装好的真实操作系统中。 #version=DEVEL # Install OS instead of upgrade install # Keyboard layouts keyboard --vckeymap=us --xlayouts='us' # Reboot after installation reboot # Root password rootpw 123456 user --name=\"kyos\" --password=\"123456\" --groups=users # System timezone timezone Asia/Shanghai --isUtc --nontp # System language lang en_US.UTF-8 # Firewall configuration firewall --disabled # SELinux configuration selinux --disabled # System authorization information auth --useshadow --passalgo=sha512 # Use graphical install #graphical text firstboot --enable # SELinux configuration selinux --disabled # ignore unsupported hardware warning unsupported_hardware # SKIP CONFIGURING X skipx # Network information network --hostname=kyos #Allow partition the system as needed %include /tmp/partition.ks %pre #!/bin/sh drives=\"\" for drv in `ls -1 /sys/block | grep \"sd\\|hd\\|vd\\|cciss\"`; do if (grep -q 0 /sys/block/${drv}/removable); then d=`echo ${drv} | sed -e 's/!/\\//'` drives=\"${drives} ${d}\" fi done default_drive=`echo ${drives} | awk '{print $1}'` act_mem=`cat /proc/meminfo | grep MemTotal | awk '{printf(\"%d\",$2/1024)}'` if [ $act_mem -ge 16384 ];then act_mem=16384 fi echo \"\" > /tmp/partition.ks # System bootloader configuration echo \"bootloader --location=mbr --driveorder=${default_drive}\" >> /tmp/partition.ks # Clear the Master Boot Record echo \"zerombr\" >> /tmp/partition.ks # Partition clearing information echo \"clearpart --all --drives=${default_drive}\" >> /tmp/partition.ks echo \"part /boot --fstype=xfs --ondisk=${default_drive} --asprimary --size=500\" >> /tmp/partition.ks echo \"part /boot/efi --fstype=efi --ondisk=${default_drive} --asprimary --size=500\" >> /tmp/partition.ks echo \"part swap --fstype=swap --ondisk=${default_drive} --size=${act_mem}\" >> /tmp/partition.ks echo \"part / --fstype=xfs --grow --ondisk=${default_drive} --size=1\" >> /tmp/partition.ks %end %packages @core @kolla %end %post --log=/var/log/post.log #!/bin/bash #change net device to ethx #sed -i 's/quiet\"/quiet net.ifnames=0 biosdevname=0\"/g' /etc/default/grub #grub2-mkconfig -o /boot/grub2/grub.cfg cd /etc/sysconfig/network-scripts before_network_script=$(ls ifcfg-*|grep -v lo|grep -v eth*) net_dev_num=$(ls ifcfg-*|grep -v lo|wc -l) rm -rf $before_network_script for ((i=0;i /etc/sysconfig/network-scripts/ifcfg-eth$i done #add pcadmin to sudoers echo \"tmpos ALL=(ALL) ALL\" >> /etc/sudoers # add tmpos to group users usermod -a -G users tmpos # kolla systemctl disable postfix systemctl disable NetworkManager systemctl enable docker systemctl enable ntpd.service mkdir -p /etc/systemd/system/docker.service.d tee /etc/systemd/system/docker.service.d/kolla.conf 9、修改isolinux.cfg文件,isolinux/isolinux.cfg。 找到lable linux，在append里面添加ks=cdrom:/isolinux/ks.cfg net.ifnames=0 biosdevname=0，修改LABLE=TMPOS（在之后构建TMPOS时-V参数会用到）。 net.ifnames=0 biosdevname=0 修改网卡名为ethx。 ... label linux menu label ^Install TMPOS kernel vmlinuz # append initrd=initrd.img inst.stage2=hd:LABEL=CentOS\\x207\\x20x86_64 quiet append initrd=initrd.img ks=cdrom:/isolinux/ks.cfg inst.stage2=hd:LABEL=TMPOS quiet net.ifnames=0 biosdevname=0 ... [说明]：斜体部分可选，代表是否在安装时对网络部分进行提示交互。 其他相关label可以根据需求注释掉。 注意点： 1）memu label 后面的内容是在光盘引导起来菜单的内容，^后面的字母是菜单的快捷键； 2）通过inst.ks关键字指明ks.cfg文件位置； 3）inst.stages2标识的是系统按照介质位置，这里使用hd:LABEL表明寻找的是label为TMPOS的安装介质，使用LABEL关键字的好处是可以精确指定安装介质，为什么label是TMPOS，是因为我在制作光盘镜像的时候指定的，方法在后面有介绍。 default vesamenu.c32 timeout 10 display boot.msg # Clear the screen when exiting the menu, instead of leaving the menu displayed. # For vesamenu, this means the graphical background is still displayed without # the menu itself for as long as the screen remains in graphics mode. menu clear menu background splash.png menu title KYOS menu vshift 8 menu rows 18 menu margin 8 #menu hidden menu helpmsgrow 15 menu tabmsgrow 13 # Border Area menu color border * #00000000 #00000000 none # Selected item menu color sel 0 #ffffffff #00000000 none # Title bar menu color title 0 #ff7ba3d0 #00000000 none # Press [Tab] message menu color tabmsg 0 #ff3a6496 #00000000 none # Unselected menu item menu color unsel 0 #84b8ffff #00000000 none # Selected hotkey menu color hotsel 0 #84b8ffff #00000000 none # Unselected hotkey menu color hotkey 0 #ffffffff #00000000 none # Help text menu color help 0 #ffffffff #00000000 none # A scrollbar of some type? Not sure. menu color scrollbar 0 #ffffffff #ff355594 none # Timeout msg menu color timeout 0 #ffffffff #00000000 none menu color timeout_msg 0 #ffffffff #00000000 none # Command prompt text menu color cmdmark 0 #84b8ffff #00000000 none menu color cmdline 0 #ffffffff #00000000 none # Do not display the actual menu unless the user presses a key. All that is displayed is a timeout message. menu tabmsg Press Enter to continue. menu separator # insert an empty line menu separator # insert an empty line label linux menu label ^Install TMPOS menu default kernel vmlinuz append initrd=initrd.img inst.stage2=hd:LABEL=TMPOS quiet net.ifnames=0 biosdevname=0 inst.ks=hd:LABEL=TMPOS:/isolinux/ks.cfg menu separator # insert an empty line menu separator # insert an empty line menu end 10、安装相关软件。 yum -y install xorriso syslinux isomd5sum squashfs-tools createrepo 11、准备efi启动。 mkdir -p /root/TMPOS/efi_image dd bs=1M count=100 if=/dev/zero of=/root/TMPOS/efiboot.img mkfs.vfat -n EFI /root/TMPOS/efiboot.img umount -l /root/TMPOS/efi_image || true mount /root/TMPOS/efiboot.img /root/TMPOS/efi_image echo \"default=0\" >> /root/TMPOS/EFI/BOOT/BOOTX64.conf echo \"timeout 60\" >> /root/TMPOS/EFI/BOOT/BOOTX64.conf echo \"hiddenmenu\" >> /root/TMPOS/EFI/BOOT/BOOTX64.conf echo \"title Install TMPOS\" >> /root/TMPOS/EFI/BOOT/BOOTX64.conf echo \" kernel /vmlinuz biosdevname=0 showmenu=yes inst.stage2=hd:LABEL=TMPOS quiet inst.ks=hd:LABEL=TMPOS:/isolinux/ks.cfg\" >> /root/TMPOS/EFI/BOOT/BOOTX64.conf echo \" initrd /initrd.img\" >> /root/TMPOS/EFI/BOOT/BOOTX64.conf 修改 /root/TMPOS/EFI/BOOT/grub.cfg 添加 ... menuentry 'Install TMPOS' --class fedora --class gnu-linux --class gnu --class os { linuxefi /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=TMPOS quiet net.ifnames=0 biosdevname=0 inst.ks=hd:LABEL=TMPOS:/isolinux/ks.cfg initrdefi /images/pxeboot/initrd.img ... cp -r /root/TMPOS/isolinux/vmlinuz /root/TMPOS/efi_image/ cp -r /root/TMPOS/isolinux/initrd.img /root/TMPOS/efi_image/ cp -rf /root/TMPOS/EFI/ /root/TMPOS/efi_image/ umount /root/TMPOS/efi_image mv -f /root/TMPOS/efiboot.img /root/TMPOS/images/ rm -rf /root/TMPOS/efi_image/ 12、准备安装源。 1）将需要的源追加复制到/root/TMPOS/Packages中。 2) 清除/root/TMPOS/repodata 中除了*.comps.xml外的其他文件，并将新加的包名写入*.comps.xml里 ntp vim-enhanced iftop telnet nmap mlocate net-tools atop iftop vim wget tmux iotop htop expect tree python2-pip epel-release kolla Kolla Kolla Kolla packages true true ansible kolla-ansible docker-ce python-docker-py python2-pbr python2-oslo-utils python2-oslo-config python2-jinja2 openstack-selinux python2-openstackclient python2-magnumclient 3）构建源 yum -y install createrepo cd /root/TMPOS createrepo -g repodata/*comps.xml . 4) 在/root/TMPOS/isolinux/ks.cf中添加相应组名 %packages @core @kolla %end 13、构建TMPOS。 mkisofs参数简介 -o 指定映像文件的名称。 -o /root/boot.iso ：创建完后放在哪 -b指定在制作可开机光盘时所需的开机映像文件。 -c 制作可开机光盘时，会将开机映像文件中的no-eltorito-catalog全部内容作成一个文件。 -c isolinux/boot.cat ：指明引导时显示菜单的文件 -no-emul-boot 非模拟模式启动。 -boot-load-size 4 设置载入部分的数量。 -boot-info-table 在启动的图像中现实信息。 -joliet-long 使用 joliet 格式的目录与文件名称，长文件名支持。 -R 或 -rock 使用 Rock RidgeExtensions 。 -J 或 -joliet 使用 Joliet 格式的目录与文件名称。 -v 或 -verbose 执行时显示详细的信息。 -T 或-translation-table 建立文件名的转换表，适用于不支持 Rock Ridge Extensions 的系统上。 -V “CentOS 6.6 X86_64 boot disk”：指明光盘标签为”CentOS 6.6 X86_64 boot disk“ xorriso -as mkisofs \\ -V \"TMPOS\" -p \"tmp\" \\ -J -R \\ -graft-points \\ -b isolinux/isolinux.bin -no-emul-boot -boot-load-size 4 -boot-info-table \\ -isohybrid-mbr /usr/share/syslinux/isohdpfx.bin \\ -eltorito-alt-boot -e images/efiboot.img -no-emul-boot \\ -isohybrid-gpt-basdat \\ -o /root/TMPOS/TMPOS.iso /root/TMPOS/ \\ --boot-catalog-hide implantisomd5 /root/TMPOS/TMPOS.iso 14、make.sh "},"notes/linux/ansible.html":{"url":"notes/linux/ansible.html","title":"ansible 相关","keywords":"","body":"ansible 相关 一、基础使用 1、配置文件 ansible 应用程序的 主配置文件：/etc/ansible/ansible.cfg Host Inventory 定义管控主机 ：/etc/ansible/hosts [both] master node1 node2 [node] node1 node2 遵循 INI风格；中括号中的字符是组名；一个主机可同时属于多个组； 2、ansible-doc命令：获取模块列表，及模块使用格式； ansible-doc -l ：获取列表 ansible-doc -s module_name ：获取指定模块的使用信息 3、模块 command 默认模块，可省略 shell 以shell解释器执行脚本 raw script 将本地脚本传送到远端节点上执行 ping yum template copy user group 模块用于在受控机上添加或删除组 service script 常用文件模块 模块名称 模块说明 blockinfile 插入、更新或删除由可自定义标记线包围的多行文本块 copy 将文件从本地或远程计算机复制到受管主机上的某个位置。 类似于file模块，copy模块还可以设置文件属性，包括SELinux上下文件。 fetch 此模块的作用和copy模块类似，但以相反方式工作。此模块用于从远程计算机获取文件到控制节点， 并将它们存储在按主机名组织的文件树中。 file 设置权限、所有权、SELinux上下文以及常规文件、符号链接、硬链接和目录的时间戳等属性。 此模块还可以创建或删除常规文件、符号链接、硬链接和目录。其他多个与文件相关的 模块支持与file模块相同的属性设置选项，包括copy模块。 lineinfile 确保特定行位于某文件中，或使用反向引用正则表达式来替换现有行。 此模块主要在用户想要更改文件的某一行时使用。 stat 检索文件的状态信息，类似于Linux中的stat命令。 synchronize 围绕rsync命令的一个打包程序，可加快和简化常见任务。 synchronize模块无法提供对rsync命令的完整功能的访问权限，但确实最常见的调用更容易实施。 用户可能仍需通过run command模块直接调用rsync命令。 4、变量使用示例： Tasks 任务、 Variables 变量、 Templates 模板、 Handlers 处理器、 Roles 角色 [root@localhost~]# vim useradd.yml -hosts: websrvs remote_user: root vars: username: testuser password: xuding tasks: -name: add user user: name={{ username }} state=present -name: set password shell: /bin/echo {{ password }} |/usr/bin/passwd --stdin {{ username }} 调用变量\\{\\{ \\}\\} ansible-playbook /PATH/TO/SOME_YAML_FILE { -eVARS|--extra-vars=VARS} 变量的重新赋值调用方法 [root@localhost ~]# ansible-playbook useradd.yml --extra-vars \"username=ubuntu\" playbook--- tasks 5、条件测试： 在某task后面添加when子句即可实现条件测试功能；when语句支持Jinja2语法； 实例：当时 RedHat 系列系统时候调用 yum 安装 tasks: -name: install web server package yum: name=httpd state=present when: ansible_os_family == \"RedHat\" 6、迭代： item 在task中调用内置的item变量；在某task后面使用with_items语句来定义元素列表； tasks: -name: add four users with_items: -testuser1 -testuser2 -testuser3 -testuser4 注意：迭代中，列表中的每个元素可以为字典格式； 实例： -name: add two users user: name= state=present groups= with_items: - { name: 'testuser5', groups: 'wheel' } - { name: 'testuser6', groups: 'root' } playbook--- handlers： 处理器；触发器 只有其关注的条件满足时，才会被触发执行的任务； 7、playbook 模板 templates： 用于生成文本文件（配置文件）；模板文件中可使用jinja2表达式，表达式要定义在\\{\\{\\}\\}，也可以简单地仅执行变量替换； roles： roles用于实现“代码复用”； roles以特定的层次型格式组织起来的playbook元素（variables,tasks, templates, handlers）； 可被playbook以role的名字直接进行调用； 用法 ：在 roles/ 下建立 [group_name] 子目录，并非全部都要创建；例如： /etc/ansible/roles/ （在 /etc/ansible/ansible.cfg 定义 roles 目录） webserver/ files/：此角色中用到的所有文件均放置于此目录中； templates/：Jinja2模板文件存放位置； tasks/：任务列表文件；可以有多个，但至少有一个叫做main.yml的文件； handlers/：处理器列表文件；可以有多个，但至少有一个叫做main.yml的文件； vars/：变量字典文件；可以有多个，但至少有一个叫做main.yml的文件； meta/：此角色的特殊设定及依赖关系； --- - hosts: webservers vars: http_port: 80 max_clients: 200 remote_user: root tasks: - name: ensure apache is at the latest version yum: pkg=httpd state=latest - name: write the apache config file template: src=/srv/httpd.j2 dest=/etc/httpd.conf notify: - restart apache - name: ensure apache is running service: name=httpd state=started handlers: - name: restart apache service: name=httpd state=restarted 8、debug 选项 描述 -v 显示任务结果 -vv 任务结果和任务配置都会显示 -vvv 包含关于与受管主机连接的信息 -vvvv 增加了连接插件相关的额外详细程序选项，包括受管主机上用于执行脚本的用户以及所执行的脚本 9、语法验证 ansible-playbook --syntax-check webserver.yml 10、执行空运行 -C选项对playbook执行空运行。使Ansible报告在执行该playbook时将会发生什么更改，但不会对受管主机进行任何实际的更改。 ansible-playbook -C webserver.yml 11、实施处理程序 tasks: - name: copy demo.example.conf configuratioon template # 通知处理程序的任务 template: src: /var/lib/templates/demo.example.conf.template dest: /etc/httpd/conf.d/demo.example.conf notify: # notify语句指出该任务需要触发一个处理程序 - restart apache # 要运行的处理程序的名称 handlers: # handlers关键字表示处理程序任务列表的开头 - name: restart apache # 被任务调用的处理程序的名称 service: # 用于该处理程序的模块 name: httpd state: restarted 12、ansible 角色子目录 子目录 功能 defaults 此目录中的main.yml文件包含角色变量的默认值，使用角色时可以覆盖这些默认值。 这些变量的优先级较低，应该在play中更改和自定义。 files 此目录包含由角色任务引用的静态文件。 handlers 此目录中的main.yml文件包含角色的处理程序定义。 meta 此目录中的main.yml文件包含与角色相关的信息，如作者、许可证、平台和可选的角色依赖项。 tasks 此目录中的main.yml文件包含角色的任务定义。 templates 此目录包含由角色任务引用的Jinja2模板。 tests 此目录可以包含清单和名为test.yml的playbook，可用于测试角色。 vars 此目录中的main.yml文件定义角色的变量值。这些变量通常用于角色内部用途。 这些变量的优先级较高，在playbook中使用时不应更改。 13、导入 task task.yaml --- - name: Install the {{ package }} package yum: name: \"{{ package }}\" state: latest - name: Start the {{ service }} service service: name: \"{{ service }}\" enabled: True state: started tasks: - name: Import task file and set variables import_tasks: task.yml vars: package: httpd service: service 14、只执行指定 tag 并且有 tag 时执行 - name: clean import_tasks: clean.yml when: \"'clean' in ansible_run_tags\" tags: - clean 15、delegate_to - name: 创建 local storage 本地目录 shell: cmd: | rm -rf {{ local_path }} mkdir -p {{ local_path }} delegate_to: \"{{ item }}\" loop: \"{{ groups['tmp'] }}\" 16、when - hosts: localhost roles: - cluster-addon - { role: tmp, when: enable_tmp=='yes' } when: label is defined and label != \"\" when: (enable_tmp is defined and enable_tmp == 'yes') or (enable_all is defined and enable_all == 'yes') 17、ignore_unreachable | ignore_errors: true - name: 清理 /etc/hosts shell: cmd: | echo \"{{ conf }}\"|grep address|cut -d'/' -f2|xargs -i sed -i \"/{}/d\" /etc/hosts delegate_to: \"{{ item }}\" ignore_unreachable: yes ignore_errors: true with_items: - \"{{ server }}\" - \"{{ node }}\" 18、copy - name: 分发 resolv.conf copy: src={{ dir }}/yml/{{ name }}/resolv.conf dest=/etc/resolv.conf delegate_to: \"{{ item }}\" with_items: - \"{{ server }}\" - \"{{ node }}\" 19、restart - name: 重启服务端 dnsmasq service: name: dnsmasq state: restarted use: systemd enabled: yes delegate_to: \"{{ item }}\" loop: \"{{ server }}\" 二、常用命令 # 拷贝文件 ansible all -m copy -a \"src=/etc/profile dest=/etc force=yes\" # ping ansible all -m ping ansible all -m ping -u root --ask-pass # 启动服务 ansible webservs -m service -a 'enabled=true name=httpd state=started' # 管道符 ansible all -m shell -a 'echo 123..com | passwd --stdin user1' # 执行脚本 ansible all -m script -a \"/tmp/a.sh\" # 安装程序，卸载加state=absent ansible all -m yum -a \"name=zsh\" ansible all -m yum -a \"name=zsh state=absent\" # 使用git部署webapp ansible webservers -m git -a \"repo=git://foo.example.org/repo.git dest=/srv/myapp version=HEAD\" # touch 一个文件并添加用户读写权限，用户组去除写执行权限，其他组减去读写执行权限 ansible web -m file -a \"path=/etc/foo.conf state=touch mode='u+rw,g-wx,o-rwx'\" # 使用file 模块创建目录，类似mkdir -p ansible web -m file -a \"dest=/tmp/test mode=755 owner=user group=user state=directory\" # 使用file 模块删除文件或者目录 ansible web -m file -a \"dest=/tmp/test state=absent\" # 搜集主机的所有系统信息 ansible all -m setup # 搜集和内存相关的信息 ansible all -m setup -a 'filter=ansible_*_mb' # 搜集网卡信息 ansible all -m setup -a 'filter=ansible_eth[0-2]' # 搜集系统信息并以主机名为文件名分别保存在/tmp/facts 目录 ansible all -m setup --tree /tmp/facts 三、其他 1、创建目录 - name: 创建目录 file: name={{ tmp_dir }} state=directory connection: local run_once: true 2、执行 shell - name: shell shell: cmd: | \\cp -rf \"{{ base_dir }}/tmp\" \"{{ tmp_dir }}\" connection: local run_once: true 3、渲染 value.yml.j2 - name: render value.yaml template: src={{ item }}.j2 dest={{ tmp_dir }}/{{ item }} with_items: - values.yaml connection: local run_once: true 4、循环 group - name: loop group shell: echo {{ item }} with_items: \"{{ groups['tmp'] }}\" connection: local run_once: true vim label-config.yml labels: a: b c: d - include_vars: \"{{ cluster_dir }}/label-config.yml\" - name: label shell: cmd: | hosts=`echo \"{{ item.value }}\"|tr -d \"',[]\"` label=\"{{ labels[item.key] }}\" for host in ${hosts};do echo $host $label done when: item.key in labels loop: \"{{ groups|combine|dict2items }}\" list -> dict -> items 5、变量渲染 https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#playbooks-filters {{ affinity | to_yaml | indent(2) }} {{ nodeSelector | to_nice_yaml }} {{ nginx.configurationSnippet.httpStart | to_nice_yaml | from_yaml | indent(8) }} 6、引用变量 ansible-playbook -i clusters/test/hosts -e @clusters/test/config.yml -e @clusters/test/lable-config.yml playbooks/106.test.yaml -vvv - hosts: localhost tasks: - include_vars: \"{{ cluster_dir }}/label-config.yml\" 7、过滤所有注释行 grep -v '^[[:blank:]]*#' values.yaml.j2 8、重试循环 - action: shell /usr/bin/foo register: result until: result.stdout.find(\"all systems go\") != -1 retries: 5 delay: 10 - name: 下载etcd二进制文件 copy: src={{ base_dir }}/bin/{{ item.1 }} dest={{ etcd.data_dir }}/{{ item.1 }} mode=0755 delegate_to: \"{{ item.0 }}\" loop: \"{{ groups['ingress'] | product([etcd, etcdctl]) | list }}\" - name: Set etcd nodes string set_fact: tmp_etcd_nodes: \"{{ tmp_etcd_nodes|default([]) + ['etcd-' ~ item ~ '=http://' ~ item ~ ':2380'] }}\" loop: \"{{ groups['ingress'] }}\" - name: Debug tmp_etcd_nodes debug: var: tmp_etcd_nodes - name: 创建etcd的systemd unit文件 template: src: apisix-etcd.service.j2 dest: /etc/systemd/system/apisix-etcd.service vars: etcd_nodes: \"{{ tmp_etcd_nodes | join(',') }}\" etcd_name: \"{{ item }}\" delegate_to: \"{{ item }}\" loop: \"{{ groups['ingress'] }}\" "},"notes/linux/cobbler.html":{"url":"notes/linux/cobbler.html","title":"cobbler 相关","keywords":"","body":"cobbler 相关 一、关闭selinux 1、修改配置 vim /etc/selinux/config #SELINUX=enforcing #SELINUXTYPE=targeted SELINUX=disabled 2、配置生效 setenforce 0 二、关闭防火墙 systemctl stop firewalld systemctl disable firewalld 三、安装Cobbler 1.安装最新的epel库 #rpm -Uvh https://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm 2.安装Cobbler以及相关的包 #yum install cobbler cobbler-web pykickstart xinetd dhcp dnsmasq syslinux -y yum install cobbler cobbler-web pykickstart xinetd dhcp httpd tftp -y 3.启动Cobbler及其他服务 systemctl enable cobblerd systemctl start cobblerd systemctl enable httpd systemctl start httpd systemctl enable xinetd systemctl start xinetd Cobbler-Web提供了一个网站管理服务，默认用户名和密码都是\"cobbler\",Web浏览地址可以通过以下链接： https://172.18.50.30/cobbler_web //注意是https 四、配置Cobbler 1、设置文件里的default_password_crypted控制在kickstart过程中的新系统的超级用户口令 openssl passwd -1 生成系统的root用户密码 $1$Ve0a/FjX$GmL.ZpJYdvFPoYdpT1Zqb1 2、修改/etc/cobbler/settings配置 vim /etc/cobbler/settings default_password_crypted: \"$1$Ve0a/FjX$GmL.ZpJYdvFPoYdpT1Zqb1\" manage_dhcp: 1 manage_dns: 1 next_server: 172.168.50.30（主机名） pxe_just_once: 1 server: 172.168.50.30（主机名） 3、修改/etc/xinetd.d/tftp配置 vim /etc/xinetd.d/tftp disable = no 4、修改/etc/cobbler/dhcp.template配置 subnet 172.18.50.0 netmask 255.255.255.0 { option routers 172.18.50.254; #网关 option domain-name-servers 61.139.2.69; #DNS option subnet-mask 255.255.255.0; #子网掩码 range dynamic-bootp 172.18.50.35 172.18.50.39; #ip范围 default-lease-time 21600; max-lease-time 43200; next-server $next_server; class \"pxeclients\" { match if substring (option vendor-class-identifier, 0, 9) = \"PXEClient\"; if option pxe-system-type = 00:02 { filename \"ia64/elilo.efi\"; } else if option pxe-system-type = 00:06 { filename \"grub/grub-x86.efi\"; } else if option pxe-system-type = 00:07 { filename \"grub/grub-x86_64.efi\"; } else { filename \"pxelinux.0\"; } } } 5、修改/etc/cobbler/dnsmasq.template配置一个简单的DHCP server vim /etc/cobbler/dnsmasq.template ... read-ethers addn-hosts = /var/lib/cobbler/cobbler_hosts dhcp-range=172.18.50.35,172.18.50.39 dhcp-option=3,$next_server dhcp-lease-max=1000 dhcp-authoritative dhcp-boot=pxelinux.0 dhcp-boot=net:normalarch,pxelinux.0 dhcp-boot=net:ia64,$elilo $insert_cobbler_system_definitions 6、检查及同步Cobbler systemctl restart cobblerd 7、其他问题 cobbler check (这个命令帮助检测Cobbler配置目前仍然存在的问题，根据提示修改这些问题，然后重启cobbler即可) 注：这里cobbler check 遇到的有: 1 : SELinux is enabled. Please review the following wiki page for details on ensuring cobbler works correctly in your SELinux environment: https://github.com/cobbler/cobbler/wiki/Selinux 2 : named is not installed and/or in path 3 : some network boot-loaders are missing from /var/lib/cobbler/loaders, you may run 'cobbler get-loaders' to download them, or, if you only want to handle x86/x86_64 netbooting, you may ensure that you have installed a *recent* version of the syslinux package installed and can ignore this message entirely. Files in this directory, should you want to support all architectures, should include pxelinux.0, menu.c32, elilo.efi, and yaboot. The 'cobbler get-loaders' command is the easiest way to resolve these requirements. 4 : enable and start rsyncd.service with systemctl 5 : debmirror package is not installed, it will be required to manage debian deployments and repositories 6 : fencing tools were not found, and are required to use the (optional) power management features. install cman or fence-agents to use them 解决： 1、关闭selinux，前面已经改过，先忽略 2、yum install bind -y 3、cobbler get-loaders 4、systemctl enable rsyncd systemctl start rsyncd 5、rpm -Uvh ftp://rpmfind.net/linux/epel/6/x86_64/debmirror-2.14-2.el6.noarch.rpm --nodeps --force 修改debmirror配置/etc/debmirror.conf vim /etc/debmirror.conf #@dists=\"sid\"; #注释 #@arches=\"i386\"; #注释 6、yum install fence-agents -y systemctl restart cobblerd cobbler sync 五、导入Distribution mkdir /mnt/iso mount -o loop /dev/cdrom /mnt/iso #也可以挂载iso文件 cobbler import --arch=x86_64 --path=/mnt/iso --name=CentOS-7 cobbler distro list cobbler profile list cobbler distro report --name=CentOS-7-x86_64 #显示详细信息 六、删除导入的系统 cobbler profile find --distro=CentOS-7-x86_64 #查看配置是否存在 cobbler profile remove --name=CentOS-7-x86_64 #删除配置文件 cobbler distro remove --name=CentOS-7-x86_64 #删除distro cobbler profile find --distro=CentOS-7-x86_64 #再查看配置已无 "},"notes/linux/apache_multi_ports.html":{"url":"notes/linux/apache_multi_ports.html","title":"apache 多端口","keywords":"","body":"apache 多端口 1、新建配置文件 touch /etc/apache2/sites-available/packages.conf 2、编辑配置文件(/etc/apache2/sites-available/packages.conf) Listen 81 ServerAdmin webmaster@localhost DocumentRoot /mnt/ramdisk Options FollowSymLinks AllowOverride None ErrorLog /var/log/apache2/error.log # Possible values include: debug, info, notice, warn, error, crit, # alert, emerg. LogLevel warn CustomLog /var/log/apache2/access.log combined 3、链接 ln -s /etc/apache2/sites-available/packages.conf /etc/apache2/sites-enabled 4、在apache配置文件中（/etc/apache2/apache2.conf），添加如下内容： Options Indexes FollowSymLinks AllowOverride None Require all granted 4、重启apache服务 service apache2 restart "},"notes/linux/dns.html":{"url":"notes/linux/dns.html","title":"搭建本地 dns 服务器","keywords":"","body":"搭建本地 dns 服务器 一、简要配置 1.dns 节点(如192.168.0.127) 确保53端口不被占用 vim /etc/dnsmasq.conf no-hosts no-resolv no-poll neg-ttl=3600 min-cache-ttl=3600 dns-forward-max=10000 cache-size=100000 edns-packet-max=1232 log-facility=/var/log/dnsmasq.log all-servers server=114.114.114.114 server=8.8.8.8 address=/hl.cn/10.0.0.127 systemctl restart dnsmasq cache-size # 设置 DNS 缓存大小，增加缓存可以减少频繁的外部 DNS 查询，提升查询速度。 min-cache-ttl # 设置缓存条目的最小 TTL（生存时间），单位为秒。此项可减少频繁的 DNS 查询，提高缓存命中率。 dns-forward-max # 设置允许的最大 DNS 并发查询数量。默认值可能较低，增加该值可以提高 DNS 并发查询性能。 edns-packet-max # 限制 EDNS0 扩展 DNS 响应包的大小，避免某些网络环境中较大的 DNS 包导致丢包问题。优化 EDNS 数据包大小，减少因 MTU 问题导致的查询失败。 neg-ttl=3600 # 对未能解析的请求结果进行缓存，减少重复解析开销。 all-servers # 同时向所有配置的上游 DNS 服务器发出查询请求，哪个服务器先返回结果就使用哪个，减少查询延迟。 strict-order # 按顺序使用 resolv-file 中的上游 DNS 服务器 no-poll # 禁止定期轮询 resolv-file 变化 no-resolv # 禁止自动读取 /etc/resolv.conf（确保只使用 resolv-file） no-hosts # 禁用系统的 hosts 文件，避免不必要的加载 log-async=10 # 异步写日志，10 行合并写入一次，减少 I/O 压力 log-facility=/var/log/dnsmasq.log # 日志输出路径，可按需调整 # 配置测试 dnsmasq --test 2.使用节点 永久修改，需修改网卡 dns vim /etc/resolv.conf nameserver 192.168.0.127 # 每次查询的超时时间为 1 秒 # 每个服务器的重试次数为 3 次 # 启用 DNS 轮询，按顺序使用多个 nameserver options timeout:1 attempts:3 rotate 二、测试 yum install -y dnsperf cat domains.txt hl.cn A xxx.yyy A dnsperf -s 192.168.0.127 -p 53 -d domains.txt -c 256 -l 10 三、其他 1、安装 yum方式安装，如下： yum -y install dnsmasq dnsmasq -v apt-get方式安装，如下： apt-get -y install dnsmasq dnsmasq -v 2、编辑配置文件 vim /etc/dnsmasq.conf resolv-file=/etc/resolv.dnsmasq.conf strict-order echo 'resolv-file=/etc/dnsmasq.d/resolv.dnsmasq.conf' >> /etc/dnsmasq.conf echo 'addn-hosts=/etc/dnsmasq.d/dnsmasq.hosts' >> /etc/dnsmasq.conf 3、添加开机启动 /etc/init.d/dnsmasq restart 4、检查是否启动 netstat -tunlp|grep 53 5、使用DNS加快解析速度。打开/etc/dnsmasq.conf文件，server=后面可以添加指定的DNS，例如不同的网站使用不同的DNS。 国内指定DNSserver=/cn/114.114.114.114 server=/taobao.com/114.114.114.114 server=/taobaocdn.com/114.114.114.114 指定DNSserver=/google.com/223.5.5.5 屏蔽网站/广告vim /etc/dnsmasq.conf address=/ad.youku.com/127.0.0.1 address=/ad.iqiyi.com/127.0.0.1 指定域名解析到特定的IP上。这个功能可以让你控制一些网站的访问，非法的DNS就经常把一些正规的网站解析到不正确IP上。 address=/olinux.org.cn/123.123.123.123 内网DNS(DNS劫持)。首先将局域网中的所有的设备的本地DNS设置为已经安装Dnsmasq的服务器IP地址。然后修改已经安装Dnsmasq的服务器Hosts文件：/etc/hosts，指定域名到特定的IP中。 6、其他电脑配置dns，检查测试缓存 dig www.baidu.com 7、Dnsmasq小结 1、Dnsmasq作为本地DNS服务器安装方便，操作简单，改动的地方也不是很多，如果用VPS来搭建本地DNS，响应的速度会更快，也更稳定。 2、Dnsmasq的功能强大，反DNS劫持、加快解析速度、屏蔽广告、控制内网DNS、强制域名跳转到特定IP上等这些功能在我们的实际的生活中都是很有用的。 "},"notes/linux/linux_u.html":{"url":"notes/linux/linux_u.html","title":"Linux U盘占用","keywords":"","body":"Linux U盘占用 强制解除U盘占用 multipath -ll /etc/multipath.conf blacklist { devnode \"^sd[a-z]\" } systemctl restart multipathd.service "},"notes/linux/ssh.html":{"url":"notes/linux/ssh.html","title":"ssh 相关","keywords":"","body":"ssh 相关 一、免密登录 ssh-keygen ssh-copy-id root@IP PreferredAuthentications publickey,password #PreferredAuthentications password StrictHostKeyChecking no ServerAliveInterval 15 ServerAliveCountMax 3 Host bosh hostname bosh user ubuntu Host hl hostname hl user centos Host ep hostname 192.168.0.57 user root port 50010 二、使用pptpd apt-get install pptpd pptpsetup -create pptpd -server 222.209.209.209 -username node2 -password 123456 -encrypt -start 如果断开了在机器上执行pon pptpd 三、sshpass yum install -y sshpass # apt install -y sshpass sshpass -p 123456 ssh -p 1000 root@192.168.11.11 sshpass -p 123456 scp -o StrictHostKeyChecking=no -r ../run.sh root@192.168.0.24:/tmp/ 四、常见问题 1、Write failed: Broken pipe 问题现象 用 ssh 命令连接服务器之后，如果一段时间不操作，再次进入 Terminal 时会有一段时间没有响应，然后就出现错误提示： Write failed: Broken pipe 只能重新用 ssh 命令进行连接。 解决方法 方法一：如果您有多台服务器，不想在每台服务器上设置，只需在客户端的 ~/.ssh/ 文件夹中添加 config 文件，并添加下面的配置： ServerAliveInterval 60 方法二：如果您有多个人管理服务器，不想在每个客户端进行设置，只需在服务器的 /etc/ssh/sshd_config 中添加如下的配置： ClientAliveInterval 60 方法三：如果您只想让当前的 ssh 保持连接，可以使用以下的命令： $ ssh -o ServerAliveInterval=60 user@sshserver ssh连接超时问题解决方案： 1.修改server端的etc/ssh/sshd_config ClientAliveInterval 60 ＃server每隔60秒发送一次请求给client，然后client响应，从而保持连接 ClientAliveCountMax 3 ＃server发出请求后，客户端没有响应得次数达到3，就自动断开连接，正常情况下，client不会不响应 2.修改client端的etc/ssh/ssh_config添加以下：（在没有权限改server配置的情形下） ServerAliveInterval 60 ＃client每隔60秒发送一次请求给server，然后server响应，从而保持连接 ServerAliveCountMax 3 ＃client发出请求后，服务器端没有响应得次数达到3，就自动断开连接，正常情况下，server不会不响应 五、ssh 权限限制 cat /etc/security/access.conf #允许所有用户配置atd和cron服务 + : ALL : atd cron #允许本地root用户通过ssh访问自己 + : root : 127.0.0.1 localhost imgr7 hostname + : root : test01 test02 + : root : 192.168.0.1 192.168.0.2 # All other users should be denied to get access from all sources. #-:ALL:ALL - : root : ALL systemctl restart sshd cat /etc/pam.d/sshd #%PAM-1.0 auth required pam_sepermit.so auth include password-auth account sufficient pam_listfile.so item=user sense=allow onerr=fail file=/etc/ssh/allowed_users account required pam_access.so account required pam_nologin.so account include password-auth password include password-auth auth substack password-auth auth include postlogin # Used with polkit to reauthorize users in remote sessions -auth optional pam_reauthorize.so prepare account required pam_nologin.so account include password-auth password include password-auth # pam_selinux.so close should be the first session rule session required pam_selinux.so close session required pam_loginuid.so # pam_selinux.so open should only be followed by sessions to be executed in the user context session required pam_selinux.so open env_params session required pam_namespace.so session optional pam_keyinit.so force revoke session include password-auth session include postlogin session required pam_mkhomedir.so # Used with polkit to reauthorize users in remote sessions -session optional pam_reauthorize.so prepare #account required pam_access.so systemctl restart sshd "},"notes/linux/mdadm.html":{"url":"notes/linux/mdadm.html","title":"软 raid","keywords":"","body":"软 raid 由于EFI并不能安装在RAID中，以上的操作只能确保系统从第一块硬盘启动，而不能从第二块硬盘启动。如果第一块硬盘出现问题，则系统将不能启动。以下过程，将使能从第二块硬盘启动。当第一块硬盘异常的时候，系统可以从第二块硬盘正常启动。 将/sda1的内容，克隆到/sdb1中，如下所示： dd if=/dev/sda1 of=/dev/sdb1 最后，将/sdb1加入到启动目录中，如下： efibootmgr -c -g -d /dev/sdb -p 1 -L \"Ubuntu #2\" -l '\\EFI\\Ubuntu\\grubx64.efi' 至此，安装的系统将可以分别从/sda和/sdb硬盘上启动。 mkfs.ext4 /dev/sda mkfs.ext4 /dev/sdb mdadm -C /dev/md0 -a yes -n 2 -l 1 /dev/sda /dev/sdc mdadm -Ds >/etc/mdadm.conf mdadm -Ds >/etc/mdadm/mdadm.conf update-initramfs -u mkfs.ext4 -F /dev/md0 mdadm --detail --scan mdadm -Ds mdadm -Ds >> /etc/mdadm/mdadm.conf 软RAID管理命令mdadm详解 一、创建模式 选项：-C 专用选项： -l 级别 -n 设备个数 -a {yes|no} 自动为其创建设备文件 -c 指定数据块大小（chunk） -x 指定空闲盘（热备磁盘）个数，空闲盘（热备磁盘）能在工作盘损坏后自动顶替 注意：创建阵列时，阵列所需磁盘数为-n参数和-x参数的个数和 1、创建raid0： 1.1 创建raid mdadm -C /dev/md0 -a yes -l 0 -n 2 /dev/sdb{1,2} 注意：用于创建raid的磁盘分区类型需为fd 1.2 格式化： mkfs.ext4 /dev/md0 注意：在格式化时，可以指定-E选项下的stride参数指定条带是块大小的多少倍，有在一定程度上提高软RAID性能，如块默认大小为4k，而条带大小默认为64k，则stride为16，这样就避免了RAID每次存取数据时都去计算条带大小，如： mkfs.ext4 -E stride=16 -b 4096 /dev/md0 其中stride=chunk/block，为2的n次方 2、创建raid1： 2.1 创建raid mdadm -C /dev/md1 -a yes -n 2 -l 1 /dev/sdb{5,6} 注意：这个提示是说软raid不能用作启动分区。 2.2 格式化： mkfs.ext4 /dev/md1 3、创建raid5： 由于没有磁盘空间，我将原来做raid1的测试磁盘全部删除后重新建立四个分区用于raid5测试，分别为sdb5-8 3.1 创建raid5 mdadm -C /dev/md2 -a yes -l 5 -n 3 /dev/sdb{5,6,7} 3.2 格式化： mkfs.ext4 /dev/md2 3.3 增加热备磁盘： mdadm /dev/md2 -a /dev/sdb8 4、查看md状态： 4.1 查看RAID阵列的详细信息： 选项： -D = --detail mdadm -D /dev/md# 查看指定RAID设备的详细信息 4.2 查看raid状态 cat /proc/mdstat 注意：在创建raid前，应该先查看磁盘是否被识别，如果内核还为识别，创建Raid时会报错： cat /proc/partitions 如果没有被识别，可以执行命令： kpartx /dev/sdb或者partprobe/dev/sdb 二、管理模式 选项：-a(--add)，-d(--del),-r(--remove),-f(--fail) 1、模拟损坏： mdadm /dev/md1 -f /dev/sdb5 2、移除损坏的磁盘： mdadm /dev/md1 -r /dev/sdb5 3、添加新的硬盘到已有阵列： mdadm /dev/md1 -a /dev/sdb7 注意： 3.1、新增加的硬盘需要与原硬盘大小一致 3.2、如果原有阵列缺少工作磁盘（如raid1只有一块在工作，raid5只有2块在工作），这时新增加的磁盘直接变为工作磁盘，如果原有阵列工作正常，则新增加的磁盘为热备磁盘。 4、停止阵列： 选项：-S = --stop mdadm -S /dev/md1 三、监控模式 选项：-F 不常用，不做详细说明。 四、增长模式，用于增加磁盘，为阵列扩容： 选项：-G 示例，将上述raid5的热备磁盘增加到阵列工作磁盘中 [root@localhost ~]# mdadm -G /dev/md2 -n 4 注意：-n 4 表示使用四块工作磁盘 再次使用-D选项查看阵列详细信息如下： mdadm -D /dev/md2 五、装配模式，软RAID是基于系统的，当原系统损坏了，需要重新装配RAID 选项：-A 示例：将上述已经停止的阵列重新装配： mdadm -A /dev/md1 /dev/sdb5 /dev/sdb6 实现自动装配： mdadm运行时会自动检查/etc/mdadm.conf 文件并尝试自动装配，因此第一次配置raid后可以将信息导入到/etc/mdadm.conf 中，命令如下： [root@localhost ~]# mdadm -Ds >/etc/mdadm.conf RAID --- 磁盘阵列,简言之,用来提高硬盘的利用率和速度 RAID种类(理论): RAID 0 : 读写性能(最少两块硬盘) --- 硬盘使用量是所有硬盘大小之和,性能是所有硬盘之和 RAID 1 : 读写性能,冗余性(最少两块硬盘) --- 空间利用率:所有磁盘中最小的那块(n/2); 读性能接近RAID0,写性能较raid 0 弱一些;有 冗余能力 RAID 5 : 读写性能,冗余性(至少3块硬盘) --- 空间利用率:1-1/n .读性能接近RAID0 ,写性能较RAID0弱一些 . 冗余能力:可接受一块硬盘的损坏; RAID 6 : 读写性能,冗余性(至少4块硬盘) --- 空间利用率:1 - 2/n .读写性能较RAID5,读性能比RAID5还要弱一些; 冗余能力:可接受2块硬盘损坏; mdadm 常用参数解释 选项(高亮的是很常用的)： -f : fail , 将一个磁盘设置为故障状态 -l : LEVEL , 设置磁盘阵列的级别 -r : 移除故障设备 -a : 添加新设备进入磁盘阵列 -S : 停止一个磁盘阵列 -v : --verbose：显示细节 -D, --detail： 打印一个或多个md device 的详细信息 -x :--spare-devices 指定一个备份磁盘,也就是指定初始阵列的冗余device 数目即spare device数目； - n : 指定磁盘的个数 -A : --assemble：加入一个以前定义的阵列 -B : --build：创建一个没有超级块的阵列(Build a legacy array without superblocks.) -C : --create：创建一个新的阵列 -F : --follow, --monitor：选择监控(Monitor)模式 -G : --grow：改变激活阵列的大小或形态 -I : --incremental：添加一个单独的设备到合适的阵列，并可能启动阵列 --auto-detect：请求内核启动任何自动检测到的阵列 -h : --help：帮助信息，用在以上选项后，则显示该选项信息 --help-options：显示更详细的帮助 -V : --version：打印mdadm的版本信息 -b : --brief：较少的细节。用于 --detail 和 --examine 选项 -Q : --query：查看一个device，判断它为一个 md device 或是 一个 md 阵列的一部分 -E : --examine：打印 device 上的 md superblock 的内容 -c : --config= ：指定配置文件，缺省为 /etc/mdadm.conf -s : --scan：扫描配置文件或 /proc/mdstat以搜寻丢失的信息。配置文件/etc/mdadm.conf 使用mdadm 创建RAID (级别只是修改个数字,其他参数基本一样..) "},"notes/linux/root_extend.html":{"url":"notes/linux/root_extend.html","title":"将 home 目录空间扩容到根目录","keywords":"","body":"将 home 目录空间扩容到根目录 1、查看 df -h 2、取消挂载 [root@localhost ~]# umount /home/ 3、删除逻辑卷 [root@localhost ~]# lvremove /dev/mapper/centos-home Do you really want to remove active logical volume centos/home? [y/n]: y Logical volume \"home\" successfully removed 4、扩容 root 的逻辑卷 # lvextend -l 100%FREE /dev/mapper/centos-root [root@localhost ~]# lvextend -L +120G /dev/mapper/centos-root Size of logical volume centos/root changed from 50.00 GiB (12800 extents) to 170.00 GiB (43520 extents). Logical volume centos/root successfully resized. 5、对挂载目录在线扩容 [root@localhost ~]# xfs_growfs /dev/mapper/centos-root meta-data=/dev/mapper/centos-root isize=512 agcount=4, agsize=3276800 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=13107200, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=6400, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 13107200 to 44564480 6、重新创建 home 的逻辑卷 [root@localhost ~]# lvcreate -L 47G -n home centos Logical volume \"home\" created. [root@localhost ~]# mkfs.xfs /dev/mapper/centos-home meta-data=/dev/mapper/centos-home isize=512 agcount=4, agsize=3080192 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=12320768, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=6016, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@localhost ~]# mount /dev/mapper/centos-home /home [root@localhost ~]# df -h 文件系统 容量 已用 可用 已用% 挂载点 /dev/mapper/centos-root 170G 899M 170G 1% / devtmpfs 63G 0 63G 0% /dev tmpfs 63G 0 63G 0% /dev/shm tmpfs 63G 9.5M 63G 1% /run tmpfs 63G 0 63G 0% /sys/fs/cgroup /dev/sda2 1014M 135M 880M 14% /boot /dev/sda1 200M 9.8M 191M 5% /boot/efi tmpfs 13G 0 13G 0% /run/user/0 /dev/mapper/centos-home 47G 33M 47G 1% /home 7、查看 [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 223G 0 disk ├─sda1 8:1 0 200M 0 part /boot/efi ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 221.8G 0 part ├─centos-root 253:0 0 170G 0 lvm / ├─centos-swap 253:1 0 4G 0 lvm [SWAP] └─centos-home 253:2 0 47G 0 lvm /home sdb 8:16 0 8.9T 0 disk sdc 8:32 0 8.9T 0 disk 8、检查 /etc/fstab 挂载信息 cat /etc/fstab # # /etc/fstab # Created by anaconda on Thu Dec 12 10:05:59 2019 # # Accessible filesystems, by reference, are maintained under '/dev/disk' # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # /dev/mapper/centos-root / xfs defaults 0 0 UUID=90a54835-7cf0-4519-8b13-ff7b4a6076a7 /boot xfs defaults 0 0 /dev/mapper/centos-home /home xfs defaults 0 0 #/dev/mapper/centos-swap swap swap defaults 0 0 "},"notes/linux/vm_to_router.html":{"url":"notes/linux/vm_to_router.html","title":"将虚拟机作为router","keywords":"","body":"将虚拟机作为router 1、开启 ipv4 转发 echo 1 > /proc/sys/net/ipv4/ip_forward 2、配置 iptables iptables -A FORWARD -o tun0 -i eth0 -s 192.168.0.0/24 -m conntrack --ctstate NEW -j ACCEPT iptables -A FORWARD -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT iptables -A POSTROUTING -t nat -j MASQUERADE 3、将 iptables 保存 iptables-save |tee /etc/iptables.sav 4、配置该虚拟机的网关就可以使用 "},"notes/ceph/ceph.html":{"url":"notes/ceph/ceph.html","title":"Ceph","keywords":"","body":"Ceph 相关 一、常用命令 1、查询集群状态 # 健康状态 ceph -s ceph health ceph health detail ceph pg stat ceph pg dump_stuck unclean|grep unknown ceph crash ls # 查看集群得分 ceph balancer eval # pool ceph osd lspools ceph osd pool ls ceph osd pool ls detail # osd ceph osd tree # 资源使用状态 ceph df ceph df detail ceph osd df ceph osd df rbd ceph osd df tree # 其他 ceph quorum_status --format json-pretty ceph osd pool get-quota volumes ceph osd pool get rbd crush_rule 2、删除data，metadata池 ceph osd pool delete metadata metadata --yes-i-really-really-mean-it ceph osd pool delete data data --yes-i-really-really-mean-it 3、创建volumes存储池 ceph osd pool create volumes 4000 4000 4、查询 volumes 池当前复制副本数量 ceph osd dump | grep 'replicated size' | grep volumes 5、修改复制副本为 2 并验证 ceph osd pool set volumes size 2 ceph osd dump | grep 'replicated size' | grep volumes 6、查看存储使用情况 ceph df 7、查看集群得分 分数越低越好 ceph balancer eval current cluster score 0.032004 (lower is better) 8、修复 ceph pg deep-scrub 4.b6 ceph pg repair 4.b6 9、暂停与恢复osd对外访问 ceph osd pause ceph osd unpause 10、数据均衡调整 按利用率调整OSD的权重 ceph osd reweight-by-utilization 按归置组分布情况调整OSD的权重 ceph osd reweight-by-pg 11、重新设置OSD权重 crush 显示osd磁盘本身大小的标签 ceph osd crush reweight osd.8 1 ceph osd reweight osd.8 0.2 12、删除pool ceph tell mon.\\* injectargs '--mon-allow-pool-delete=true' ceph osd pool rm tmp tmp --yes-i-really-really-mean-it ceph tell mon.\\* injectargs '--mon-allow-pool-delete=false' 13、rbdmap apt -y install ceph-common cat > /etc/ceph/rbdmap 14、rbd快照 rbd snap create --snap snapshot1 rbd/hl rbd snap list rbd/hl rbd info rbd/hl rbd snap rollback rbd/hl@snapshot1 rbd snap rm rbd/hl@snapshot1 15、rbd调整大小 rbd showmapped rbd info foo rbd resize --size 20480 foo blockdev --getsize64 /dev/rbd0 resize2fs /dev/rbd0 16、PG PGP PG = Placement Group PGP = Placement Group for Placement purpose pg_num = number of placement groups mapped to an OSD When pg_num is increased for any pool, every PG of this pool splits into half, but they all remain mapped to their parent OSD. Until this time, Ceph does not start rebalancing. Now, when you increase the pgp_num value for the same pool, PGs start to migrate from the parent to some other OSD, and cluster rebalancing starts. This is how PGP plays an important role. By Karan Singh PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数 PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG当中 PGP的增加会引起部分PG的分布进行变化，但是不会引起PG内对象的变动 pg_num的增加会使原来PG中的对象均匀地分布到新建的PG中，原来的副本分布方式不变 pgp_num的增加会使PG的分布方式发生变化，但是PG内的对象并不会变动 pgp决定pg分布时的组合方式的变化 二、部署 1、cephadm 1.安装依赖 apt install -y cephadm #curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm #chmod +x cephadm cephadm add-repo --release pacific cephadm install ceph-common 2.初始化 cephadm bootstrap --mon-ip 192.168.0.31 #ceph config assimilate-conf -i -o cat initial-ceph.conf [global] osd_pool_default_size = 2 osd_pool_default_min_size = 1 mon_osd_full_ratio = .90 mon_osd_nearfull_ratio = .80 public_network = 192.168.0.0/24 cluster_network = 10.0.0.0/24 EOF cephadm bootstrap --config initial-ceph.conf --mon-ip 192.168.0.31 cephadm shell -- ceph -s #cephadm shell --fsid XXX -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring 3.host 配置 ssh-copy-id -f -i /etc/ceph/ceph.pub node2 ssh-copy-id -f -i /etc/ceph/ceph.pub node3 ceph orch host add node2 mon #ceph orch host rm node2 ceph orch host add node3 mon ceph orch host label add node1 mon #ceph orch host label rm node1 mon ceph orch apply mon \"node1,node2,node3\" ceph orch host ls 禁用自动部署 mon ceph orch apply mon --unmanaged 不同网络添加 mon ceph orch apply mon --unmanaged ceph orch daemon add mon newhost1:10.1.2.123 ceph orch daemon add mon newhost2:10.1.2.0/24 4.osd 配置 ceph orch device ls ceph orch apply osd --all-available-devices #ceph orch daemon add osd node1:/dev/sdb 删除 osd ceph osd crush remove {name} ceph auth del osd.{osd-num} ceph osd stop {osd-num} ceph osd rm {osd-num} 5.使用 cephfs ceph fs volume create fs _netdev：表示当系统联网后再进行挂载操作 noatime：这个参数来禁止记录最近一次访问时间戳，提高性能 使用 ceph-fuse cephadm install ceph-fuse #apt install -y ceph-fuse ceph-fuse -m 192.168.0.31:6789,192.168.0.32:6789,192.168.0.33:6789 /fs scp -r 192.168.0.61:/etc/ceph /etc vim /etc/fstab 192.168.0.61:6789,192.168.0.62:6789,192.168.0.64:6789:/ /fs fuse.ceph ceph.id=admin,ceph.conf=/etc/ceph/ceph.conf,noatime,_netdev,defaults 0 0 ceph fs authorize *file_system_name* client.*client_name* /*specified_directory* rw ceph fs authorize fs client.fs / rw [client.fs] key = AQA8DK5g7FkeFhAA4B6gk6M5nc+17XVKzjv27Q== #ceph auth rm client.fs mount -t ceph :/ /fs -o name=fs,secret=AQA8DK5g7FkeFhAA4B6gk6M5nc+17XVKzjv27Q== ceph fs authorize fs client.fs /yani rw mount -t ceph :/yani /fs -o name=fs,secret=AQA8DK5g7FkeFhAA4B6gk6M5nc+17XVKzjv27Q== 使用 linux kernel ssh 192.168.0.61 \"ceph-authtool -p /etc/ceph/ceph.client.admin.keyring\" > admin.key mount -t ceph 192.168.0.61:6789,192.168.0.62:6789,192.168.0.64:6789:/ /fs -o name=admin,secret=AQDXXXXX== #mount -t ceph :/ /fs -o name=admin,secret=AQDXXXXX== #mount -t ceph 192.168.0.61:6789,192.168.0.62:6789,192.168.0.64:6789:/ /fs -o name=admin,secretfile=admin.key vim /etc/fstab 192.168.0.61:6789,192.168.0.62:6789,192.168.0.64:6789:/ /fs ceph name=admin,secret=AQDXXXXX==,noatime,_netdev,defaults 0 0 #:/ /fs ceph name=admin,secret=AQDXXXXX==,noatime,_netdev,defaults 0 0 6.使用 rbd 1、创建 rbd ceph osd pool create rbd rbd pool init rbd #rbd create --size {megabytes} {pool-name}/{image-name} 要创建一个1GB的映像test，该映像将信息存储在rbd Pool中 rbd create --size 1024 rbd/test #开机自动映射 ls /etc/ceph/ ceph.client.guest.keyring ceph.conf rbdmap vim /etc/ceph/rbdmap #poolname/imagename id=client,keyring=/etc/ceph/ceph.client.guest.keyring rbd/test id=guest,keyring=/etc/ceph/ceph.client.guest.keyring vim /etc/fstab /dev/rbd0 /test xfs noatime,_netdev,defaults 0 0 # 查看已映射 rbd showmapped rbdmap unmap rbdmap unmap-all 创建块存储 pool 用户 ceph auth get-or-create client.libvirt mon 'profile rbd' osd 'profile rbd pool=rbd' [client.libvirt] key = AQCUD65goRI3ARAAtMHtLBivJ39eikR4ZRjEgg== rbd map rbd/test /dev/rbd1 mkfs.xfs -f /dev/rbd1 mount /dev/rbd1 /mnt 2、创建快照 rbd snap create --snap mysnap rbd/test #rbd snap create rbd/test@mysnap rbd snap list rbd/test rbd ls -l rbd info rbd/test@mysnap 回滚 rbd snap rollback rbd/test@mysnap 删除快照 rbd snap remove rbd/test@mysnap 3、模板与克隆 查看设备是否支持创建快照模板，image-format 必须为2 rbd info rbd/test rbd image 'test': size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: 170e5acc1f4a7 block_name_prefix: rbd_data.170e5acc1f4a7 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Wed May 26 11:13:38 2021 access_timestamp: Wed May 26 11:13:38 2021 modify_timestamp: Wed May 26 11:13:38 2021 rbd create rbd/test --size 1024 --image-format 2 把该块做成模板，首先要把做成模板的快照做成protect rbd snap protect rbd/test@mysnap 可以去掉这个保护，但是这样的话就不能克隆 rbd snap unprotect rbd/test@mysnap 克隆 rbd clone rbd/test@mysnap rbd/test1 查看快照children rbd children rbd/test@mysnap 这个时候的test1还是依赖test的镜像mysnap的，如果test的mysnap被删除或者怎么样，test1也不能够使用了，要想独立出去，就必须将父镜像的信息合并flattern到子镜像中 去掉快照的parent rbd flatten rbd/test1 rbd create 创建块设备映像 rbd ls 列出 rbd 存储池中的块设备 rbd info 查看块设备信息 rbd diff 可以统计 rbd 使用量 rbd map 映射块设备 rbd showmapped 查看已映射块设备 rbd remove 删除块设备 rbd resize 更改块设备的大小 rbd export rbd/test /tmp/test 导出rbd镜像 rbd import /tmp/test rbd/test --image-format 2 导入rbd镜像 7.性能测试 rados bench -p $poolname 60 write [ --no-cleanup ] #测试写性能，不删除写入的数据用来测试读取性能 rados bench -p $poolname 60 [ seq | rand ] #测试读取性能 rados -p $poolname cleanup #清理测试写性能时写入的数据 8.其他 创建 pool ceph osd pool create test_metadata 32 32 重新设置权重 ceph osd crush reweight osd.3 3.3 ceph osd reweight osd.4 0.3 ceph osd crush tree --show-shadow 获取 osdmap ceph osd dump ceph osd getmap -o osds.map osdmaptool --print osds.map 获取 monmap ceph mon dump ceph mon getmap -o monmap.map monmaptool --print monmap.map 获取 crushmap ceph osd getcrushmap -o crushmap.dump crushtool -d crushmap.dump -o crushmap.txt crushtool -c crushmap.txt -o crushmap.done ceph osd setcrushmap -i crushmap.done 新创建的 osd 不会自动加入 crushmap osd crush update on start = false ceph osd lspools rbd ls -l rbd remove rbd-name rbd map disk01 rbd showmapped ceph fs ls ceph mds stat 2、ceph deploy 1、添加release.key apt-get update && apt-get -y upgrade wget -q -O- 'https://download.ceph.com/keys/autobuild.asc' | apt-key add - 2、查看当前系统的发行版本信息 lsb_release -sc 3、添加Ceph软件包源，用Ceph稳定版（如cuttlefish、dumpling、emperor、firefly、hammer、jewel等）替换掉jewel echo deb http://mirrors.aliyun.com/ceph/debian-jewel/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E84AC2C0460F3994 4、更新源仓库并安装Ceph apt-get update && apt-get -y install ceph-deploy 5、将ceph deploy源换为国内源 vim /usr/lib/python2.7/dist-packages/ceph_deploy/hosts/debian/install.py sed -i 's/{protocol}:\\/\\/download.ceph.com/http:\\/\\/mirrors.aliyun.com\\/ceph/g' /usr/lib/python2.7/dist-packages/ceph_deploy/hosts/debian/install.py sed -i 's/{protocol}:\\/\\/download.ceph.com/http:\\/\\/mirrors.aliyun.com\\/ceph/g' /usr/lib/python2.7/dist-packages/ceph_deploy/hosts/debian/install.py sed -i 's/download.ceph.com/mirrors.tuna.tsinghua.edu.cn\\/ceph/g' /usr/lib/python2.7/dist-packages/ceph_deploy/util/constants.py 6、安装NTP（以免因时钟漂移导致故障） apt-get -y install ntp 7、安装 SSH 服务器 apt-get -y install openssh-server 8、配置hosts echo \" 172.18.20.181 ceph-node1 172.18.20.182 ceph-node2 172.18.20.183 ceph-node3\" | tee >> /etc/hosts 9、创建部署 CEPH 的用户（可不用） 1、在各 Ceph 节点创建新用户。 useradd -d /home/myCeph -m myCeph passwd myCeph 2、确保各 Ceph 节点上新创建的用户都有 sudo 权限。 echo \"myCeph ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/myCeph chmod 0440 /etc/sudoers.d/myCeph 10、允许无密码 SSH 登录 1、生成 SSH 密钥对，但不要用 sudo 或 root 用户。提示 “Enter passphrase” 时，直接回车，口令即为空： ssh-keygen 2、把公钥拷贝到各 Ceph 节点，把下列命令中的 myCeph 替换成前面创建部署 Ceph 的用户里的用户名。 ssh-copy-id myCeph@node1 3、（推荐做法）修改 ceph-deploy 管理节点上的 ~/.ssh/config 文件，这样 ceph-deploy 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 ceph-deploy 都要指定 --username {username} 。这样做同时也简化了 ssh 和 scp 的用法。把 {username} 替换成你创建的用户名。 Host node1 Hostname node1 User myCeph ssh-copy-id ceph-node1 11、安装Ceph ceph-deploy install controller compute{2,3,4,5,6,7,8} 12、创建集群 ceph-deploy new controller compute{2,3,4,5} 13、修改配置文件 ，把下面这行加入 [global] 段： echo \"osd crush chooseleaf type = 0\" >> ceph.conf echo \"osd pool default size = 1\" >> ceph.conf echo \"osd journal size = 100\" >> ceph.conf 14、配置初始monitor(s)、并收集所有秘钥： #ceph-deploy mon create-initial ceph-deploy mon create controller compute{2,3,4,5} ceph-deploy gatherkeys controller compute{2,3,4,5} 15、擦净磁盘 ceph-deploy disk zap controller:sd{b,c,d,e} compute2:sd{b,c,d} compute{3,4,5,6,7,8}:sd{b,c,d,e} 16、准备磁盘 ceph-deploy osd prepare controller:sd{b,c,d,e} compute2:sd{b,c,d} compute{3,4,5,6,7,8}:sd{b,c,d,e} ceph-deploy osd prepare ceph-node1:xvdb ceph-node1:xvdc ceph-node1:xvde ceph-node2:xvdb ceph-node2:xvdc ceph-node2:xvde ceph-node3:xvdb ceph-node3:xvdc ceph-node3:xvde 17、激活磁盘 ceph-deploy osd activate controller:sd{b1,c1,d1,e1} compute2:sd{b1,c1,d1} compute{3,4,5,6,7,8}:sd{b1,c1,d1,e1} 18、复制 复制配置文件到所有节点，用ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了。 ceph-deploy admin node1 ceph-deploy --overwrite-conf admin ceph-node1 ceph-node2 ceph-node3 19、确保你对 ceph.client.admin.keyring 有正确的操作权限。 chmod +r /etc/ceph/ceph.client.admin.keyring 20、检查集群的健康状况 ceph health ceph -s ceph osd tree 21、清除部署 ceph-deploy purge ceph-node1 ceph-node2 ceph-node3 ceph-deploy purgedata ceph-node1 ceph-node2 ceph-node3 ceph-deploy forgetkeys ceph osd pool delete rbd rbd --yes-i-really-really-mean-it 22、修改crushmap 1、获得 crush map，获得默认 crushmap (加密) ceph osd getcrushmap -o crushmap.dump 2、转换 crushmap 格式 (加密 -> 明文格式) crushtool -d crushmap.dump -o crushmap.txt 3、转换 crushmap 格式(明文 -> 加密格式) crushtool -c crushmap.txt -o crushmap.done 4、重新使用新 crushmap ceph osd setcrushmap -i crushmap.done 23、修改osd权重 ceph osd crush reweight osd.1 1 三、使用 1、将ceph配置文openstack的后端配置 1、创建pool，初始化pool。 ceph osd pool create volumes 128 ceph osd pool create images 128 ceph osd pool create backups 128 ceph osd pool create vms 128 rbd pool init volumes rbd pool init images rbd pool init backups rbd pool init vms 2、在glance-api节点安装以下包。 apt-get -y install python-rbd yum -y install python-rbd 3、在nova-compute, cinder-backup,cinder-volume安装以下包。 apt-get -y install ceph-common yum -y install ceph-common 4、如果开启了 cephx authentication, 需要为nova、cinder、glance创建新用户。 #ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images, allow rwx pool=images.cache' ceph auth get-or-create client.glance mon 'profile rbd' osd 'profile rbd pool=images' ceph auth get-or-create client.cinder mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images' ceph auth get-or-create client.cinder-backup mon 'profile rbd' osd 'profile rbd pool=backups' 5、为相应节点的client.cinder, client.glance, client.cinder-backup添加keyrings，改变它们的所有权。 ceph auth get-or-create client.glance | ssh {your-glance-api-server} tee /etc/ceph/ceph.client.glance.keyring ssh {your-glance-api-server} chown glance:glance /etc/ceph/ceph.client.glance.keyring ceph auth get-or-create client.cinder | ssh {your-volume-server} tee /etc/ceph/ceph.client.cinder.keyring ssh {your-cinder-volume-server} chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring ceph auth get-or-create client.cinder-backup | ssh {your-cinder-backup-server} tee /etc/ceph/ceph.client.cinder-backup.keyring ssh {your-cinder-backup-server} chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring 6、运行了nova-compute的节点需要keyring文件。 ceph auth get-or-create client.cinder | ssh {your-nova-compute-server} tee /etc/ceph/ceph.client.cinder.keyring 7、需要将client.cinder用户的秘钥配置到libvirt中，libvirt进程从cinder挂载块设备时，需要有权限访问ceph集群。获取并拷贝一个临时的secret key到所有运行的nova-compute节点。 ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key 8、然后，在compute节点，添加secret key到libvirt，再删除临时的secret key。 # 保存uuid，后面会用到 uuidgen 457eb676-33da-42ec-9a8c-9293d545c337 cat > secret.xml 457eb676-33da-42ec-9a8c-9293d545c337 client.cinder secret EOF virsh secret-define --file secret.xml Secret 457eb676-33da-42ec-9a8c-9293d545c337 created virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) && rm client.cinder.key secret.xml 9、修改glance-api.conf。 [glance_store] stores = rbd default_store = rbd rbd_store_pool = images rbd_store_user = glance rbd_store_ceph_conf = /etc/ceph/ceph.conf rbd_store_chunk_size = 8 ... # 启用copy-on-write [DEFAULT] show_image_direct_url = True ... [paste_deploy] flavor = keystone 10、修改cinder.conf。 [DEFAULT] ... enabled_backends = ceph glance_api_version = 2 ... # 配置cinder backup backup_driver = cinder.backup.drivers.ceph backup_ceph_conf = /etc/ceph/ceph.conf backup_ceph_user = cinder-backup backup_ceph_chunk_size = 134217728 backup_ceph_pool = backups backup_ceph_stripe_unit = 0 backup_ceph_stripe_count = 0 restore_discard_excess_bytes = true ... [ceph] volume_driver = cinder.volume.drivers.rbd.RBDDriver volume_backend_name = ceph rbd_pool = volumes rbd_ceph_conf = /etc/ceph/ceph.conf rbd_flatten_volume_from_snapshot = false rbd_max_clone_depth = 5 rbd_store_chunk_size = 4 rados_connect_timeout = -1 rbd_user = cinder rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337 11、配置nova.conf将nova挂载到ceph rbd块设备。 [libvirt] ... images_type = rbd images_rbd_pool = vms images_rbd_ceph_conf = /etc/ceph/ceph.conf rbd_user = cinder rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337 disk_cachemodes=\"network=writeback\" 12、重启服务 glance-control api restart service nova-compute restart service cinder-volume restart service cinder-backup restart service openstack-glance-api restart service openstack-nova-compute restart service openstack-cinder-volume restart service openstack-cinder-backup restart 13、使用块设备 cinder create --image-id {id of image} --display-name {name of volume} {size of volume} qemu-img convert -f {source-format} -O {output-format} {source-filename} {output-filename} qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw 2、使用glance task-create上传镜像并将格式转为raw。 1、编辑glance-api.conf。 [task] task_executor = taskflow work_dir=/tmp [taskflow_executor] engine_mode = serial max_workers = 10 conversion_format=raw 2、重启服务 glance-control api restart service openstack-glance-api restart 3、上传镜像 glance task-create --type import --input '{\"import_from_format\": \"qcow2\", \"import_from\": \"http://172.18.20.160/cirros-0.3.4-x86_64-disk.img\", \"image_properties\": {\"name\": \"cirros-RAW\", \"disk_format\": \"qcow2\", \"container_format\": \"bare\"}}' 4.移除osd。 ceph osd tree ceph osd out osd.0 ceph osd crush rm osd.0 ceph auth del osd.0 ceph osd rm osd.0 3、添加mon，移除mon ceph-deploy install ctrl ceph-deploy mon create ctrl ceph-deploy mon add ctrl ceph-deploy mon create-initial ceph quorum_status --format json-pretty ceph-deploy mon destroy comp-3 ceph quorum_status --format json-pretty ceph-deploy gatherkeys comp-1 4、启动ceph自带监控界面 1、查看 ceph 状态 ceph -s 2、启用 dashboard ceph mgr module enable dashboard 3、生成并安装一个自签名证书 ceph dashboard create-self-signed-cert 4、配置服务地址、端口，默认的端口是8443 ceph config set mgr mgr/dashboard/server_addr 192.168.0.127 ceph config set mgr mgr/dashboard/server_port 8443 ceph mgr services 5、创建一个用户、密码 ceph dashboard set-login-credentials admin admin 6、重启 mgr systemctl restart ceph-mgr@node1 7、访问 https://192.168.0.127:8443 5、kolla 中ceph相关 parted /dev/xvdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1 修改globalsl.yaml enable_ceph: \"yes\" ceph_enable_cache: \"yes\" ceph_cache_mode: \"writeback\" 2个240 固态(一个做journal，一个做cache) 【cache】 parted /dev/xvdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_CACHE_BOOTSTRAP 1 -1 【journal】 parted /dev/xvdd \\ -s mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDB_J 0% 5G \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 5G 10G \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 10G 15G \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 15G 20G \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 20G 25G \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 25G 30G \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 30G 35G \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 35G 40G \\ -s mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC_J 40G 45G 9个2T SATA parted /dev/vdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDB 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 parted /dev/vdc -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP_VDC 1 -1 8、pg状态表 状态 描述 Activating PG已经互联，但是还没有active。 Peering已经完成，PG正在等待所有PG实例同步并固化Peering的结果(Info、Log等) Active 活跃态。PG可以正常处理来自客户端的读写请求 PG正常的状态应该是Active+Clean的。 Backfilling Ceph正常扫描并同步整个PG的数据，而不是从最近的操作日志中推断需要同步的数据，Backfill（回填）是恢复的一个特殊状态。 正在后台填充态。 backfill是recovery的一种特殊场景，指peering完成后，如果基于当前权威日志无法对Up Set当中的某些PG实例实施增量同步(例如承载这些PG实例的OSD离线太久，或者是新的OSD加入集群导致的PG实例整体迁移) 则通过完全拷贝当前Primary所有对象的方式进行全量同步 Backfill-toofull backfill_toofull backfill操作因为目标OSD容量超过指标而挂起 某个需要被Backfill的PG实例，其所在的OSD可用空间不足，Backfill流程当前被挂起 backfill_unfound Backfill因为没有找到对应对象而停止 Backfill-wait backfill_wait PG正在等待backfill被调度执行。 等待Backfill 资源预留 Clean 干净态。PG内所有的对象都被正确的复制了对应的份数。 PG**当前不存在待修复的对象， Acting Set和Up Set**内容一致，并且大小等于存储池的副本数 Creating PG正在被创建 Deep Ceph 正在检查PG数据和checksums的一致性。 PG**正在或者即将进行对象一致性扫描清洗 Degraded PG中的一些对象还没有被复制到规定的份数。 降级状态。Peering完成后，PG检测到任意一个PG实例存在不一致(需要被同步/修复)的对象，或者当前**ActingSet 小于存储池副本数 Down 一个包含必备数据的副本离线，所以PG也离线了 Peering过程中，PG检测到某个不能被跳过的Interval中(例如该Interval期间，PG完成了Peering，并且成功切换至Active状态，从而有可能正常处理了来自客户端的读写请求),当前剩余在线的OSD**不足以完成数据修复 Incomplete Ceph 探测到某一PG可能丢失了写入信息，或者没有健康的副本。如果你看到了这个状态，尝试启动有可能包含所需信息的失败OSD， 如果是erasure coded pool的话，临时调整一下min_size也可能完成恢复 Peering**过程中， 由于 a. 无非选出权威日志 b. 通过choose_acting选出的Acting Set后续不足以完成数据修复，导致Peering无非正常完成 Inconsistent Ceph检测到PG中对象的一份或多份数据不一致（比如对象大学不一直，或者恢复成功后对象依然没有等） 不一致态。集群清理和深度清理后检测到PG中的对象在副本存在不一致，例如对象的文件大小不一致或Recovery结束后一个对象的副本丢失 Peered PG已互联，但是不能向客户端提供服务，因为其副本数没达到本存储池的配置值（ min_size 参数）。 在此状态下恢复会进行，所以此PG最终能达到 min_size 。 Peering已经完成，但是PG当前ActingSet规模小于存储池规定的最小副本数(min_size) Peering PG正在互联过程中。正在同步态。PG正在执行同步处理。 类似Raft的Leader选举，使一个PG内的OSD达成一致，不涉及数据迁移等操作。 Recovering 正在恢复态。集群正在执行迁移或同步对象和他们的副本 Recovering-wait 等待recovery资源预留。PG正在等待恢复被调度执行。 recovery_toofull 恢复操作因为目标OSD容量超过指标而挂起。 recovery_unfound 恢复因为没有找到对应对象而停止。 Remapped PG被临时分配到了和CRUSH所指定的不同的OSD上。 重新映射态。PG活动集任何的一个改变，数据发生从老活动集到新活动集的迁移。在迁移期间还是用老的活动集中的主OSD处理客户端请求，一旦迁移完成新活动集中的主OSD开始处理 Repair Ceph正在检查PG并且修复所有发现的不一致情况（如果有的话） PG在执行Scrub**过程中，如果发现存在不一致的对象，并且能够修复，则自动进行修复状态 Scrubbing Ceph 正在检查PG metadata的一致性。 PG正在或者即将进行对象一致性扫描 Unactive 非活跃态。PG不能处理读写请求 Unclean 非干净态。PG不能从上一个失败中恢复 Stale PG状态未知，从PG mapping更新后Monitor一直没有收到更新 未刷新态。PG状态没有被任何OSD更新，这说明所有存储这个PG的OSD可能挂掉**, 或者Mon没有检测到Primary统计信息(网络抖动**) snaptrim 正在对快照做Trim操作。 snaptrim_Wait Trim操作等待被调度执行 snaptrim_Error Trim操作因为错误而停止 Undersized 该PG的副本数量小于存储池所配置的副本数量。 PG当前Acting Set小于存储池副本数。ceph默认3副本，min_size参数通常为2，即副本数>=2时就可以进行IO，否则阻塞IO。 forced_recovery 用户指定的PG高优先级恢复 forced_backfill 用户指定的高优先级backfill 6、参数调优 ceph -s cluster: id: 79d60ec1-cece-464e-9d2c-37ed3581a3dd health: HEALTH_ERR 1 scrub errors Possible data damage: 1 pg inconsistent ceph health detail ceph healthrepair 14.1a #保留一段时间以来的访问记录，这样 Ceph 就能判断一客户端在一段时间内访问了某对象一次、还是多次（存活期与热度）。 #为缓存存储池保留的命中集数量 ceph osd pool set vms.cache hit_set_count 1 #为缓存存储池保留的命中集有效期 ceph osd pool set vms.cache hit_set_period 3600 #缓存存储池包含的脏对象达到多少比例时就把它们回写到后端的存储池 ceph osd pool set vms.cache cache_target_dirty_ratio 0.4 #缓存存储池内包含的已修改（脏的）对象达到此比例时，缓存层代理就会更快地把脏对象刷回到后端存储池 ceph osd pool set vms.cache cache_target_dirty_high_ratio 0.6 #缓存存储池包含的干净对象达到多少比例时，缓存代理就把它们赶出缓存存储池 ceph osd pool set vms.cache cache_target_full_ratio 0.8 #达到 max_bytes 阀值时 Ceph 就回写或赶出对象（200G） ceph osd pool set vms.cache target_max_bytes 200000000000 #达到 max_objects 阀值时 Ceph 就回写或赶出对象(每个对象1M) ceph osd pool set vms.cache target_max_objects 200000 #达到此时间（单位为秒）时，缓存代理就把某些对象从缓存存储池刷回到存储池 ceph osd pool set vms.cache cache_min_flush_age 600 #达到此时间（单位为秒）时，缓存代理就把某些对象从缓存存储池赶出 ceph osd pool set vms.cache cache_min_evict_age 1800 hit_set_count 描述: 为缓存存储池保留的命中集数量。此值越高， ceph-osd 守护进程消耗的内存越多。 类型: 整数 有效范围: 1. Agent doesn’t handle > 1 yet. hit_set_period 描述: 为缓存存储池保留的命中集有效期。此值越高， ceph-osd 消耗的内存越多。 类型: 整数 实例: 3600 1hr ceph osd pool set vms-cache hit_set_count 1 ceph osd pool set vms-cache hit_set_period 3600 ceph osd pool set vms-cache cache_target_dirty_ratio 0.4 ceph osd pool set vms-cache cache_target_dirty_high_ratio 0.6 ceph osd pool set vms-cache cache_target_full_ratio 0.8 ceph osd pool set vms-cache target_max_bytes 200000000000 ceph osd pool set vms-cache target_max_objects 200000 ceph osd pool set vms-cache cache_min_flush_age 600 ceph osd pool set vms-cache cache_min_evict_age 1800 ceph osd pool set images-cache hit_set_count 1 ceph osd pool set images-cache hit_set_period 3600 ceph osd pool set images-cache cache_target_dirty_ratio 0.4 ceph osd pool set images-cache cache_target_dirty_high_ratio 0.6 ceph osd pool set images-cache cache_target_full_ratio 0.8 ceph osd pool set images-cache target_max_bytes 200000000000 ceph osd pool set images-cache target_max_objects 200000 ceph osd pool set images-cache cache_min_flush_age 600 ceph osd pool set images-cache cache_min_evict_age 1800 ceph osd pool set volumes-cache hit_set_count 1 ceph osd pool set volumes-cache hit_set_period 3600 ceph osd pool set volumes-cache cache_target_dirty_ratio 0.4 ceph osd pool set volumes-cache cache_target_dirty_high_ratio 0.6 ceph osd pool set volumes-cache cache_target_full_ratio 0.8 ceph osd pool set volumes-cache target_max_bytes 200000000000 ceph osd pool set volumes-cache target_max_objects 200000 ceph osd pool set volumes-cache cache_min_flush_age 600 ceph osd pool set volumes-cache cache_min_evict_age 1800 ceph osd pool set backups-cache hit_set_count 1 ceph osd pool set backups-cache hit_set_period 3600 ceph osd pool set backups-cache cache_target_dirty_ratio 0.4 ceph osd pool set backups-cache cache_target_dirty_high_ratio 0.6 ceph osd pool set backups-cache cache_target_full_ratio 0.8 ceph osd pool set backups-cache target_max_bytes 200000000000 ceph osd pool set backups-cache target_max_objects 200000 ceph osd pool set backups-cache cache_min_flush_age 600 ceph osd pool set backups-cache cache_min_evict_age 1800 ceph osd pool set gnocchi-cache hit_set_count 1 ceph osd pool set gnocchi-cache hit_set_period 3600 ceph osd pool set gnocchi-cache cache_target_dirty_ratio 0.4 ceph osd pool set gnocchi-cache cache_target_dirty_high_ratio 0.6 ceph osd pool set gnocchi-cache cache_target_full_ratio 0.8 ceph osd pool set gnocchi-cache target_max_bytes 200000000000 ceph osd pool set gnocchi-cache target_max_objects 200000 ceph osd pool set gnocchi-cache cache_min_flush_age 600 ceph osd pool set gnocchi-cache cache_min_evict_age 1800 四、FAQ 1、日志盘bug ceph-disk list vim /usr/lib/python2.7/dist-packages/ceph_disk/main.py sgdisk --typecode=6:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sda sgdisk --typecode=1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/ssd chown -R ceph:ceph /dev/nvme0n1p2 parted /dev/sdg mklabel GPT mkpart cache xfs 0G 380G mkpart sdb_journal xfs 380G 385G 2、NO_PUBKEY E84AC2C0460F3994 apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E84AC2C0460F3994 3、** ERROR: error creating empty object store in /var/lib/ceph/tmp/mnt.blV9DA: (13) Permission denied 将ceph集群需要使用的所有磁盘权限，所属用户、用户组改给ceph chown ceph:ceph /dev/xvdb 4、clock skew detected on mon.ceph-node2, mon.ceph-node3 Monitor clock skew detected /etc/init.d/ntpd stop service chrony stop ntpdate time.nist.gov service chrony start 5、 OSD_SCRUB_ERRORS: 1 scrub errors；PG_DAMAGED: Possible data damage: 1 pg inconsistent docker exec ceph_mon sh -c 'for i in \"`ceph health detail|grep -vE \\\"(PG_DAMAGED|HEALTH_ERR|OSD_SCRUB_ERRORS)\\\"|awk \\\"{print $2}\\\"`\";do ceph pg repair $i ;done' for i in `ceph health detail|grep 'active+clean+inconsistent'|awk '{print $2}'`;do ceph pg repair $i ;done ceph -s 6、往下刷缓存 rados -p images-cache cache-flush-evict-all rados -p volumes-cache cache-flush-evict-all rados -p backups-cache cache-flush-evict-all rados -p vms-cache cache-flush-evict-all rados -p gnocchi-cache cache-flush-evict-all 7、application not enabled on 1 pool(s) ceph osd pool application enable k8s rbd 8、手动刷新 ceph-cache rados -p cache cache-flush-evict-all 9、新版本特性问题 rbd: image foo: image uses unsupported features: 0x38 rbd info foo rbd image 'foo': size 1024 MB in 256 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10612ae7234b format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: layering: 支持分层 striping: 支持条带化 v2 exclusive-lock: 支持独占锁 object-map: 支持对象映射（依赖 exclusive-lock ） fast-diff: 快速计算差异（依赖 object-map ） deep-flatten: 支持快照扁平化操作 journaling: 支持记录 IO 操作（依赖独占锁） rbd feature disable foo exclusive-lock, object-map, fast-diff, deep-flatten 10、源 echo \"deb https://download.ceph.com/debian-octopus/ bionic main\" > /etc/apt/sources.list.d/ceph.list apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E84AC2C0460F3994 apt update apt install ceph-common 11、时间同步问题 Server server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst # 记录系统时钟的漂移 driftfile /var/lib/chrony/drift # 初始同步允许较大时间偏差，允许大幅调整时间，该配置允许在前 3 次同步时直接调整时间 makestep 1.0 3 # 启用实时硬件时钟同步 rtcsync # Allow NTP client access from local network. allow 192.168.0.0/16 allow 10.0.0.0/24 # 如果不需要其他机器访问该客户端的 NTP 服务，可以禁用访问 # deny all logdir /var/log/chrony Client server 10.0.0.1 iburst driftfile /var/lib/chrony/drift makestep 1.0 3 rtcsync logdir /var/log/chrony 验证 chronyc sources -v 210 Number of sources = 1 .-- Source mode '^' = server, '=' = peer, '#' = local clock. / .- Source state '*' = current synced, '+' = combined, '-' = not combined, | / '?' = unreachable, 'x' = time may be in error, '~' = too variable. || .- xxxx [ xxxx ] || Reachability register (octal) -. +---------------- || Log2(Polling interval) --. -. | Last sample || | .--|----. | (time offset) MS Name/IP address Stratum Poll Reach LastRx Last sample =============================================================================== ^* 10.0.0.1 2 6 377 32 -10us[ -15us] +/- 100ms ^* 表示已成功与 10.0.31.13 同步。 Reach 的值为 377 表示与服务器的通信良好。 # 检查当前时间差 chronyc tracking # 强制同步时间源 chronyc -a makestep timedatectl 适用于一次性手动时间同步，非持续服务 ntpdate ntp8.flashdance.cx 12、ceph在容器启动前先启动 需要在每个节点的 service 中添加 docker.service，等 docker 先启动 vim /etc/systemd/system/ceph-de9c839a-bd3e-11eb-aefc-377d8c7eac71@.service ... After=network-online.target local-fs.target time-sync.target docker.service Wants=network-online.target local-fs.target time-sync.target docker.service ... 13、Possible data damage: 1 pg inconsistent 不一致性校验失败，数据不一致性（inconsisitent）指对象的大小不正确，恢复结束后某副本出现了对象丢失的情况，数据不一致会导致清理失败（scrub error）。Ceph在存储过程中，由于特殊原因，可能遇到对象信息大小和物理磁盘上实际数据大小不一致的情况，也会导致清理失败。 (1)停止osd 先查看所要恢复pg ceph health detail ceph pg repair 4.3f systemctl stop ceph-de9c839a-bd3e-11eb-aefc-377d8c7eac71@osd.14.service (2)刷入日志 需将相应数据复制进容器 docker cp /etc/ceph b951114a1b9d:/etc docker cp /var/lib/ceph/de9c839a-bd3e-11eb-aefc-377d8c7eac71/osd.14/ b951114a1b9d:/var/lib/ceph/osd/ceph-14/ docker exec -it b951114a1b9d bash ceph-osd -i 14 --flush-journal (3)启动osd systemctl start ceph-de9c839a-bd3e-11eb-aefc-377d8c7eac71@osd.14.service (4)修复 ceph health detail ceph pg repair 4.3f "},"notes/ceph/crushmap.html":{"url":"notes/ceph/crushmap.html","title":"crushmap","keywords":"","body":"crushmap 一、获取与使用 crushmap 1、获得 crush map，获得默认 crushmap (加密) ceph osd getcrushmap -o crushmap.dump 2、转换 crushmap 格式 (加密 -> 明文格式) crushtool -d crushmap.dump -o crushmap.txt 3、转换 crushmap 格式(明文 -> 加密格式) crushtool -c crushmap.txt -o crushmap.done 4、重新使用新 crushmap ceph osd setcrushmap -i crushmap.done 二、crushmap # begin crush map tunable choose_local_tries 0 tunable choose_local_fallback_tries 0 tunable choose_total_tries 50 tunable chooseleaf_descend_once 1 tunable chooseleaf_vary_r 1 tunable straw_calc_version 1 # devices device 0 osd.0 device 1 osd.1 device 2 osd.2 device 3 osd.3 device 4 osd.4 device 5 osd.5 device 6 osd.6 device 7 osd.7 device 8 osd.8 device 9 osd.9 device 10 osd.10 device 11 osd.11 device 12 osd.12 device 13 osd.13 device 14 osd.14 # types type 0 osd type 1 host type 2 chassis type 3 rack type 4 row type 5 pdu type 6 pod type 7 room type 8 datacenter type 9 region type 10 root # buckets host ceph-node1-sata { id -3 # do not change unnecessarily # weight 4.000 alg straw hash 0 # rjenkins1 item osd.0 weight 1.000 item osd.1 weight 1.000 item osd.2 weight 1.000 item osd.3 weight 1.000 } host ceph-node2-sata { id -4 # do not change unnecessarily # weight 4.000 alg straw hash 0 # rjenkins1 item osd.4 weight 1.000 item osd.5 weight 1.000 item osd.6 weight 1.000 item osd.7 weight 1.000 } host ceph-node3-sata { id -5 # do not change unnecessarily # weight 4.000 alg straw hash 0 # rjenkins1 item osd.8 weight 1.000 item osd.9 weight 1.000 item osd.10 weight 1.000 item osd.11 weight 1.000 } root sata { id -1 # do not change unnecessarily # weight 12.000 alg straw hash 0 # rjenkins1 item ceph-node1-sata weight 4.000 item ceph-node2-sata weight 4.000 item ceph-node3-sata weight 4.000 } host ceph-node1-ssd { id -6 # do not change unnecessarily # weight 1.000 alg straw hash 0 # rjenkins1 item osd.12 weight 1.000 } host ceph-node2-ssd { id -7 # do not change unnecessarily # weight 1.000 alg straw hash 0 # rjenkins1 item osd.13 weight 1.000 } host ceph-node3-ssd { id -8 # do not change unnecessarily # weight 1.000 alg straw hash 0 # rjenkins1 item osd.14 weight 1.000 } root ssd { id -2 # do not change unnecessarily # weight 3.000 alg straw hash 0 # rjenkins1 item ceph-node1-ssd weight 1.000 item ceph-node2-ssd weight 1.000 item ceph-node3-ssd weight 1.000 } # rules rule sata { ruleset 0 type replicated min_size 1 max_size 10 step take sata step chooseleaf firstn 0 type host step emit } rule ssd { ruleset 1 type replicated min_size 1 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit } # end crush map "},"notes/ceph/cephconf.html":{"url":"notes/ceph/cephconf.html","title":"ceph.conf","keywords":"","body":"ceph.conf 一、优化 1、 Kernel pid max echo 4194303 > /proc/sys/kernel/pid_max 2、 设置MTU，交换机端需要支持该功能，系统网卡设置才有效果 配置文件追加MTU=9000 3、 read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作 echo “8192” > /sys/block/sda/queue/read_ahead_kb 4、 swappiness, 主要控制系统对swap的使用 echo “vm.swappiness = 0″/etc/sysctl.conf ; sysctl –p 5、 I/O Scheduler，SSD要用noop，SATA/SAS使用deadline echo “deadline” >/sys/block/sd[x]/queue/scheduler echo “noop” >/sys/block/sd[x]/queue/scheduler 二、配置 [global] fsid = 6c4d5e9e-03ee-4812-a565-cec78596ae68 mon_initial_members = comp-1, comp-2, comp-3, comp-4, comp-5 mon_host = 172.18.20.1,172.18.20.2,172.18.20.3,172.18.20.4,172.18.20.5 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd pool default size = 2 osd pool default min size = 1 mon_pg_warn_max_per_osd = 0 mon osd full ratio = .98 mon osd nearfull ratio = .95 [osd] osd journal size = 20000 osd mkfs type = xfs osd mkfs options xfs = -f filestore xattr use omap = true filestore min sync interval = 10 filestore max sync interval = 15 filestore queue max ops = 25000 filestore queue max bytes = 10485760 filestore queue committing max ops = 5000 filestore queue committing max bytes = 10485760000 journal max write bytes = 1073714824 journal max write entries = 10000 journal queue max ops = 50000 journal queue max bytes = 10485760000 osd max write size = 512 osd client message size cap = 2147483648 osd deep scrub stride = 131072 osd op threads = 8 osd disk threads = 4 osd map cache size = 1024 osd map cache bl size = 128 osd mount options xfs = \"rw,noexec,nodev,noatime,nodiratime,nobarrier\" osd recovery op priority = 4 osd recovery max active = 10 osd max backfills = 4 osd crush update on start = false [client] rbd cache = true rbd cache size = 268435456 rbd cache max dirty = 134217728 rbd cache max dirty age = 5 [client.glance] key = AQA5G6JXXgc7KBAAh6ug4Amkb0nMdS7BY4IUmQ== [client.cinder] key = AQBLHKJXNZe6MhAA6QBlPu7tjFYQPBhRkQaRnA== [client.cinder-backup] key = AQDzxaFXbd5jABAARm71lJW1anfe6XMFxuGiAw== [global] cluster_network = 192.168.180.0/24 #集群网络 public_network = 192.168.170.0/24 #管理网络，分配集群的外网网段，即对外数据交流的网段。 osd_pool_default_size = 2 #osd的默认副本数，默认值3 osd_pool_default_min_size = 1 # 这是处于degraded状态的副本数目，它应该小于osd_pool_default_size的值， 为存储池中的object设置最小副本数目来确认写操作。即使集群处于degraded状态。 如果最小值不匹配，Ceph将不会确认写操作给客户端。 osd_pool_default_pg_num = 128 #每个存储池默认的pg数，默认32 osd_pool_default_pgp_num = 128 #PG和PGP的个数应该保持一致。PG和PGP的值很大程度上取决于集群大小，默认32 osd_crush_chooseleaf type = 1 #CRUSH规则用到chooseleaf时的bucket的类型，默认值就是1 osd_journal_size = 1024 # 缺省值为0。你应该使用这个参数来设置日志大小。 日志大小应该至少是预期磁盘速度和filestore最大同步时间间隔的两倍。如果使用了SSD日志， 最好创建大于10GB的日志，并调大filestore的最小、最大同步时间间隔。 mon_pg_warn_max_per_osd = 1000 #每个osd上pg数量警告值，这个可以根据具体规划来设定 mon_osd_full_ratio = .85 #存储使用率达到85%将不再提供数据存储，默认0.95 mon_osd_backfillfull_ratio = .80 #OSD被认为已满而不能回填之前的设备空间利用率阈值，默认0.90 mon_osd_nearfull_ratio = .75 #存储使用率达到70%集群将会warn状态，默认0.85 osd_deep_scrub_randomize_ratio = 0.01 #随机深度清洗概率,值越大，随机深度清洗概率越高,太高会影响业务 [mon] mon_clock_drift_allowed = 1 #monitor间的clock drift，默认值0.05 mon_osd_min_down_reporters = 13 #向monitor报告down的最小OSD数，默认值2 mon_osd_down_out_interval = 600 #标记一个OSD状态为down和out之前ceph等待的秒数，默认值600 mon_cpu_threads = 4 mon_osd_cache_size = 500 mon_osd_cache_size_min = 134217728 [osd] osd_journal_size = 20000 #osd journal大小，默认5120 osd_max_write_size = 512 #OSD一次可写入的最大值(MB)，默认值90 osd_client_message_size_cap = 2147483648 #客户端允许在内存中的最大数据(bytes)，默认值524288000,500M osd_deep_scrub_stride = 131072 #在Deep Scrub时候允许读取的字节数(bytes)，默认值524288 osd_map_cache_size = 1024 #保留OSD Map的缓存(MB)，默认值50 osd_recovery_sleep = 0 #recovery的时间间隔，会影响recovery时长，如果recovery导致业务不正常，可以调大该值，增加时间间隔，默认0 osd_recovery_op_priority = 3 #恢复操作优先级，取值1-63，值越高占用资源越高，默认值3 osd_recovery_max_chunk = 1048576 #设置恢复数据块的大小，以防网络阻塞，默认为8388608 osd_recovery_max_active = 0 #同一时间内活跃的恢复请求数，默认值0 osd_recovery_max_single_start = 1 # 和osd_recovery_max_active一起使用。 假设配置osd_recovery_max_single_start为1， osd_recovery_max_active为3 意味着OSD在某个时刻会为一个PG启动一个恢复操作，而且最多可以有三个恢复操作同时处于活动状态。 osd_max_backfills = 4 #一个OSD允许的最大backfills数，默认值1 osd_min_pg_log_entries = 250 #修建PGLog是保留的最小PGLog数，默认值250 osd_max_pg_log_entries = 10000 #修建PGLog是保留的最大PGLog数，默认值10000 osd_mon_heartbeat_interval = 40 #OSD ping一个monitor的时间间隔，默认值30 ms_dispatch_throttle_bytes = 1048576000 #等待派遣的最大消息数，默认值 104857600 objecter_inflight_ops = 819200 #客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限，默认值1024 osd_op_log_threshold = 50 #一次显示多少操作的log，默认值5 filestore_min_sync_interval = 0.1 #从日志到数据盘最小同步间隔(seconds)，默认值0.01 filestore_max_sync_interval = 15 #从日志到数据盘最大同步间隔(seconds)，默认5 filestore_queue_max_ops = 25000 #数据盘最大接受的操作数，默认值50 filestore_queue_max_bytes = 1048576000 #数据盘一次操作最大字节数(bytes)，默认104857600 filestore_split_multiple = 8 #前一个子目录分裂成子目录中的文件的最大数量，默认值2 filestore_merge_threshold = -10 #前一个子类目录中的文件合并到父类的最小数量，默认值-10 filestore_fd_cache_size = 1024 #对象文件句柄缓存大小，默认值128 filestore_op_threads = 32 #并发文件系统操作数，默认值2 journal_max_write_bytes = 1073714824 #journal一次性写入的最大字节数(bytes)，默认值10485760 journal_max_write_entries = 10000 #journal一次性写入的最大记录数，默认值100 osd_scrub_begin_hour = 22 #清洗开始时间为晚上22点，默认0 osd_scrub_end_hour = 7 #清洗结束时间为早上7点，默认0 osd_crush_update_on_start = false # 新加的osd会up/in,但并不会更新crushmap，prepare+active期间不会导致数据迁移，默认true osd_crush_chooseleaf_type = 0 #CRUSH规则用到chooseleaf时的bucket的类型，默认值1 rbd_op_threads = 10 #rbd 操作线程数，默认值1 [client] rbd_cache = true #RBD缓存，默认值 true rbd_op_threads = 10 #rbd 操作线程数，默认值1 rbd_cache_size = 335544320 #RBD缓存大小(bytes)，默认值33554432 rbd_cache_max_dirty = 134217728 #缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through，默认值25165824 rbd_cache_max_dirty_age = 30 #在被刷新到存储盘前dirty数据存在缓存的时间(seconds)，默认值1 rbd_cache_writethrough_until_flush = false # 该选项是为了兼容linux-2.6.32之前的virtio驱动， 避免因为不发送flush请求，数据不回写，设置该参数后， librbd会以writethrough的方式执行io， 直到收到第一个flush请求，才切换为writeback方式。默认值true rbd_cache_max_dirty_object = 2 # 最大的Object对象数，表示通过rbd cache size计算得到， librbd默认以4MB为单位对磁盘Image进行逻辑切分， 每个chunk对象抽象为一个Object，librbd中以Object为单位来管理缓存， 增大该值可以提升性能，默认值0 rbd_cache_target_dirty = 235544320 #开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty，默认值16777216 三、其他 ceph osd set-nearfull-ratio 0.85 ceph osd set-backfillfull-ratio 0.90 ceph osd set-full-ratio 0.95 "},"notes/python/xpath.html":{"url":"notes/python/xpath.html","title":"xpath 相关","keywords":"","body":"xpath 相关 XPath，全称 XML Path Language，既 XML 路径语言，它是一门在 XML 文档中查找信息的语言。同样适用于 HTML 文档内容的搜索。 官方文档 一、xpath 语法 1、常用规则 路径表达式 描述 nodename 选取此节点的所有子节点 / 从根节点选取 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置 . 选取当前节点 .. 选取当前节点的父节点 @ 选取属性 2、xpath 运算符 运算符 描述 实例 返回值 or 或 age=18 or age=20 age=18：True；age=21：False and 与 age>18 and age age=20：True；age=21：False mod 计算除法的余数 5 mod 2 1 \\ 计算两个节点集 //book \\ //cd 返回所有拥有 book 和 cd 元素的节点集 + 加法 5 + 3 8 - 减法 5 - 3 2 * 乘法 5 * 3 15 div 除法 8 div 4 2 = 等于 age=19 判断简单，不再赘述 != 不等于 age!=19 判断简单，不再赘述 小于 age 判断简单，不再赘述 小于等于 age 判断简单，不再赘述 > 大于 age>19 判断简单，不再赘述 >= 大于等于 age>=19 判断简单，不再赘述 3、实例 路径表达式 结果 bookstore 选取 bookstore 元素的所有子节点。 /bookstore 选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ bookstore/book 选取属于 bookstore 的子元素的所有 book 元素。 //book 选取所有 book 子元素，而不管它们在文档中的位置。 bookstore//book 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。 //@lang 选取名为 lang 的所有属性。 /bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素。 /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素。 /bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素。 /bookstore/book[position() 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。 //title[@lang] 选取所有拥有名为 lang 的属性的 title 元素。 //title[@lang='eng'] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。 /bookstore/book[price>35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 /bookstore/book[price>35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 * 匹配任何元素节点。 @* 匹配任何属性节点。 node() 匹配任何类型的节点。 /bookstore/* 选取 bookstore 元素的所有子元素。 //* 选取文档中的所有元素。 //title[@*] 选取所有带有属性的 title 元素。 //book/title \\ //book/price 选取 book 元素的所有 title 和 price 元素。 //title \\ //price 选取文档中的所有 title 和 price 元素。 /bookstore/book/title \\ //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 二、实例 1、安装 pip install lxml 2、编写代码实例 from lxml import etree text = ''' first item second item third item fourth item fifth item ''' html = etree.HTML(text) # 获取所有祖先节点 result = html.xpath('//li[1]/ancestor::*') print(result) # 获取 div 祖先节点 result = html.xpath('//li[1]/ancestor::div') print(result) # 获取当前节点所有属性值 result = html.xpath('//li[1]/attribute::*') print(result) # 获取 href 属性值为 link1.html 的直接子节点 result = html.xpath('//li[1]/child::a[@href=\"link1.html\"]') print(result) # 获取所有的的子孙节点中包含 span 节点但不包含 a 节点 result = html.xpath('//li[1]/descendant::span') print(result) # 获取当前所有节点之后的第二个节点 result = html.xpath('//li[1]/following::*[2]') print(result) # 获取当前节点之后的所有同级节点 result = html.xpath('//li[1]/following-sibling::*') print(result) \"\"\" [, , , ] [] ['item-0'] [] [] [] [, , , ] \"\"\" "},"notes/python/regular_expression.html":{"url":"notes/python/regular_expression.html","title":"正则表达式相关","keywords":"","body":"正则表达式相关 一、常用元字符和语法 语法 说明 实例 匹配结果 . 匹配任意除换行符\"\\n\"外的所有字符。 a.c abc [..] 字符集，对应位置可以是字符集中任意字符。 a[bcd]e abeaceade \\d 数字，[0-9] a\\dc a1c \\D 非数字，\\d\\ a\\Dc abc \\s 空白字符，匹配任何空白字符，包括空格、制表符、换页符等等，[ \\t\\r\\n\\f\\v] a\\sc a c \\S 非空白字符，\\s\\ a\\Sc abc \\w 单词字符，[A-Za-z0-9] a\\wc abc \\W 非单词字符，\\w\\ a\\Wc a c * 匹配前一个字符0次或多次 abc* ababccc + 匹配前一个字符1次或多次 abc+ abcabccc ? 匹配前一个字符0次或1次 abc？ ababc {m} 匹配前一个字符m次 ab{2}c abbc {m,n} 匹配前一个字符m到n次。m和n可以省略，若省略m，则匹配0到n次；若省略n，则匹配m至多次 ab{1,2}c abcabbc ^ 匹配字符串开头。在多行时，匹配每一行开头。匹配输入字符串的开始位置，除非在方括号表达式中使用，此时它表示不接受该字符集合。 ^abc abc $ 匹配字符串末尾。在多行时，匹配每一行末尾。 abc$ abc \\A 仅匹配字符串开头。 \\Aabc abc \\Z 仅匹配字符串末尾。 abc\\Z abc \\b 匹配\\w和\\W之间匹配一个单词边界，即字与空格间的位置。 a\\b!bc a!bc \\B 非单词边界匹配。[^\\b] a\\Bbc abc \\ 左右表达式任意匹配一个 abc\\ def abcdef (...) 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用。 (abc){2}a(123\\ 456)c abcabca456c [\\u4e00-\\u9fa5] 匹配中文字符 [\\s\\S]*? 表示匹配任意字符 二、相关概念 1、数量词的贪婪模式和非贪婪模式 正则表达式通常用于在文本中查找匹配的字符串。 贪婪模式：总是尝试匹配尽可能多的字符。 非贪婪模式：总是尝试匹配尽可能少的字符。 Python里数量词默认是贪婪的 例：正则表达式\"ab\"，如果用于查找\"abbbc\"，将找到\"abbb\"。如果用非贪婪模式的数量词\"ab\\?\"，将找到\"a\"。 源字符串：aatest1bbtest2cc 正则表达式一：.* # 贪婪模式 匹配结果一：test1bbtest2 正则表达式二：.*? # 非贪婪模式 匹配结果二：test1（这里指的是一次匹配结果，所以没包括test2） 2、原生字符串 匹配一个数字的\"\\d\"可以写成r\"\\d\" 3、flags re.I(全拼：IGNORECASE): 忽略大小写（括号内是完整写法，下同） re.M(全拼：MULTILINE): 多行模式，改变'^'和'$'的行为（参见上图） re.S(全拼：DOTALL): 点任意匹配模式，改变'.'的行为 re.L(全拼：LOCALE): 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定 re.U(全拼：UNICODE): 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性 re.X(全拼：VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。 4、group 除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如：^(\\d{3})-(\\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码 m = re.match(r'^(\\d{3})-(\\d{3,8})$', '010-12345') print(m.group(0)) print(m.group(1)) print(m.group(2)) # 010-12345 # 010 # 12345 三、常用方法 1、match re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。 print(re.match('com', 'www.test.com')) 2、search 扫描整个字符串并返回第一个成功的匹配 print(re.search('www', 'www.test.com').span()) 3、findall 在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 import re p = re.compile(r'\\d+') print p.findall('one1two2three3four4') ### output ### # ['1', '2', '3', '4'] 4、sub sub用于替换字符串中的匹配项 phone = \"2004-959-559 # 这是一个电话号码\" # 删除注释 num = re.sub(r'#.*$', \"\", phone) print (\"电话号码 : \", num) 5、split split 方法按照能够匹配的子串将字符串分割后返回列表 re.split('\\W+', 'runoob, runoob, runoob.') ['runoob', 'runoob', 'runoob', ''] 6、compile compile 函数用于编译正则表达式，生成一个正则表达式（ Pattern ）对象，供 match() 和 search() 这两个函数使用。 如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式 # 编译 tele = re.compile(r'^(\\d{3})-(\\d{3,8})$') # 使用： print(tele.match('010-12345').groups()) 7、finditer 和 findall 类似，在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回 it = re.finditer(r\"\\d+\",\"12a32bc43jf3\") for match in it: print (match.group() ) "},"notes/python/django.html":{"url":"notes/python/django.html","title":"Django 相关","keywords":"","body":"Django 相关 "},"notes/python/flask.html":{"url":"notes/python/flask.html","title":"Flask 相关","keywords":"","body":"Flask 相关 flask 官网文档 Flask 教程 一、准备 1、安装 pip install Flask 2、测试 python -c \"import flask\" flask --version 3、一般flask项目的目录结构 flask-demo/ ├ run.py # 应用启动程序 ├ config.py # 环境配置 ├ requirements.txt # 列出应用程序依赖的所有Python包 ├ tests/ # 测试代码包 │ ├ __init__.py │ └ test_*.py # 测试用例 └ myapp/ ├ admin/ # 蓝图目录 ├ static/ │ ├ css/ # css文件目录 │ ├ img/ # 图片文件目录 │ └ js/ # js文件目录 ├ templates/ # 模板文件目录 ├ __init__.py ├ forms.py # 存放所有表单，如果多，将其变为一个包 ├ models.py # 存放所有数据模型，如果多，将其变为一个包 └ views.py # 存放所有视图函数，如果多，将其变为一个包 二、使用 1、新建文件 hello.py ，并输入以下内容 from flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return 'Hello, World!' if __name__ == '__main__': app.debug = True # 设置调试模式，生产模式的时候要关掉debug app.run(host=\"0.0.0.0\") 2、运行 python hello.py * Running on http://0.0.0.0:5000/ 在浏览器中访问 IP:5000 IP 为你的虚拟机地址，例： http://192.168.1.1:5000 3、模板 flask基于jinja2模板引擎实现 jinja2 相关 编辑 hello.html Hello Sample {% if name %} Hello {{ name }}! {% else %} Hello World! {% endif %} 编辑 hello.py from flask import Flask from flask import render_template app = Flask(__name__) @app.route('/hello') @app.route('/hello/') def hello(name=None): return render_template('hello.html', name=name) if __name__ == '__main__': app.run(host='0.0.0.0', debug=True) 浏览器访问： http://192.168.1.1:5000/hello/man 4、模板继承 新建，并编辑 layout.html 的模板 Hello Sample {% block body %} {% endblock %} 修改之前的 hello.html {% extends \"layout.html\" %} {% block body %} {% if name %} Hello {{ name }}! {% else %} Hello World! {% endif %} {% endblock %} 浏览器访问： http://192.168.1.1:5000/hello/man ，查看源码 5、HTML自动转义 打开页面，你会看到”Hello Flask”字样，而且”Flask”是斜体的，因为我们加了”em”标签。但有时我们并不想让这些HTML标签自动转义，特别是传递表单参数时，很容易导致HTML注入的漏洞。我们把上面的代码改下，引入”Markup”类： from flask import Flask, Markup app = Flask(__name__) @app.route('/') def index(): return Markup('Hello %s') % 'Flask' 6、request 对象 layout.html Hello Sample {% block body %} {% endblock %} login.html {% extends \"layout.html\" %} {% block body %} Hello {{ title }}, please login by: {% endblock %} from flask import Flask,url_for,request,render_template @app.route('/login', methods=['POST', 'GET']) def login(): if request.method == 'POST': if request.form['user'] == 'admin': return 'Admin login successfully!' else: return 'No such user!' title = request.args.get('title', 'Default') return render_template('login.html', title=title) if __name__ == \"__main__\": app.run(debug=True) request中”method”变量可以获取当前请求的方法，即”GET”, “POST”, “DELETE”, “PUT”等；”form”变量是一个字典，可以获取Post请求表单中的内容，在上例中，如果提交的表单中不存在”user”项，则会返回一个”KeyError”，可以不捕获，页面会返回400错误（想避免抛出这”KeyError”，可以用request.form.get(“user”)来替代）。而”request.args.get()”方法则可以获取Get请求URL中的参数，该函数的第二个参数是默认值，当URL参数不存在时，则返回默认值。 在当前目录下，创建一个子目录”templates”（注意，一定要使用这个名字）。然后在”templates”目录下，添加 7、会话 session from flask import Flask,url_for,request,render_template,redirect,session @app.route('/login', methods=['POST', 'GET']) def login(): if request.method == 'POST': if request.form['user'] == 'admin': session['user'] = request.form['user'] return 'Admin login successfully!' else: return 'No such user!' if 'user' in session: return 'Hello %s!' % session['user'] else: title = request.args.get('title', 'Default') return render_template('login.html', title=title) @app.route('/logout') def logout(): session.pop('user', None) return redirect(url_for('login')) app.secret_key = '123456' if __name__ == \"__main__\": app.run(debug=True) 可以看到，”admin”登陆成功后，再打开”login”页面就不会出现表单了。然后打开logout页面可以登出。session对象的操作就跟一个字典一样。特别提醒，使用session时一定要设置一个密钥”app.secret_key”，如上例。不然会得到一个运行时错误，内容大致是”RuntimeError: the session is unavailable because no secret key was set”。密钥要尽量复杂，最好使用一个随机数，这样不会有重复，上面的例子不是一个好密钥。 8、构建响应 在之前的例子中，请求的响应都是直接返回字符串内容，或者通过模板来构建响应内容然后返回。其实也可以先构建响应对象，设置一些参数（比如响应头）后，再将其返回。修改下上例中的Get请求部分： from flask import Flask,url_for,request,render_template,redirect,session,make_response @app.route('/login', methods=['POST', 'GET']) def login(): if request.method == 'POST': ... if 'user' in session: ... else: title = request.args.get('title', 'Default') response = make_response(render_template('login.html', title=title), 200) response.headers['key'] = 'value' return response if __name__ == \"__main__\": app.run(debug=True) 9、Cookie的使用 from flask import Flask,url_for,request,render_template,redirect,session,make_response import time @app.route('/login', methods=['POST', 'GET']) def login(): response = None if request.method == 'POST': if request.form['user'] == 'admin': session['user'] = request.form['user'] response = make_response('Admin login successfully!') response.set_cookie('login_time', time.strftime('%Y-%m-%d %H:%M:%S')) ... else: if 'user' in session: login_time = request.cookies.get('login_time') response = make_response('Hello %s, you logged in on %s' % (session['user'], login_time)) ... return response app.secret_key = '123456' if __name__ == \"__main__\": app.run(debug=True) 10、错误处理 from flask import Flask,abort app = Flask(__name__) @app.route('/error') def error(): abort(404) if __name__ == \"__main__\": app.run(debug=True) from flask import Flask,abort app = Flask(__name__) @app.errorhandler(404) def page_not_found(error): return render_template('404.html'), 404 if __name__ == \"__main__\": app.run(debug=True) 在实际开发过程中，并不会经常使用”abort()”来退出，常用的错误处理方法一般都是异常的抛出或捕获。装饰器”@app.errorhandler()”除了可以注册错误代码外，还可以注册指定的异常类型。让我们来自定义一个异常： class InvalidUsage(Exception): status_code = 400 def __init__(self, message, status_code=400): Exception.__init__(self) self.message = message self.status_code = status_code @app.errorhandler(InvalidUsage) def invalid_usage(error): response = make_response(error.message) response.status_code = error.status_code return response @app.route('/exception') def exception(): raise InvalidUsage('No privilege to access the resource', status_code=403) if __name__ == \"__main__\": app.run(debug=True) 在上面的代码中定义了一个异常”InvalidUsage”，同时通过装饰器”@app.errorhandler()”修饰了函数”invalid_usage()”，装饰器中注册了我们刚定义的异常类。这也就意味着，一但遇到”InvalidUsage”异常被抛出，这个”invalid_usage()”函数就会被调用。 11、url 重定向 Flask的URL规则是基于Werkzeug的路由模块。模块背后的思想是基于 Apache 以及更早的 HTTP 服务器主张的先例，保证优雅且唯一的 URL。 @app.route('/projects/') def projects(): return 'The project page' @app.route('/about') def about(): return 'The about page' 访问第一个路由不带/时，Flask会自动重定向到正确地址。 访问第二个路由时末尾带上/后Flask会直接报404 NOT FOUND错误。 重定向”redirect()”函数的使用在上面例子中已有出现。作用就是当客户端浏览某个网址时，将其导向到另一个网址。常见的例子，比如用户在未登录时浏览某个需授权的页面，将其重定向到登录页要求其登录。 from flask import session, redirect @app.route('/') def index(): if 'user' in session: return 'Hello %s!' % session['user'] else: return redirect(url_for('login'), 302) 12、配置热加载 watchdog pip install watchdog import json from watchdog.observers import Observer from watchdog.events import FileSystemEventHandler CONFIG_PATH = \"config.json\" config = {} class ConfigHandler(FileSystemEventHandler): def on_modified(self, event): if event.src_path.endswith(CONFIG_PATH): load_config() def load_config(): global config with open(CONFIG_PATH, \"r\") as f: config = json.load(f) print(\"配置已更新:\", config) # 监听配置文件变更 event_handler = ConfigHandler() observer = Observer() observer.schedule(event_handler, \".\", recursive=False) observer.start() try: while True: pass except KeyboardInterrupt: observer.stop() observer.join() importlib.reload() import importlib import config # 假设配置在 config.py 中 def reload_config(): global config importlib.reload(config) print(\"配置已重新加载:\", config.settings) # 例如 config.py 里有 settings 变量 # 调用 reload_config() 即可刷新配置 inotify-tools apk add inotify-tools # Alpine apt-get update && apt-get install -y inotify-tools # Debian/Ubuntu while inotifywait -e modify /conf/app.conf; do echo \"Config file changed, reloading...\" # 在这里可以执行重载命令，比如： # nginx -s reload done apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: app-container image: my-app volumeMounts: - name: config-volume mountPath: /conf - name: reload-sidecar image: busybox command: [ \"sh\", \"-c\", \"while inotifywait -e modify /conf/app.conf; do echo 'Reloading...'; kill -HUP 1; done\" ] volumeMounts: - name: config-volume mountPath: /conf volumes: - name: config-volume configMap: name: my-config 13、gunicorn gunicorn_config.py from gevent import monkey monkey.patch_all() from api.utils.log_utils import initRootLogger initRootLogger(\"ragflow_server\") import logging import gevent import signal from api import settings from api.db.runtime_config import RuntimeConfig from api.db.services.document_service import DocumentService from api import utils from gevent.event import Event from api.db.db_models import init_database_tables as init_web_db from api.db.init_data import init_web_data from api.versions import get_ragflow_version from api.utils import show_configs from rag.settings import print_rag_settings from rag.utils.redis_conn import RedisDistributedLock stop_event = Event() def update_progress(): redis_lock = RedisDistributedLock(\"update_progress\", timeout=60) while not stop_event.is_set(): acquired = False try: acquired = redis_lock.acquire() if not acquired: gevent.sleep(2) continue DocumentService.update_progress() gevent.sleep(6) except Exception as e: logging.exception(\"update_progress exception: %s\", e) finally: if acquired: try: redis_lock.release() except Exception as e: logging.warning(\"Failed to release Redis lock: %s\", e) def start_prometheus_server(): logging.info(\"promethues HTTP server start...\") from prometheus_client import start_http_server as prometheus_start_http_server # TODO settings.METRICS_PORT try: prometheus_start_http_server(8000) except Exception as e: logging.error(\"Failed to start promethues: %s\", e) def start_background_task(): gevent.spawn(update_progress) gevent.spawn(start_prometheus_server) def signal_handler(signum, frame): logging.info(f\"Received signal {signum}, shutting down...\") if not stop_event.is_set(): stop_event.set() def register_nacos_server(): if settings.NACOS.get(\"address\"): logging.info(\"Regist to nacos start...\") import nacos try: address = settings.NACOS.get(\"address\") namespace = settings.NACOS.get(\"namespace\") username = settings.NACOS.get(\"username\") password = settings.NACOS.get(\"password\") instance_ip = settings.NACOS.get(\"instance_ip\") instance_port = settings.NACOS.get(\"instance_port\") client = nacos.NacosClient( address, namespace=namespace, username=username, password=password ) client.add_naming_instance( \"ragflow\", instance_ip, instance_port, cluster_name=\"DEFAULT\", ephemeral=True, heartbeat_interval=5 ) logging.info(\"Successfully registered to Nacos\") except Exception as e: logging.error(\"Failed to register to Nacos: %s\", e) def on_starting(server): pass def post_fork(server, worker): settings.init_settings() RuntimeConfig.init_env() RuntimeConfig.init_config(JOB_SERVER_HOST=settings.HOST_IP, HTTP_PORT=settings.HOST_PORT) if not server.WORKERS: # 只有主进程的 worker的 server.WORKERS 为 {} logging.info(r\"\"\" ____ ___ ______ ______ __ / __ \\ / | / ____// ____// /____ _ __ / /_/ // /| | / / __ / /_ / // __ \\| | /| / / / _, _// ___ |/ /_/ // __/ / // /_/ /| |/ |/ / /_/ |_|/_/ |_|\\____//_/ /_/ \\____/ |__/|__/ \"\"\") logging.info( f'RAGFlow version: {get_ragflow_version()}' ) logging.info( f'project base: {utils.file_utils.get_project_base_directory()}' ) show_configs() print_rag_settings() init_web_db() init_web_data() register_nacos_server() start_background_task() signal.signal(signal.SIGTERM, signal_handler) signal.signal(signal.SIGINT, signal_handler) apps.py from pathlib import Path from flask import Blueprint, Flask __all__ = [\"app\"] app = Flask(__name__) pages_dir = [ Path(__file__).parent, Path(__file__).parent.parent / \"api\" / \"apps\", Path(__file__).parent.parent / \"api\" / \"apps\" / \"sdk\", ] client_urls_prefix = [ register_page(path) for dir in pages_dir for path in search_pages_path(dir) ] 包安装 pip install gunicorn==23.0.0 gevent==24.2.1 entrypoint.sh host=0.0.0.0 port=9380 while [ 1 -eq 1 ];do python3 -m gunicorn -w 2 -k gevent -b $host:$port -c /api/gunicorn_config.py api.apps:app done wait; 进程生命周期钩子 钩子函数 触发时机 作用 on_starting(server) 主进程启动时 在 Gunicorn 启动之前 执行，适合做全局初始化 post_fork(server, worker) Worker 进程 fork 之后 适合 Worker 进程的初始化操作 pre_fork(server, worker) Worker 进程 fork 之前 在主进程里执行，适用于日志初始化、环境变量设置 pre_exec(server) 执行 exec 之前 在 Gunicorn 重新执行（restart）前触发 when_ready(server) 主进程完成启动 适用于 Gunicorn 完成所有 worker 进程初始化后，可以用来发送通知 on_exit(server) 主进程退出前 适合做清理工作，如释放资源、发送日志等进程生命周期钩子 Worker 进程相关钩子 钩子函数 触发时机 作用 pre_request(worker, req) 收到请求之前 适用于日志记录、限流等操作 post_request(worker, req, environ, resp) 请求处理完毕 适用于记录访问日志、统计请求数等 worker_int(worker) Worker 进程收到 SIGINT 适合做 Worker 进程内的清理，如关闭数据库连接 worker_abort(worker) Worker 进程因超时被杀死 可以用于记录异常日志 worker_exit(server, worker) Worker 进程退出 可以用于释放资源，比如关闭数据库连接、缓存等 worker_ready(server, worker) Worker 进程初始化完成 Worker 完成启动后触发 nanny_callback(server, worker) Worker 进程异常时触发 适用于健康检查、异常恢复 14、uv uv 是一个比 pip 更快的 Python 依赖管理工具的命令 pip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple && \\ pip3 config set global.trusted-host mirrors.aliyun.com; \\ mkdir -p /etc/uv && \\ echo \"[[index]]\" > /etc/uv/uv.toml && \\ echo 'url = \"https://mirrors.aliyun.com/pypi/simple\"' >> /etc/uv/uv.toml && \\ echo \"default = true\" >> /etc/uv/uv.toml; \\ pipx install uv pyproject.toml uv init pyproject.toml 取代了 setup.py 和 requirements.txt，让 Python 项目更加标准化。 uv add requests uv.lock 重新解析 pyproject.toml 并更新 uv.lock uv resolve 遇到依赖冲突，可以删除 uv.lock 并重新解析： rm uv.lock uv resolve 用于在 Python 3.10 环境下，根据 uv.lock 严格安装锁定版本的依赖，不会重新解析 pyproject.toml uv sync --python 3.10 --frozen --frozen：严格按照锁定文件（如 requirements.txt 或 poetry.lock / pipenv.lock / uv.lock）中的版本安装，不做版本解析（类似于 pip install --require-hashes）。 命令 速度 依赖解析 严格版本 pip install -r requirements.txt 慢 需要解析 允许部分版本变动 pip install --require-hashes -r requirements.txt 慢 需要解析 严格 uv pip install -r requirements.txt 快 需要解析 允许部分版本变动 uv sync --frozen 快 不解析 严格 命令 描述 run 运行命令或脚本 init 创建一个新项目 add 向项目中添加依赖项 remove 从项目中移除依赖项 sync 更新项目的环境 lock 更新项目的锁定文件 export 将项目的锁定文件导出为其他格式 tree 显示项目的依赖树 tool 运行和安装由 Python 包提供的命令 python 管理 Python 版本和安装 pip 使用兼容 pip 的接口管理 Python 包 venv 创建虚拟环境 build 将 Python 包构建为源代码分发包和 wheels publish 将分发包上传到索引 cache 管理 uv 的缓存 self 管理 uv 可执行文件 version 显示 uv 的版本 generate-shell-completion 生成 shell 自动补全脚本 help 显示某个命令的文档 执行命令 环境处理 uv run xxx 自动关联虚拟环境： - 优先使用当前目录下的.venv - 若不存在会自动创建 - 无需手动激活/停用 python xxx.py 依赖当前Shell环境： - 需手动激活虚拟环境 sync # 同步所有依赖（包括dev） uv sync # 仅同步生产依赖 uv sync --production # 同步并清理多余包 uv sync --clean lock # 生成新锁定文件 uv lock # 检查更新但不写入（dry-run） uv lock --check # 强制重新解析 uv lock --update tree # 显示完整依赖树 uv tree # 仅显示指定包的依赖路径 uv tree flask # 反向追溯依赖（谁依赖了这个包） uv tree --reverse sqlalchemy # 输出为JSON格式 uv tree --format json 创建虚拟环境 uv venv -p 3.12 uv pip install -e . uv python 命令 描述 list 列出可用的Python安装版本 install 下载并安装Python版本 find 显示当前Python安装位置 pin 固定使用特定Python版本 dir 显示uv Python安装目录 uninstall 卸载Python版本 15、skywalking uv pip install apache-skywalking==1.1.0 def register_skywalking_server(): services = os.environ.get('SW_AGENT_COLLECTOR_BACKEND_SERVICES') if services: logging.info(\"starting registered to skywalking..\") from skywalking import agent try: agent.start() logging.info(\"Successfully register to skywalking\") except Exception as e: logging.error(\"Failed to register to skywalking: %s\", e) 16、logging logging.getLogger(\"nacos\").setLevel(logging.WARNING) logging.getLogger(\"nacos\").disabled = True logging.getLogger(\"nacos.heartbeat\").setLevel(logging.WARNING) "},"notes/python/python.html":{"url":"notes/python/python.html","title":"Python 相关","keywords":"","body":"Python 相关 一、安装 wget https://www.python.org/ftp/python/3.13.0/Python-3.13.0.tgz ./configure --enable-optimizations --with-ensurepip=install --with-openssl=/usr/local/openssl-1.1.1 --with-openssl-rpath=auto make altinstall -j 20 二、查看 gpu 是否可用 python -c \"import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())\" python -c \"import torch.distributed as dist; print(dist.is_nccl_available())\" 三、代码片断 1、enumerate， zip for i, v in enumerate(['tic', 'tac', 'toe']): for q, a in zip(questions, answers): >>> for x in range(1, 11): ... print(repr(x).rjust(2), repr(x*x).rjust(3), end=' ') ... # 注意前一行 'end' 的使用 ... print(repr(x*x*x).rjust(4)) for x in range(1, 11): ... print('{0:2d} {1:3d} {2:4d}'.format(x, x*x, x*x*x)) 2、remove_all_whitespace def remove_all_whitespace(text: str) -> str: \"\"\" 移除字符串中的所有空白字符（包括空格、换行、制表符等） Args: text: 要处理的文本 Returns: 处理后的无空白字符文本 \"\"\" return re.sub(r'\\s+', '', text) 3、collect_tools_from_modules def collect_tools_from_modules(module_paths): from langchain_core.tools import BaseTool import importlib import inspect tools = [] for path in module_paths: module = importlib.import_module(path) for _, obj in inspect.getmembers(module): if isinstance(obj, BaseTool): tools.append(obj) return tools 4、load_modules def load_modules(package=\"src.agents\"): for _, module_name, _ in pkgutil.iter_modules([package.replace(\".\", \"/\")]): # 排除 agents.py if module_name in [\"agents\"]: continue try: module = importlib.import_module(f\"{package}.{module_name}\") for attr_name in dir(module): if is_agent_node(attr_name): describe = getattr(module, \"Describe\") # 排除不需要描述的 node if attr_name not in [\"executor_general_node\"]: AgentDescribe[get_node_name(attr_name)] = describe AgentExecutorNode[get_node_name( attr_name)] = getattr(module, attr_name) except Exception as e: print(f\"加载模块 {module_name} 失败: {e}\") 5、exc_info=True logger.error(f\"Failed to connect to Redis: {e}\", exc_info=True) 6、列表推导式、生成器表达式 # 列表推导式 lst = [x * 2 for x in range(5)] # [0, 2, 4, 6, 8] # 生成器表达式 gen = (x * 2 for x in range(5)) # 生成器对象 "},"notes/python/python_obfuscated_code.html":{"url":"notes/python/python_obfuscated_code.html","title":"Python 代码混淆","keywords":"","body":"Python 代码混淆 1、字符串加密 import base64 # 加密 a=base64.b64encode(b'cat /sys/class/dmi/id/product_uuid'） #解密 b=bytes.decode(base64.b64decode('Y2F0IC9zeXMvY2xhc3MvZG1pL2lkL3Byb2R1Y3RfdXVpZA==')) 2、在线方法，变量混淆 http://pyob.oxyry.com/ 3、nuitka pip install nuitka nuitka3 --module register.py 4、 Cython pip install Cython cat setup.py from distutils.core import setup from Cython.Build import cythonize setup( ext_modules=cythonize(\"hello.py\") ) python setup.py build_ext --inplace 5、变量混淆 pip install pyminifier pyminifier -O register.py 6、python编译 python -m compileall controllers.py "},"notes/python/sqlalchemy.html":{"url":"notes/python/sqlalchemy.html","title":"Python Sqlalchemy","keywords":"","body":"Python Sqlalchemy 一、安装 pip install sqlalchemy 1、安装mysql apt-get install mysql-server apt-get install mysql-client apt-get install libmysqlclient15-dev 2、安装python-mysqldb apt-get install python-mysqldb 3、easy_install wget http://peak.telecommunity.com/dist/ez_setup.py python ez_setup.py 4、MySQL-python easy_install MySQL-Python 5、SQLAlchemy easy_install SQLAlchemy 6、导包测试 vim test.py from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker DB_CONNECT_STRING 连接数据库的路径，mysql+mysqldb 指定使用MySQL-Python来连接，“ooxx”是使用的数据库名（可省略），“charset”指定了连接时使用的字符集（可省略）。 DB_CONNECT_STRING = 'mysql+mysqldb://root:123@localhost/ooxx?charset=utf8' create_engine() 会返回一个数据库引擎，echo 参数为 True 时，会显示每条执行的 SQL 语句，生产环境下可关闭。 engine = create_engine(DB_CONNECT_STRING, echo=True) sessionmaker() 会生成一个数据库会话类。这个类的实例可以当成一个数据库连接，它同时还记录了一些查询的数据，并决定什么时候执行 SQL 语句。由于 SQLAlchemy 自己维护了一个数据库连接池（默认 5 个连接），因此初始化一个会话的开销并不大。对 Tornado 而言，可以在 BaseHandler 的 initialize() 里初始化： DB_Session = sessionmaker(bind=engine) session = DB_Session() 代码如下: session.execute('create database abc') print session.execute('show databases').fetchall() session.execute('use abc') 建 user 表的过程略 print session.execute('select * from user where id = 1').first() print session.execute('select * from user where id = :id', {'id': 1}).first() 定义一个表： from sqlalchemy import Column from sqlalchemy.types import CHAR, Integer, String from sqlalchemy.ext.declarative import declarative_base BaseModel = declarative_base() def init_db(): BaseModel.metadata.create_all(engine) def drop_db(): BaseModel.metadata.drop_all(engine) class User(BaseModel): __tablename__ = 'user' id = Column(Integer, primary_key=True) name = Column(CHAR(30)) # or Column(String(30)) init_db() from sqlalchemy.ext.declarative import declarative_base from sqlalchemy import Column,Integer,String from sqlalchemy.orm import sessionmaker from sqlalchemy import create_engine # DB_CONNECT_STRING = 'mysql_mysqldb://root:0127@localhost/ooxx?charset=utf8' DB_CONNECT_STRING = 'mysql+mysqldb://root:0127@172.18.50.7/test' engine = create_engine(DB_CONNECT_STRING,echo=True) DB_Session = sessionmaker(bind=engine) session = DB_Session() Base = declarative_base() # BaseModel.metadata.create_all(engine) 会找到 BaseModel 的所有子类，并在数据库中建立这些表 class User(Base): # __tablename__ 属性就是数据库中该表的名称 __tablename__ = 'user' id = Column(Integer,primary_key=True) name = Column(String) fullname = Column(String(30)) password = Column(String(30)) # name = Column(CHAR(30)) ed_user = User(name='ll',fullname='lil',password='0829') session.add(ed_user) session.commit() query = session.query(User) 显示SQL 语句 print(query) 显示SQL 语句 print(query.statement) 遍历时查询 for user in query: print(user.name) 返回的是一个类似列表的对象 print(query.all()) print(query.first().name) # 记录不存在时，first() 会返回 None # print(query.one().name) # 不存在，或有多行记录时会抛出异常 print query.filter(User.id == 2).first().name print query.get(2).name # 以主键获取，等效于上句 print query.filter('id = 2').first().name # 支持字符串 query2 = session.query(User.name) print query2.all() # 每行是个元组 print query2.limit(1).all() # 最多返回 1 条记录 print query2.offset(1).all() # 从第 2 条记录开始返回 print query2.order_by(User.name).all() print query2.order_by('name').all() print query2.order_by(User.name.desc()).all() print query2.order_by('name desc').all() print session.query(User.id).order_by(User.name.desc(), User.id).all() print query2.filter(User.id == 1).scalar() # 如果有记录，返回第一条记录的第一个元素 print session.query('id').select_from(User).filter('id = 1').scalar() print query2.filter(User.id > 1, User.name != 'a').scalar() # and query3 = query2.filter(User.id > 1) # 多次拼接的 filter 也是 and query3 = query3.filter(User.name != 'a') print query3.scalar() print query2.filter(or_(User.id == 1, User.id == 2)).all() # or print query2.filter(User.id.in_((1, 2))).all() # in query4 = session.query(User.id) print query4.filter(User.name == None).scalar() print query4.filter('name is null').scalar() print query4.filter(not_(User.name == None)).all() # not print query4.filter(User.name != None).all() print query4.count() print session.query(func.count('*')).select_from(User).scalar() print session.query(func.count('1')).select_from(User).scalar() print session.query(func.count(User.id)).scalar() print session.query(func.count('*')).filter(User.id > 0).scalar() # filter() 中包含 User，因此不需要指定表 print session.query(func.count('*')).filter(User.name == 'a').limit(1).scalar() == 1 # 可以用 limit() 限制 count() 的返回数 print session.query(func.sum(User.id)).scalar() print session.query(func.now()).scalar() # func 后可以跟任意函数名，只要该数据库支持 print session.query(func.current_timestamp()).scalar() print session.query(func.md5(User.name)).filter(User.id == 1).scalar() query.filter(User.id == 1).update({User.name: 'c'}) user = query.get(1) print user.name user.name = 'd' session.flush() # 写数据库，但并不提交 print query.get(1).name session.delete(user) session.flush() print query.get(1) session.rollback() print query.get(1).name query.filter(User.id == 1).delete() session.commit() print query.get(1) 通过Session的query()方法创建一个查询对象。这个函数的参数数量是可变的，参数可以是任何类或者是类的描述的集合。下面是一个迭代输出User类的例子： for instance in session.query(User).order_by(User.id): print instance.name,instance.fullname Query也支持ORM描述作为参数。任何时候，多个类的实体或者是基于列的实体表达都可以作为query()函数的参数，返回类型是元组： for name, fullname in session.query(User.name,User.fullname): print name, fullname 使用关键字变量过滤查询结果，filter 和 filter_by都适用。【2】使用很简单，下面列出几个常用的操作： query.filter(User.name == 'ed') #equals query.filter(User.name != 'ed') #not equals query.filter(User.name.like('%ed%')) #LIKE uery.filter(User.name.in_(['ed','wendy', 'jack'])) #IN query.filter(User.name.in_(session.query(User.name).filter(User.name.like('%ed%'))#IN query.filter(~User.name.in_(['ed','wendy', 'jack']))#not IN query.filter(User.name == None)#is None query.filter(User.name != None)#not None from sqlalchemy import and_ query.filter(and_(User.name =='ed',User.fullname =='Ed Jones')) # and query.filter(User.name == 'ed',User.fullname =='Ed Jones') # and query.filter(User.name == 'ed').filter(User.fullname == 'Ed Jones')# and from sqlalchemy import or_ query.filter(or_(User.name =='ed', User.name =='wendy')) #or query.filter(User.name.match('wendy')) #match "},"notes/python/python_logging.html":{"url":"notes/python/python_logging.html","title":"Python logging模块","keywords":"","body":"Python logging模块 一、介绍 1、日志级别 日志一共分成5个等级，从低到高分别是：NOTSET DEBUG：详细的信息，通常只出现在诊断问题上 INFO：确认一切按预期运行 WARNING：一个迹象表明，一些意想不到的事情发生了，或表明一些问题在不久的将来(例如：磁盘空间低)。这个软件还能按预期工作。 ERROR：更严重的问题，软件没能执行一些功能 CRITICAL：一个严重的错误,这表明程序本身可能无法继续运行 这5个等级，也分别对应5种打日志的方法： debug 、info 、warning 、error 、critical。默认的是WARNING，当在WARNING或之上时才被跟踪。 既要把日志输出到控制台， 还要写入日志文件 这就需要一个叫作Logger 的对象来帮忙，下面将对他进行详细介绍，现在这里先学习怎么实现把日志既要输出到控制台又要输出到文件的功能。 二、使用 1、第一步，创建一个logger import logging logger = logging.getLogger() logger.setLevel(logging.INFO) # Log等级总开关 2、第二步，创建一个handler，用于写入日志文件 logfile = './log/logger.txt' fh = logging.FileHandler(logfile, mode='w') fh.setLevel(logging.DEBUG) # 输出到file的log等级的开关 3、第三步，再创建一个handler，用于输出到控制台 ch = logging.StreamHandler() ch.setLevel(logging.WARNING) # 输出到console的log等级的开关 4、第四步，定义handler的输出格式 formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\") fh.setFormatter(formatter) ch.setFormatter(formatter) 5、第五步，将logger添加到handler里面 logger.addHandler(fh) logger.addHandler(ch)]() 三、日志 logger.debug('this is a logger debug message') logger.info('this is a logger info message') logger.warning('this is a logger warning message') logger.error('this is a logger error message') logger.critical('this is a logger critical message') 日志格式说明 logging.basicConfig函数中，可以指定日志的输出格式format，这个参数可以输出很多有用的信息，如下所示： %(levelno)s: 打印日志级别的数值 %(levelname)s: 打印日志级别名称 %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0] %(filename)s: 打印当前执行程序名 %(funcName)s: 打印日志的当前函数 %(lineno)d: 打印日志的当前行号 %(asctime)s: 打印日志的时间 %(thread)d: 打印线程ID %(threadName)s: 打印线程名称 %(process)d: 打印进程ID %(message)s: 打印日志信息 我在工作中给的常用格式在前面已经看到了。就是： format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s' 1、简单的将日志打印到屏幕 import logging logging.debug('This is debug message') logging.info('This is info message') logging.warning('This is warning message') 屏幕上打印: WARNING:root:This is warning message 2、通过logging.basicConfig函数对日志的输出格式及方式做相关配置 import logging logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s', datefmt='%a, %d %b %Y %H:%M:%S', filename='myapp.log', filemode='w') logging.debug('This is debug message') logging.info('This is info message') logging.warning('This is warning message') ./myapp.log文件中内容为: Sun, 24 May 2009 21:48:54 demo2.py[line:11] DEBUG This is debug message Sun, 24 May 2009 21:48:54 demo2.py[line:12] INFO This is info message Sun, 24 May 2009 21:48:54 demo2.py[line:13] WARNING This is warning message logging.basicConfig函数各参数: filename: 指定日志文件名 filemode: 和file函数意义相同，指定日志文件的打开模式，'w'或'a' format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示: %(levelno)s: 打印日志级别的数值 %(levelname)s: 打印日志级别名称 %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0] %(filename)s: 打印当前执行程序名 %(funcName)s: 打印日志的当前函数 %(lineno)d: 打印日志的当前行号 %(asctime)s: 打印日志的时间 %(thread)d: 打印线程ID %(threadName)s: 打印线程名称 %(process)d: 打印进程ID %(message)s: 打印日志信息 datefmt: 指定时间格式，同time.strftime() level: 设置日志级别，默认为logging.WARNING stream: 指定将日志的输出流，可以指定输出到sys.stderr,sys.stdout或者文件，默认输出到sys.stderr，当stream和filename同时指定时，stream被忽略 3、将日志同时输出到文件和屏幕 import logging logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s', datefmt='%a, %d %b %Y %H:%M:%S', filename='myapp.log', filemode='w') 定义一个StreamHandler，将INFO级别或更高的日志信息打印到标准错误，并将其添加到当前的日志处理对象 console = logging.StreamHandler() console.setLevel(logging.INFO) formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s') console.setFormatter(formatter) logging.getLogger('').addHandler(console) logging.debug('This is debug message') logging.info('This is info message') logging.warning('This is warning message') 屏幕上打印: root : INFO This is info message root : WARNING This is warning message ./myapp.log文件中内容为: Sun, 24 May 2009 21:48:54 demo2.py[line:11] DEBUG This is debug message Sun, 24 May 2009 21:48:54 demo2.py[line:12] INFO This is info message Sun, 24 May 2009 21:48:54 demo2.py[line:13] WARNING This is warning message 4、logging之日志回滚 import logging from logging.handlers import RotatingFileHandler 定义一个RotatingFileHandler，最多备份5个日志文件，每个日志文件最大10M Rthandler = RotatingFileHandler('myapp.log', maxBytes=10*1024*1024,backupCount=5) Rthandler.setLevel(logging.INFO) formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s') Rthandler.setFormatter(formatter) logging.getLogger('').addHandler(Rthandler) 从上例和本例可以看出，logging有一个日志处理的主对象，其它处理方式都是通过addHandler添加进去的。 logging的几种handle方式如下： logging.StreamHandler: 日志输出到流，可以是sys.stderr、sys.stdout或者文件 logging.FileHandler: 日志输出到文件 日志回滚方式，实际使用时用RotatingFileHandler和TimedRotatingFileHandler logging.handlers.BaseRotatingHandler logging.handlers.RotatingFileHandler logging.handlers.TimedRotatingFileHandler logging.handlers.SocketHandler: 远程输出日志到TCP/IP sockets logging.handlers.DatagramHandler: 远程输出日志到UDP sockets logging.handlers.SMTPHandler: 远程输出日志到邮件地址 logging.handlers.SysLogHandler: 日志输出到syslog logging.handlers.NTEventLogHandler: 远程输出日志到Windows NT/2000/XP的事件日志 logging.handlers.MemoryHandler: 日志输出到内存中的制定buffer logging.handlers.HTTPHandler: 通过\"GET\"或\"POST\"远程输出到HTTP服务器 由于StreamHandler和FileHandler是常用的日志处理方式，所以直接包含在logging模块中，而其他方式则包含在logging.handlers模块中， 5、通过logging.config模块配置日志 vim logger.conf [loggers] keys=root,example01,example02 [logger_root] level=DEBUG handlers=hand01,hand02 [logger_example01] handlers=hand01,hand02 qualname=example01 propagate=0 [logger_example02] handlers=hand01,hand03 qualname=example02 propagate=0 [handlers] keys=hand01,hand02,hand03 [handler_hand01] class=StreamHandler level=INFO formatter=form02 args=(sys.stderr,) [handler_hand02] class=FileHandler level=DEBUG formatter=form01 args=('myapp.log', 'a') [handler_hand03] class=handlers.RotatingFileHandler level=INFO formatter=form02 args=('myapp.log', 'a', 10*1024*1024, 5) [formatters] keys=form01,form02 [formatter_form01] format=%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s datefmt=%a, %d %b %Y %H:%M:%S [formatter_form02] format=%(name)-12s: %(levelname)-8s %(message)s datefmt= 上例3： import logging import logging.config logging.config.fileConfig(\"logger.conf\") logger = logging.getLogger(\"example01\") logger.debug('This is debug message') logger.info('This is info message') logger.warning('This is warning message') 上例4： import logging import logging.config logging.config.fileConfig(\"logger.conf\") logger = logging.getLogger(\"example02\") logger.debug('This is debug message') logger.info('This is info message') logger.warning('This is warning message') 6、logging是线程安全的 http://blog.csdn.net/yatere/article/details/6655445 "},"notes/python/python_decorators.html":{"url":"notes/python/python_decorators.html","title":"Python 装饰器","keywords":"","body":"Python 装饰器 装饰器本质上是一个Python函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码并继续重用。 https://www.cnblogs.com/cicaday/p/python-decorator.html 1、python 2.4之前实现，函数添加额外功能 def debug(func): def wrapper(): print \"[DEBUG]: enter {}()\".format(func.__name__) return func() return wrapper def say_hello(): print \"hello!\" say_hello = debug(say_hello) 2、支持@语法糖后 def debug(func): def wrapper(): print \"[DEBUG]: enter {}()\".format(func.__name__) return func() return wrapper @debug def say_hello(): print \"hello!\" 3、被装饰函数传入参数 def debug(func): def wrapper(something): # 指定一毛一样的参数 print \"[DEBUG]: enter {}()\".format(func.__name__) return func(something) return wrapper # 返回包装过函数 @debug def say(something): print \"hello {}!\".format(something) 4、被装饰函数传入可变参数 def debug(func): def wrapper(*args, **kwargs): # 指定宇宙无敌参数 print \"[DEBUG]: enter {}()\".format(func.__name__) print 'Prepare and say...', return func(*args, **kwargs) return wrapper # 返回 @debug def say(something): print \"hello {}!\".format(something) 5、装饰器传入参数 def logging(level): def wrapper(func): def inner_wrapper(*args, **kwargs): print \"[{level}]: enter function {func}()\".format( level=level, func=func.__name__) return func(*args, **kwargs) return inner_wrapper return wrapper @logging(level='INFO') def say(something): print \"say {}!\".format(something) # 如果没有使用@语法，等同于 # say = logging(level='INFO')(say) @logging(level='DEBUG') def do(something): print \"do {}...\".format(something) if __name__ == '__main__': say('hello') do(\"my work\") 6、基于类实现的装饰器，让类的构造函数init()接受一个函数，然后重载call()并返回一个函数，也可以达到装饰器函数的效果。 class logging(object): def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): print \"[DEBUG]: enter function {func}()\".format( func=self.func.__name__) return self.func(*args, **kwargs) @logging def say(something): print \"say {}!\".format(something) 7、带参数的类装饰器 class logging(object): def __init__(self, level='INFO'): self.level = level def __call__(self, func): # 接受函数 def wrapper(*args, **kwargs): print \"[{level}]: enter function {func}()\".format( level=self.level, func=func.__name__) func(*args, **kwargs) return wrapper #返回函数 @logging(level='INFO') def say(something): print \"say {}!\".format(something) 8、装饰器 @property @property def getx(self): return self._x def setx(self, value): self._x = value def delx(self): del self._x x = property(getx, setx, delx, \"I am doc for x property\") @property def x(self): ... # 等同于 def x(self): ... x = property(x) # 例子 def __init__(self): self._role_id = None @property def role_id(self): if not self._role_id: self._role_id = self.keystone.roles.find(name = '_member_').id return self._role_id def _grant_role(self,openstack_user): self.keystone.roles.grant( self.role_id, user = openstack_user.id, project = self.default_project) 9、wraps 作用 # wraps作用 https://blog.csdn.net/hqzxsc2006/article/details/50337865 Python装饰器（decorator）在实现的时候，被装饰后的函数其实已经是另外一个函数了（函数名等函数属性会发生改变），为了不影响，Python的functools包中提供了一个叫wraps的decorator来消除这样的副作用。写一个decorator的时候，最好在实现之前加上functools的wrap，它能保留原有函数的名称和docstring。 from functools import wraps def my_decorator(func): @wraps(func) def wrapper(*args, **kwargs): '''decorator''' print('Calling decorated function...') return func(*args, **kwargs) return wrapper @my_decorator def example(): \"\"\"Docstring\"\"\" print('Called example function') print(example.__name__, example.__doc__) # 不使用wraps结果 ('wrapper', 'decorator') # 使用wraps结果 ('example', 'Docstring') # 例子 from functools import wraps def logit(func): @wraps(func) def with_logging(*args, **kwargs): print(func.__name__ + \" was called\") return func(*args, **kwargs) return with_logging @logit def addition_func(x): \"\"\"Do some math.\"\"\" return x + x result = addition_func(4) 10、装饰器 @staticmethod 返回函数的静态方法,无需实例化就可以使用。 class C(object): @staticmethod def f(): print('runoob'); C.f(); # 静态方法无需实例化 cobj = C() cobj.f() # 也可以实例化后调用 11、装饰器 @classmethod classmethod 修饰符对应的函数不需要实例化，不需要 self 参数，但第一个参数需要是表示自身类的 cls 参数，可以来调用类的属性，类的方法，实例化对象等。 class A(object): bar = 1 def func1(self): print ('foo') @classmethod def func2(cls): print ('func2') print (cls.bar) cls().func1() # 调用 foo 方法 A.func2() # 不需要实例化 12、闭包 在一些语言中，在函数中可以（嵌套）定义另一个函数时，如果内部的函数引用了外部的函数的变量，则可能产生闭包。闭包可以用来在一个函数与一组“私有”变量之间创建关联关系。在给定函数被多次调用的过程中，这些私有变量能够保持其持久性。 用比较容易懂的人话说，就是当某个函数被当成对象返回时，夹带了外部变量，就形成了一个闭包。 def make_printer(msg): def printer(): print msg # 夹带私货（外部变量） return printer # 返回的是函数，带私货的函数 printer = make_printer('Foo!') printer() https://www.cnblogs.com/cicaday/p/python-decorator.html "},"notes/python/python_file.html":{"url":"notes/python/python_file.html","title":"Python 文件操作","keywords":"","body":"Python 文件操作 open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) 1、参数说明 file: 必需，文件路径（相对或者绝对路径）。 mode: 可选，文件打开模式 buffering: 设置缓冲 encoding: 一般使用utf8 errors: 报错级别 newline: 区分换行符 closefd: 传入的file参数类型 opener: 2、mode参数 模式 描述 t 文本模式 (默认)。 x 写模式，新建一个文件，如果该文件已存在则会报错。 b 二进制模式。 + 打开一个文件进行更新(可读可写)。 U 通用换行模式（Python 3 不支持）。 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 3、常用方法 序号 方法 描述 1 file.close() 关闭文件。关闭后文件不能再进行读写操作。 2 file.flush() 刷新文件内部缓冲，直接把内部缓冲区的数据立刻写入文件, 而不是被动的等待输出缓冲区写入。 3 file.fileno() 返回一个整型的文件描述符(file descriptor FD 整型), 可以用在如os模块的read方法等一些底层操作上。 4 file.isatty() 如果文件连接到一个终端设备返回 True，否则返回 False。 5 file.next() Python 3 中的 File 对象不支持 next() 方法。返回文件下一行。 6 file.read([size]) 从文件读取指定的字节数，如果未给定或为负则读取所有。 7 file.readline([size]) 读取整行，包括 \"\\n\" 字符。 8 file.readlines([sizeint]) 读取所有行并返回列表，若给定sizeint>0，返回总和大约为sizeint字节的行, 实际读取值可能比 sizeint 较大, 因为需要填充缓冲区。 9 file.seek(offset[, whence]) 移动文件读取指针到指定位置 10 file.tell() 返回文件当前位置。 11 file.truncate([size]) 从文件的首行首字符开始截断，截断文件为 size 个字符，无 size 表示从当前位置截断；截断之后后面的所有字符被删除，其中 Widnows 系统下的换行代表2个字符大小。 12 file.write(str) 将字符串写入文件，返回的是写入的字符长度。 13 file.writelines(sequence) 向文件写入一个序列字符串列表，如果需要换行则要自己加入每行的换行符。 f.seek(-3, 2) # 移动到文件倒数第三个字节 f.seek(5) # 移动到文件的第六个字节 with open(r'./README.md', \"a+\") as f: print(f.readlines()) 4、其他 模式 可做操作 若文件不存在 是否覆盖 r 只能读 报错 - r+ 可读可写 报错 是 w 只能写 创建 是 w+ 可读可写 创建 是 a 只能写 创建 否，追加写 a+ 可读可写 创建 否，追加写 "},"notes/python/scrapy.html":{"url":"notes/python/scrapy.html","title":"scrapy 爬虫","keywords":"","body":"scrapy 爬虫 scrapy官网 scrapy.cfg: 项目的配置文件 tutorial/: 该项目的python模块。之后您将在此加入代码。 tutorial/items.py: 项目中的item文件. tutorial/pipelines.py: 项目中的pipelines文件. tutorial/settings.py: 项目的设置文件. tutorial/spiders/: 放置spider代码的目录. Item 是保存爬取到的数据的容器 Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类 为了创建一个Spider，您必须继承 scrapy.Spider 类， 且定义以下三个属性: name: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。 start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。 parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。 Selector有四个基本的方法(点击相应的方法可以看到详细的API文档): xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。 css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表。 extract(): 序列化该节点为unicode字符串并返回list。 re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。 一、基本使用 1、安装 pip install scrapy 2、查看所有命令 scrapy genspider -l 3、查看模板命令 scrapy genspider -d 模板名称 4、展示爬虫应用列表 scrapy list 5、创建scrapy项目 scrapy startproject yani cd yani/ 6、生成 spider scrapy genspider hotel \"https://hotels.ctrip.com/hotel/\" 7、运行单独爬虫应用 scrapy crawl 爬虫应用名称 scrapy crawl hotel scrapy crawl hotel -o output.json 8、示例代码 https://github.com/hlyani/myscrapy git clone https://github.com/hlyani/myscrapy.git 9、scrapy shell scrapy shell 'http://scrapy.org' --nolog response.xpath('//title/text()').re('(\\w+):') 二、节点匹配相关 1、xpath xpath 2、正则 response.xpath(\"//span[contains(@class, 'bookmark-btn')]/text()\").re('.*?(\\d+).*') response.css(\".bookmark-btn::text\").re('.*?(\\d+).*') response.xpath('//div[@class=\"quote\"]/span[@class=\"text\"]/text()').re(\"Harry, (\\w.*)far more than\") 3、css response.css(\"a::attr(href)\") response.css(\".page-navigator a::attr(href)\").extract() response.css(\".post-content img::attr(src)\").extract() response.css(\"p::text\").extract() response.css(\"title::text\").extract() response.css(\"div::text\").extract() response.css(\".center::text\").extract() response.css(\".quote span::text\").getall() response.css(\".quote span::text\").getall() response.xpath('//small[@class=\"author\"]/text()').getall() response.xpath('//div[@class=\"quote\"]/span[@class=\"text\"]/text()').re(\"Harry, (\\w.*)far more than\") "},"notes/python/urllib2.html":{"url":"notes/python/urllib2.html","title":"urllib2 爬虫","keywords":"","body":"urllib2 爬虫 import urllib2 import re import os url = r'https://movie.douban.com/cinema/nowplaying/chengdu/' req = urllib2.Request(url, headers={'User-Agent': 'Magic Browser'}) webpage = urllib2.urlopen(req) html = webpage.read() # html.decode('utf-8') re_general = r'([\\s\\S]*?)' general = re.findall(re_general, html)[0] tg_start = 0 tg_end = 0 names = [] rates = [] tg_start = general.count('') for i in range(34): tg_start = general.find('') # if tg_start == -1: # print(\"not find start tag\") # os.exit() tmp = general[tg_start:-1] general = tmp[len(''):-1] tg_end = tmp.find(\"\") # if tg_end == -1: # print(\"not find end tag\") # os.exit() tmp = tmp[len(''):tg_end] re_name = r'\"title\">([\\s\\S]*?)' re_rate = r'\"subject-rate\">([\\s\\S]*?)' re_no_rate = r'\"text-tip\">([\\s\\S]*?)' name = re.findall(re_name, tmp)[0].strip() # rate = re.findall(re_rate, tmp)[0].strip() if re.findall( # re_rate, tmp) else re.findall(re_no_rate, tmp)[0].strip() rate = re.findall(re_rate, tmp)[0].strip() if re.findall( re_rate, tmp) else \"0\" names.append(name) rates.append(float(rate)) print(name + \":\" + rate) print(names) print(rates) def get_avarage(num_list): nsum = 0 for i in range(len(num_list)): nsum += i return nsum/len(num_list) film_num = len(names) max_rate = max(rates index_max_rate = rates.index(max_rate) avarage_rate = get_avarage(rates) print(\"film sum: %d\" % film_num) print(\"max rate: %f,film name is: %s\" % (max_rate, names[index_max_rate])) print(\"avarage rate: %.2f\" % avarage_rate) "},"notes/python/yield.html":{"url":"notes/python/yield.html","title":"yield","keywords":"","body":"yield def fab(max): n, a, b, ret = 0, 0, 1,[0] while n class Fab(object): def __init__(self, max): self.max = max self.n, self.a, self.b = 0, 0, 1 def __iter__(self): return self def next(self): if self.n def fab(max): n, a, b = 0, 0, 1 while n def yani(): i = 1 print('aaa') return i print('bbb') ret = yani() print(ret) def fab(max): n, a, b = 0, 0, 1 L = [] while n class Fab(object): def __init__(self, max): self.max = max self.n, self.a, self.b = 0, 0, 1 def __iter__(self): return self def next(self): if self.n def fab(max): n, a, b = 0, 0, 1 while n "},"notes/python/jinja2.html":{"url":"notes/python/jinja2.html","title":"Jinja2 相关","keywords":"","body":"Jinja2 相关 jinja2 是 Flask 作者开发的一个模板系统 jinja2 官网 一、安装 1、安装jinja2 pip install jinja2 2、测试是否安装成功 python -c \"import jinja2\" 二、基本语法 控制结构 ​ {% %} 变量取值 ​ {{ }} 注释 ​ {# #} 例子： {# This is jinja code {% for file in filenames %} ... {% endfor %} #} 可以看到，for循环的使用方式和Python比较类似，但是没有了句尾的冒号，另外需要使用endfor最为结尾，其实在jinja2中，if也是一样的，结尾需要使用endif。 1、变量 this is a dicectory:{{ mydict['key'] }} this is a list:{{ mylist[3] }} this is a object:{{ myobject.something() }} 2、过滤器 变量可以通过“过滤器”进行修改，过滤器可以理解为是jinja2里面的内置函数和字符串处理函数。 常用过滤器 说明 safe 渲染时值不转义 capitialize 把值的首字母转换成大写，其他子母转换为小写 lower 把值转换成小写形式 upper 把值转换成大写形式 title 把值中每个单词的首字母都转换成大写 trim 把值的首尾空格去掉 striptags 渲染之前把值中所有的HTML标签都删掉 join 拼接多个值为字符串 replace 替换字符串的值 round 默认对数字进行四舍五入，也可以用参数进行控制 int 把值转换成整型 {{ 'abc' | captialize }} # Abc {{ 'abc' | upper }} # ABC {{ 'hello world' | title }} # Hello World {{ \"hello world\" | replace('world','daxin') | upper }} # HELLO DAXIN {{ 18.18 | round | int }} # 18 3、控制结构 {% if daxin.safe %} daxin is safe. {% elif daxin.dead %} daxin is dead {% else %} daxin is okay {% endif %} 4、for 循环 {% block title %}{% endblock %} {% for user in users %} {{ user.username }} {% endfor %} 循环生成 {% for user in users %} {{ user.username|title }} {% endfor %} 迭代字典 {% for key, value in my_dict.iteritems() %} {{ key }} {{ value}} {% endfor %} 在for循环中，jinja2还提供了一些特殊的变量，用以来获取当前的遍历状态 变量 描述 loop.index 当前迭代的索引（从1开始） loop.index0 当前迭代的索引（从0开始） loop.first 是否是第一次迭代，返回bool loop.last 是否是最后一次迭代，返回bool loop.length 序列中的项目数量 loop.revindex 到循环结束的次数（从1开始） loop.revindex0 到循环结束的次数(从0开始） 5、继承和super函数 base.html {% block head %} {% block title %}{% endblock %} - My Webpage {% endblock %} {% block content %}{% endblock %} {% block footer %} This is javascript code {% endblock %} 使用继承， 这里定义了四处 block，即：head，title，content，footer。那怎么进行继承和变量替换呢？注意看下面的文件 {% extend \"base.html\" %} # 继承base.html文件 {% block title %} Dachenzi {% endblock %} # 定制title部分的内容 {% block head %} {{ super() }} # 用于获取原有的信息 .important { color: #FFFFFF } {% endblock %} # 其他不修改的原封不动的继承 三、基本使用方法 >>> from jinja2 import Template >>> template = Template('Hello {{ name }}!') >>> template.render(name='John Doe') u'Hello John Doe!' 大多数应用都在初始化的时候撞见一个Environment对象，并用它加载模板。Environment支持两种加载方式： PackageLoader：包加载器 FileSystemLoader：文件系统加载器 from jinja2 import PackageLoader,Environment env = Environment(loader=PackageLoader('python_project','templates')) # 创建一个包加载器对象 template = env.get_template('bast.html') # 获取一个模板文件 template.render(name='daxin',age=18) # 渲染 PackageLoader()的两个参数为：python包的名称，以及模板目录名称。 get_template()：获取模板目录下的某个具体文件。 render()：接受变量，对模板进行渲染 "},"notes/python/python_code_standards.html":{"url":"notes/python/python_code_standards.html","title":"Python 代码规范","keywords":"","body":"Python 代码规范 一、编码 如无特殊情况，文件一律使用 UTF-8 编码。 如无特殊情况，文件头部必须加入 # coding: utf-8 标识。 二、代码格式 1、缩进 统一使用4个空格进行缩进 2、行宽 每行代码尽量不超过80个字符，特殊情况下可以略微超过80，但最长不超过120。 理由： 方便查看 side-by-side 的 diff 方便在控制台查看代码 太长可能是设计缺陷 3、引号 自然语言使用双引号，机器标识使用单引号，因此代码里多数应该使用单引号。 自然语言使用双引号\"\"。例如：错误信息；unicode 信息，u\"你好世界\"。 机器标识使用单引号''，例如：dict 里的 key。 正则表达式使用原生的双引号 r\"...\"。 文档字符串（docstring）使用三引号\"\"\"...\"\"\"。 4、空行 模块级函数和类定义之间空两行。 类成员函数之间空一行。 class A: def __init__(self): pass def hello(self): pass def main(): pass 可以使用多个空格分隔多组相关的函数。 函数中可以使用空行分隔出逻辑相关的代码。 python 支持括号内的换行，这时有两种情况 1）、第二行缩进到括号的起始处 foo = long_function_name(var_one, var_two, var_three, var_four) 2）、第二行缩进4个空格，适用于起始括号就换行的情形 def long_function_name( var_one, var_two, var_three, var_four): print(var_one) 使用反斜杠\\换行，二元运算符+ .等应出现在行末；长字符串也可以用此法换行。 session.query(MyTable).\\ filter_by(id=1).\\ one() print 'Hello, '\\ '%s %s!' %\\ ('Harry', 'Potter') 禁止复合语句，即一行中包含多个语句： # 正确的写法 do_first() do_second() do_third() # 不推荐的写法 do_first();do_second();do_third(); if/for/while 一定要换行： # 正确的写法 if foo == 'blah': do_blah_thing() # 不推荐的写法 if foo == 'blah': do_blash_thing() 5、import import 语句应该分行书写 # 正确的写法 import os import sys # 不推荐的写法 import sys,os # 正确的写法 from subprocess import Popen, PIPE import 语句应该使用 absolute import # 正确的写法 from foo.bar import Bar # 不推荐的写法 from ..bar import Bar import 语句应该放在文件头部，置于模块说明及 docstring 之后，于全局变量之前 import 语句应该按照顺序排列，每组之间用一个空行分隔 import os import sys import msgpack import zmq import foo 导入其他模块的类定义时，可以使用相对导入 from myclass import MyClass 如果发生命名冲突，则可以使用命名空间 import bar import foo.bar bar.Bar() foo.bar.Bar() 6、空格 在二元运算符两边各空一格[=, -, +=, ==, >, in, is not, and]: # 正确的写法 i = i + 1 submitted += 1 x = x * 2 - 1 hypot2 = x * x + y * y c = (a + b) * (a - b) # 不推荐的写法 i=i+1 submitted +=1 x = x*2 - 1 hypot2 = x*x + y*y c = (a+b) * (a-b) 函数的参数列表中，默认值等号两边不需要添加空格 # 正确的写法 def complex(real, imag=0.0): pass # 不推荐的写法 def complex(real, imag = 0.0): pass 左括号之后，右括号之前不需要加多余的空格 # 正确的写法 spam(ham[1], {eggs: 2}) # 不推荐的写法 spam( ham[1], { eggs : 2 } ) 字典对象的左括号之前不要多余的空格 # 正确的写法 dict['key'] = list[index] # 不推荐的写法 dict ['key'] = list [index] 不要为对齐赋值语句而使用的额外空格 # 正确的写法 x = 1 y = 2 long_variable = 3 # 不推荐的写法 x = 1 y = 2 long_variable = 3 7、注释 在代码的关键部分(或比较复杂的地方)，能写注释的要尽量写注释。 比较重要的注释段，使用多个等号隔开，可以更加醒目，突出重要性。 app = create_app(name, options) # ===================================== # 请勿在此处添加 get post等app路由行为 !!! # ===================================== if __name__ == '__main__': app.run() 1）、块注释 “#” 号后空一格，段落间用空行分开（同样需要 \"#\" 号） # 块注释 # 块注释 # # 块注释 # 块注释 2）、行注释 至少使用两个空格和语句分开，注意不要使用无意义的注释 # 正确的写法 x = x + 1 # 边框加粗一个像素 # 不推荐的写法(无意义的注释) x = x + 1 # x加1 8、docstring 1）、所有的公共模板、函数、类、方法，都应该写 docstring。私有方法不一定需要，但应该在 def 后提供一个块注释来说明。 2）、docstring 的结束“”“应该独占一行，除非此 docstring 只有一行 \"\"\"Return a foobar Optional plotz says to frobnicate the bizbaz first. \"\"\" \"\"\"Oneline docstring\"\"\" 作为文档的Docstring一般出现在模块头部、函数和类的头部，这样在python中可以通过对象的doc对象获取文档。编辑器和IDE也可以根据Docstring给出自动提示。 文档注释以 \"\"\" 开头和结尾，首行不换行，如有多行，末行必需换行，以下是Google的docstring风格示例： # -*- coding: utf-8 -*- \"\"\"Example docstrings. This module demonstrates documentation as specified by the `Google Python Style Guide`_. Docstrings may extend over multiple lines. Sections are created with a section header and a colon followed by a block of indented text. Example: Examples can be given using either the ``Example`` or ``Examples`` sections. Sections support any reStructuredText formatting, including literal blocks:: $ python example_google.py Section breaks are created by resuming unindented text. Section breaks are also implicitly created anytime a new section starts. \"\"\" 不要在文档注释复制函数定义原型，而是具体描述其具体内容，解释具体参数和返回值等： # 不推荐的写法(不要写函数原型等废话) def function(a, b): \"\"\"function(a, b) -> list\"\"\" ... ... # 正确的写法 def function(a, b): \"\"\"计算并返回a到b范围内数据的平均值\"\"\" ... ... 对函数参数、返回值等的说明采用numpy标准，如下所示： def func(arg1, arg2): \"\"\"在这里写函数的一句话总结(如: 计算平均值). 这里是具体描述. 参数 ---------- arg1 : int arg1的具体描述 arg2 : int arg2的具体描述 返回值 ------- int 返回值的具体描述 参看 -------- otherfunc : 其它关联函数等... 示例 -------- 示例使用doctest格式, 在`>>>`后的代码可以被文档测试工具作为测试用例自动运行 >>> a=[1,2,3] >>> print [x + 3 for x in a] [4, 5, 6] \"\"\" 文档注释不限于中英文，但不要中英文混用。 文档注释不是越长越好，通常一两句话能把情况说清楚即可。 模块、公有类、公有方法，能写文档注释的，应该尽量写文档注释。 9、命名规范 应避免使用小写字母 l(L)，大写字母 O(o)，大写字母 I(i )单独作为一个变量的名称，以区分数字1和0 包和模块使用全小写命名，尽量不使用下划线 类名使用 CamelCase (即驼峰风格)命名风格。内部类可以用一个下划线开头 函数使用下划线分隔的小写命名 当参数名称和 Python 保留字冲突时，可在最后添加一个下划线，而不是使用缩写或自造词 常量使用以下划线分隔的大写命名 MAX_OVERFLOW = 100 Class FooBar: def foo_bar(self, print_): print(print_) "},"notes/bigdata/bigdata_deploy.html":{"url":"notes/bigdata/bigdata_deploy.html","title":"大数据相关环境安装","keywords":"","body":"大数据相关环境安装 一、基础准备 1、关闭防火墙等（所有虚拟机都执行） 1）、关闭 firewalld systemctl stop firewalld systemctl disable firewalld systemctl status firewalld 2)、关闭 selinux getenforce setenforce 0 vim /etc/selinux/config SELINUX=disabled # 重启虚拟机 reboot 2、修改 hostname 和 hosts（所有虚拟机都执行） 1）、vim /etc/hostname hp1 2）、使 hostname 生效 重启虚拟机 reboot 或执行以下命令，然后重新登录 hostnamectl set-hostname hp1 3）、vim /etc/hosts 192.168.1.11 hp1 192.168.1.12 hp2 192.168.1.13 hp3 3、设置免密 1）、生成秘钥对 ssh-keygen 2）、将公钥拷贝到其他节点实现免密 ssh-copy-id hp1 ssh-copy-id hp2 ssh-copy-id hp3 4、安装 mysql 数据库 1）、安装 mariadb-server yum install -y mariadb-server 2）、开机启用、启动 mariadb、查看状态 systemctl enable mariadb systemctl start mariadb systemctl status mariadb 3）、初始化 mysql 数据库 mysql_secure_installation Enter current password for root (enter for none): (enter) Change the root password? [Y/n] (y) New password: (qwe) Re-enter new password: (qwe) Remove anonymous users? [Y/n] (n) Disallow root login remotely? [Y/n] (n) Remove test database and access to it? [Y/n] (y) Reload privilege tables now? [Y/n] (y) 4）、连接测试 mysql -uroot -pqwe 二、开始安装 1、解压相关软件包 mkdir -p /usr/local/src tar -zxvf hadoop-2.6.0.tar.gz -C /usr/local/src mv /usr/local/src/hadoop-2.6.0 /usr/local/src/hadoop tar -zxvf apache-hive-1.1.0-bin.tar.gz -C /usr/local/src mv /usr/local/src/apache-hive-1.1.0-bin /usr/local/src/hive tar -zxvf hbase-1.2.0-bin.tar.gz -C /usr/local/src mv /usr/local/src/hbase-1.2.0 /usr/local/src/hbase tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /usr/local/src mv /usr/local/src/sqoop-1.4.7.bin__hadoop-2.6.0 /usr/local/src/sqoop tar -zxvf jdk1.8.0_111.tar.gz -C /usr/local/src mv /usr/local/src/jdk1.8.0_111 /usr/local/src/jdk tar -zxvf zookeeper-3.4.5.tar.gz -C /usr/local/src mv /usr/local/src/zookeeper-3.4.5 /usr/local/src/zookeeper 2、修改环境变量 1)、vim /etc/profile （在文件后面追加） export JAVA_HOME=/usr/local/src/jdk export HADOOP_HOME=/usr/local/src/hadoop export ZOOKEEPER_HOME=/usr/local/src/zookeeper export HBASE_HOME=/usr/local/src/hbase export SQOOP_HOME=/usr/local/src/sqoop export HIVE=/usr/local/src/hive export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$HBASE_HOME/bin:$SQOOP_HOME/bin:$HIVE/bin 使环境变量生效 source /etc/profile 注意：别漏掉PATH=$PATH，否则 linux 环境会出问题 scp -r /etc/profile hp2:/etc scp -r /etc/profile hp3:/etc # 分别使环境变量生效 source /etc/profile 3、配置 zookeeper 1）、进入配置文件目录 cd /usr/local/src/zookeeper/conf 2）、复制配置文件 cp zoo_sample.cfg zoo.cfg 3）、修改配置文件，（vim zoo.cfg） dataDir 数据存放路径 tickTime=2000 clientPort=2181 initLimit=5 syncLimit=2 dataDir=/usr/local/src/zookeeper/data server.0=hp1:2888:3888 server.1=hp2:2888:3888 server.2=hp3:2888:3888 接下来分别在 hp1、hp2、hp3 节点上的/usr/local/src/zookeeper/data 目录下创建一 个名为 myid 的文件，并在hp1节点上的 myid 文件里输入 0，在hp2节点上的myid输入 1，在hp3节点上的 myid 文件里输入 2 mkdir /usr/local/src/zookeeper/data echo 0 >> /usr/local/src/zookeeper/data/myid 4)、拷贝jdk、 zookeeper 配置到所有其他节点 scp -r /usr/local/src/jdk hp2:/usr/local/src/ scp -r /usr/local/src/jdk hp3:/usr/local/src/ scp -r /usr/local/src/zookeeper hp2:/usr/local/src/ scp -r /usr/local/src/zookeeper hp3:/usr/local/src/ 5）、每个节点启动zkserver zkServer.sh start [root@hp1 ~]# jps 12427 Jps 12380 QuorumPeerMain 验证 ZooKeeper 服务，三台节点必须是 1 个 leader 2 个 follower 的状态才算配置正确 zkServer.sh status [root@hp2 ~]# zkServer.sh status JMX enabled by default Using config: /usr/local/src/zookeeper/bin/../conf/zoo.cfg Mode: leader [root@hp1 ~]# zkServer.sh status JMX enabled by default Using config: /usr/local/src/zookeeper/bin/../conf/zoo.cfg Mode: follower [root@hp3 ~]# zkServer.sh status JMX enabled by default Using config: /usr/local/src/zookeeper/bin/../conf/zoo.cfg Mode: follower 4、安装 hadoop 1）、进入配置文件目录 cd /usr/local/src/hadoop/etc/hadoop 2）、编辑 hadoop-env.sh，添加以下内容。（vim hadoop-env.sh） export JAVA_HOME=/usr/local/src/jdk export HADOOP_HOME=/usr/local/src/hadoop 3）、编辑 slaves，将 slave 的主机名写入到该文件中。（vim slaves） hp3 4）、编辑 core-site.xml，（vim core-site.xml） fs.defaultFS hdfs://ns hadoop.tmp.dir file:/usr/local/src/hadoop/tmp ha.zookeeper.quorum hp1:2181,hp2:2181,hp3:2181 5）、编辑 hdfs-site.xml，（vim hdfs-site.xml） dfs.nameservices ns dfs.ha.namenodes.ns nn1,nn2 dfs.namenode.rpc-address.ns.nn1 hp1:9000 dfs.namenode.http-address.ns.nn1 hp1:50070 dfs.namenode.rpc-address.ns.nn2 hp2:9000 dfs.namenode.http-address.ns.nn2 hp2:50070 dfs.namenode.shared.edits.dir qjournal://hp1:8485;hp2:8485;hp3:8485/ns dfs.journalnode.edits.dir /usr/local/src/hadoop/hdfs/journal dfs.ha.automatic-failover.enabled true dfs.client.failover.proxy.provider.ns org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider dfs.ha.fencing.methods sshfence shell(/bin/true) dfs.ha.fencing.ssh.private-key-files /root/.ssh/id_rsa dfs.replication 1 dfs.namenode.name.dir file:/usr/local/src/hadoop/dfs/name dfs.datanode.name.dir file:/usr/local/src/hadoop/dfs/data 6）、复制一份 mapred-site.xml 配置文件. cp mapred-site.xml.template mapred-site.xml 编辑配置文件 mapred-site.xml，（vim mapred-site.xml） mapreduce.framework.name yarn 7、）编辑 yarn-site 配置文件，（vim yarn-site.xml） yarn.resourcemanager.ha.enabled true yarn.resourcemanager.cluster-id yrc yarn.resourcemanager.ha.rm-ids rm1,rm2 yarn.resourcemanager.hostname.rm1 hp1 yarn.resourcemanager.hostname.rm2 hp2 yarn.resourcemanager.zk-address hp1:2181,hp2:2181,hp3:2181 yarn.nodemanager.aux-services mapreduce_shuffle 8、）拷贝配置到所有其他节点 scp -r /usr/local/src/hadoop hp2:/usr/local/src/ scp -r /usr/local/src/hadoop hp3:/usr/local/src/ 注意：hp2、hp3 的 /usr/local/src/ 目录不存在时，需先创建 9)、HA 模式启动服务 1、首先启动各个节点的 Zookeeper，在各个节点执行以下命令 zkServer.sh start [root@hp1 ~]# jps 2147 QuorumPeerMain 2277 Jps 2、在各个节点启动 journalnode 服务，在各个节点执行以下命令 hadoop-daemon.sh start journalnode [root@hp1 ~]# jps 2336 Jps 2147 QuorumPeerMain 2301 JournalNode 3、在主 namenode 节点（hp1）格式化 namenode 和 journalnode 目录 hdfs namenode -format 19/11/05 09:30:11 INFO common.Storage: Storage directory /usr/local/src/hadoop/dfs/name has been successfully formatted. 19/11/05 09:30:13 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0 19/11/05 09:30:13 INFO util.ExitUtil: Exiting with status 0 19/11/05 09:30:13 INFO namenode.NameNode: SHUTDOWN_MSG: 成功的话，会看到 \"successfully formatted\" 或 \"Exitting with status 0\" 的提示，若为 \"Exitting with status 1\" 则是出错。 4、在某一个 namenode 节点（hp1）执行如下命令，创建命名空间 hdfs zkfc -formatZK Proceed formatting /hadoop-ha/ns? (Y or N) 19/11/05 09:31:16 INFO ha.ActiveStandbyElector: Session connected. y 19/11/05 09:31:46 INFO ha.ActiveStandbyElector: Recursively deleting /hadoop-ha/ns from ZK... 19/11/05 09:31:47 INFO ha.ActiveStandbyElector: Successfully deleted /hadoop-ha/ns from ZK. 19/11/05 09:31:47 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ns in ZK. 5、在主 namenode 节点（hp1）启动 namenode 进程 hadoop-daemon.sh start namenode [root@hp1 ~]# hadoop-daemon.sh start namenode starting namenode, logging to /usr/local/src/hadoop/logs/hadoop-root-namenode-hp1.out [root@hp1 ~]# jps 2147 QuorumPeerMain 2500 NameNode 2596 JournalNode 2630 Jps 6、在备 namenode 节点 （hp2）执行第一行命令，这个是把备 namenode 节点的目录格式化并把元数据从主 namenode 节点 copy 过来，并且这个命令不会把 journalnode 目录再格式化了！然后用第二个命令启动备 namenode 进程！ hdfs namenode -bootstrapStandby # 19/11/05 09:34:46 INFO util.ExitUtil: Exiting with status 0 hadoop-daemon.sh start namenode [root@hp2 ~]# jps 1619 JournalNode 1749 NameNode 1785 Jps 1533 QuorumPeerMain 7、在两个 namenode 节点（hp1,hp2）都执行以下命令 hadoop-daemon.sh start zkfc 2147 QuorumPeerMain 2500 NameNode 2596 JournalNode 2681 DFSZKFailoverController 2734 Jps 8、在所有 datanode 节点都执行以下命令启动 datanode hadoop-daemon.sh start datanode 1665 DataNode 1733 Jps 1514 QuorumPeerMain 1595 JournalNode 9、启动 yarn (hp1) start-yarn.sh 10、查看 namenode 状态 hdfs haadmin -getServiceState nn1 hdfs haadmin -getServiceState nn2 11、查看集群状态 如果 Live datanode 不为 0，则说明集群启动成功 hdfs dfsadmin -report 12、web 页面访问 hadoop hp1 为对应的ip 地址 hadoop 地址 http://hp1:50070 yarn 地址 http://hp1:8088 hbase 地址 http://hp1:16010 5、hadoop 使用测试 mkdir input echo \"hello world\">./input/test1.txt echo \"hello hadoop\">./input/test2.txt hdfs dfs -mkdir /in hdfs dfs -put input/ /in hadoop jar /usr/local/src/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /in/input /out hdfs dfs -cat /out/* 6、配置sqoop 1）、进入配置文件目录 cd /usr/local/src/sqoop/conf 2）、复制环境变量 sqoop-env.sh 文件 cp sqoop-env-template.sh sqoop-env.sh 3）、修改配置文件 （vim sqoop-env.sh ） 没用到 Hive 和 HBase 可以不用配置相关项，使用时会弹出警告 export HADOOP_COMMON_HOME=/usr/local/src/hadoop export HADOOP_MAPRED_HOME=/usr/local/src/hadoop export HIVE_HOME=/usr/local/src/hive export ZOOKEEPER_HOME=/usr/local/src/zookeeper export ZOOCFGDIR=/usr/local/src/zookeeper/conf export HBASE_HOME=/usr/local/src/hbase 4）、 复制 mysql 数据库驱动包到指定文件路径 cp mysql-connector-java-5.1.46-bin.jar /usr/local/src/sqoop/lib/ 5）、验证 sqoop version 6）、链接测试 sqoop list-databases --connect jdbc:mysql://hp1:3306/ --username root --password qwe 7）、将 mysql 中数据导入到 hdfs 中 mysql -uroot -pqwe create database hotels; use hotels; CREATE TABLE IF NOT EXISTS `hotel`( `id` INT UNSIGNED AUTO_INCREMENT, `name` VARCHAR(255) NOT NULL, `diamond` VARCHAR(255) NOT NULL, `last_order` VARCHAR(255) NOT NULL, `address` VARCHAR(255) NOT NULL, `score` VARCHAR(255) NOT NULL, `level` VARCHAR(255) NOT NULL, `recommend` VARCHAR(255) NOT NULL, `commend_people` VARCHAR(255) NOT NULL, `commend` VARCHAR(255) NOT NULL, PRIMARY KEY ( `id` ) )ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into hotel (name,diamond,last_order,address,score,level,recommend,commend_people,commend) values ('长沙华晨豪生大酒店','豪华型','最新预订：2分钟前','雨花区万家丽中路二段8号(长沙大道与万家丽中路交叉处、近高桥居然之家)。','4.7','很好','98%','903','早餐丰富') sqoop import \\ --connect jdbc:mysql://hp1:3306/hotels \\ --username root \\ --password qwe \\ --table hotel \\ -m 1 7、配置hbase 1）、进入配置文件目录 cd /usr/local/src/hbase/conf 2）、编辑（vim hbase-env.sh） export JAVA_HOME=/usr/local/src/jdk 3）、编辑 （vim hbase-site.xml） hbase.rootdir hdfs://hp1:9000/hbase hbase.cluster.distributed true hbase.zookeeper.quorum hp1,hp2,hp3 hbase.zookeeper.property.dataDir /usr/local/src/zookeeper/data/ 4）、编辑 （vim regionservers） hp1 hp2 hp3 5）、将hbase所有配置拷贝到其他所有节点 scp -r /usr/local/src/hbase/ hp2:/usr/local/src/ scp -r /usr/local/src/hbase/ hp3:/usr/local/src/ 6）、启动 hbase 启动 HBase 时需要确保 hdfs 已经启动，使用命令 hdfs dfsadmin -report 查看以下 HDFS 集群是否正常 如果正常，在 master 节点上执行以下命令启动 HBase 集群: start-hbase.sh 3924 HMaster 4038 HRegionServer 7）、测试 hbase shell 8）、web 访问 hbase http://hp1:16010 8、配置 hive 1）、进入配置文件目录 cd /usr/local/src/hive/conf 2）、复制配置文件 cp hive-env.sh.template hive-env.sh cp hive-default.xml.template hive-site.xml # 创建文件夹，hive-site.xml 中配置需要 mkdir -p /usr/local/src/hive/tmp 3）、编辑 （vim hive-env.sh） export JAVA_HOME=/usr/local/src/jdk export HADOOP_HOME=/usr/local/src/hadoop export HIVE_HOME=/usr/local/src/hive export HIVE_CONF_DIR=/usr/local/src/hive/conf 4)、编辑 （vim hive-site.xml） 注意修改数据库连接密码 qwe 没有的 property 则添加，有的则根据需求修改 system:java.io.tmpdir /usr/local/src/hive/tmp javax.jdo.option.ConnectionURL jdbc:mysql://hp1:3306/hive?createDatabaseIfNotExist=true javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword qwe sed -i \"s#\\${system:java.io.tmpdir}/\\${system:user.name}#/usr/local/src/hive/iotmp#g\" /usr/local/src/hive/conf/hive-site.xml 5）、 复制 mysql 数据库驱动包到指定文件路径 cp mysql-connector-java-5.1.46-bin.jar /usr/local/src/hive/lib/ 6）、在 hdfs 中创建下面的目录 ，并赋予所有权限 hdfs dfs -mkdir -p /usr/local/src/hive/warehouse hdfs dfs -mkdir -p /usr/local/src/hive/tmp hdfs dfs -mkdir -p /usr/local/src/hive/log hdfs dfs -chmod -R 777 /usr/local/src/hive/warehouse hdfs dfs -chmod -R 777 /usr/local/src/hive/tmp hdfs dfs -chmod -R 777 /usr/local/src/hive/log 7）、初始化 hive schematool -dbType mysql -initSchema Metastore connection URL: jdbc:mysql://hp:3306/hive?createDatabaseIfNotExist=true Metastore Connection Driver : com.mysql.jdbc.Driver Metastore connection User: root Starting metastore schema initialization to 1.1.0 Initialization script hive-schema-1.1.0.mysql.sql Initialization script completed schemaTool completed 如果没有以上返回，则初始化失败，见常见问题 8）、安装hive到此结束，进入hive命令行 hive 9）、常见问题 常见问题 因为版本原因，需要重新拷贝一个 jar 包 rm -rf /usr/local/src/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar cp /usr/local/src/hive/lib/jline-2.12.jar /usr/local/src/hadoop/share/hadoop/yarn/lib/ 初始化后如果想要再重启服务 hive --service metastore & hive --service hiveserver2 & 这个在1.4.7中需要配置，否则在执行数据导入到hive时会报错 Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly. cp /usr/local/src/hive/lib/hive-exec-1.1.0.jar /usr/local/src/sqoop/lib/ 9、重启 hadoop 相关环境 1、启动各个节点（hp1、hp2、hp3）的zookeper服务 zkServer.sh start zkServer.sh status 2、启动各个节点（hp1、hp2、hp3）的 journalnode 服务 hadoop-daemon.sh start journalnode 3、启动主节点（hp1）的 namenode 服务 hadoop-daemon.sh start namenode 4、启动备用节点（hp2）的 namenode 服务 hdfs namenode -bootstrapStandby hadoop-daemon.sh start namenode 5、启动各个节点（hp1、hp2、hp3）的 datanode 服务 hadoop-daemon.sh start datanode 6、在两个 namenode 节点（hp1、hp2）分别启动zkfc服务 hadoop-daemon.sh start zkfc 7、启动 yarn 服务 start-yarn.sh "},"notes/bigdata/hadoop.html":{"url":"notes/bigdata/hadoop.html","title":"Hadoop 相关","keywords":"","body":"Hadoop 相关 Hadoop的核心就是HDFS和MapReduce，而两者只是理论基础，不是具体可使用的高级应用，Hadoop旗下有很多经典子项目，比如HBase、Hive等，这些都是基于HDFS和MapReduce发展出来的。要想了解Hadoop，就必须知道HDFS和MapReduce是什么。 一、HDFS HDFS（Hadoop Distributed File System，Hadoop分布式文件系统），它是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。 HDFS的设计特点是： 1、大数据文件，非常适合上T级别的大文件或者一堆大数据文件的存储，如果文件只有几个G甚至更小就没啥意思了。 2、文件分块存储，HDFS会将一个完整的大文件平均分块存储到不同计算器上，它的意义在于读取文件时可以同时从多个主机取不同区块的文件，多主机读取比单主机读取效率要高得多得都。 3、流式数据访问，一次写入多次读写，这种模式跟传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化也只能在文件末添加内容。 4、廉价硬件，HDFS可以应用在普通PC机上，这种机制能够让给一些公司用几十台廉价的计算机就可以撑起一个大数据集群。 5、硬件故障，HDFS认为所有计算机都可能会出问题，为了防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其它某几个主机上，如果其中一台主机失效，可以迅速找另一块副本取文件。 HDFS的关键元素： Block：将一个文件进行分块，通常是64M。 NameNode：保存整个文件系统的目录信息、文件信息及分块信息，这是由唯一一台主机专门保存，当然这台主机如果出错，NameNode就失效了。在Hadoop2.*开始支持activity-standy模式----如果主NameNode失效，启动备用主机运行NameNode。 DataNode：分布在廉价的计算机上，用于存储Block块文件。 HDFS体系结构 文件写入： 1） Client向NameNode发起文件写入的请求。 2） NameNode根据文件大小和文件块配置情况，返回给Client它管理的DataNode的信息。 3） Client将文件划分为多个block，根据DataNode的地址，按顺序将block写入DataNode块中。 文件读取： 1） Client向NameNode发起读取文件的请求。 2） NameNode返回文件存储的DataNode信息。 3） Client读取文件信息。 文件块的放置：一个Block会有三份备份，一份在NameNode指定的DateNode上，一份放在与指定的DataNode不在同一台机器的DataNode上，一根在于指定的DataNode在同一Rack上的DataNode上。备份的目的是为了数据安全，采用这种方式是为了考虑到同一Rack失败的情况，以及不同数据拷贝带来的性能的问题。 HDFS和MR共同组成Hadoop分布式系统体系结构的核心。HDFS在集群上实现了分布式文件系统，MR在集群上实现了分布式计算和任务处理。HDFS在MR任务处理过程中提供了文件操作和存储等支持，MR在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成分布式集群的主要任务。 二、MapReduce 通俗说MapReduce是一套从海量源数据提取分析元素最后返回结果集的编程模型，将文件分布式存储到硬盘是第一步，而从海量数据中提取分析我们需要的内容就是MapReduce做的事了。 MapReduce是一种计算模型，用以进行大数据量的计算。其中Map对数据集上的独立元素进行指定的操作，生成键-值对形式中间结果。Reduce则对中间结果中相同“键”的所有“值”进行规约，以得到最终结果。MapReduce这样的功能划分，非常适合在大量计算机组成的分布式并行环境里进行数据处理。 下面以一个计算海量数据最大值为例：一个银行有上亿储户，银行希望找到存储金额最高的金额是多少，按照传统的计算方式，我们会这样： Long moneys[]; Long max = 0L; for(int i=0;imax){ max = moneys[i]; } } 如果计算的数组长度少的话，这样实现是不会有问题的，还是面对海量数据的时候就会有问题。 MapReduce会这样做：首先数字是分布存储在不同块中的，以某几个块为一个Map，计算出Map中最大的值，然后将每个Map中的最大值做Reduce操作，Reduce再取最大值给用户。 MapReduce的基本原理就是： 将大的数据分析分成小块逐个分析，最后再将提取出来的数据汇总分析，最终获得我们想要的内容。当然怎么分块分析，怎么做Reduce操作非常复杂，Hadoop已经提供了数据分析的实现，我们只需要编写简单的需求命令即可达成我们想要的数据。 Hadoop典型应用有： 搜索、日志处理、推荐系统、数据分析、视频图像分析、数据保存等。 总的来说Hadoop适合应用于大数据存储和大数据分析的应用，适合于服务器几千台到几万台的集群运行，支持PB级的存储容量。 三、其他 Hadoop是一个开源框架，它允许在整个集群使用简单编程模型计算机的分布式环境存储并处理大数据。它的目的是从单一的服务器到上千台机器的扩展，每一个台机都可以提供本地计算和存储。 通常，集群里的一台机器被指定为NameNode，另一台不同的机器被指定为JobTracker。这些机器是masters。余下的机器即作为DataNode也作为TaskTracker。这些机器是slaves。 Hadoop守护进程指NameNode/DataNode和JobTracker/TaskTracker。 HDFS由一个NameNode和多个DataNode组成 MapReduce由一个JobTracker和多个TaskTracker组成 MapReduce为海量的数据提供了计算 HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。 HDFS为海量的数据提供了存储。 HDFS是Hadoop应用用到的一个最主要的分布式存储系统。一个HDFS集群主要由一个NameNode和很多个Datanode组成：Namenode管理文件系统的元数据，而Datanode存储了实际的数据。基本上，客户端联系Namenode以获取文件的元数据或修饰属性，而真正的文件I/O操作是直接和Datanode进行交互的。 NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；namenode内存中存储的是=fsimage+edits。 SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。 DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。 热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。 冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。 fsimage:元数据镜像文件（文件系统的目录树。） edits：元数据的操作日志（针对文件系统做的修改操作记录） MapReduce：是一种编程模型，用于大规模数据集（大于1TB）的并行运算。映射（Map）、化简（Reduce）的概念和它们的主要思想都是从函数式编程语言中借鉴而来的。它极大地方便了编程人员——即使在不了解分布式并行编程的情况下，也可以将自己的程序运行在分布式系统上。 historyserver：Hadoop自带了一个历史服务器，可以通过历史服务器查看已经运行完的Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息 http://blog.csdn.net/zhangliangzi/article/details/52071218 四、mapreduce 实现流程 五、基本概念 Hadoop HDFS 海量存储 MapReduce 海量计算 Yarn ResourceManger 统一管理和调度 NodeManger 执行任务、领取任务 ApplicationMaster 向RM申请资源，创建任务 HDFS NameNode 管理 SecondaryNameNode 协助NameNode（不是副本） DataNode 管理存储 MapReduce mapper 拆分 reducer 合并 "},"notes/bigdata/hadoop_single.html":{"url":"notes/bigdata/hadoop_single.html","title":"Hadoop 单机伪分布式部署","keywords":"","body":"Hadoop 单机伪分布式部署 1、安装软件（如果没有） apt update apt -y install vim openssh-server 2、免密 修改ssh配置文件 vim /etc/ssh/sshd_config ... PasswordAuthentication yes .... PermitRootLogin yes ... 重启ssh服务 service ssh restart 生成秘钥对（根据提示回车） ssh-keygen 将公钥拷贝到免密节点 ssh-copy-id hadoop1 验证（ssh连接如果没提示输入密码，则免密成功） ssh hadoop1 3、解压相关包 tar -zxvf jdk1.8.0_111.tar.gz tar -zxvf hadoop-2.7.3.tar.gz 4、修改环境变量 vim ~/.bashrc export JAVA_HOME=/opt/jdk1.8.0_111 export HADOOP_HOME=/opt/hadoop-2.7.3 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 使添加环境变量生效 source /etc/profile 5、验证 jps hadoop version java -version hadoop version 6、单机版hadoop测试 mkdir input hadoop jar /opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output 'dfs[a-z.]+' hadoop jar /opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep /root/hp output 'hello' cat output/* 7、编辑hosts、hostname vim /etc/hosts ... 192.169.1.1 hadoop1 ... vim /etc/hostname hp hostnamectl set-hostname hp 或者 reboot 8、解压相关软件包 cd /opt tar -zxvf jdk1.8.0_111.tar.gz tar -zxvf hadoop-2.7.3.tar.gz 9、配置hadoop cd hadoop-2.7.3/etc/hadoop vim hadoop-env.sh export JAVA_HOME=/opt/jdk1.8.0_111 将 slave 的主机名写入到该文件(这里是单节点伪分布式所以只需要加入本机host) vim slaves hadoop1 编辑相关配置文件 vim core-site.xml ... fs.defaultFS hdfs://hadoop1:9000 　 hadoop.tmp.dir file:/usr/local/hadoop/tmp Abase for other temporary directories. vim hdfs-site.xml ... dfs.namenode.http-address hadoop1:50070 dfs.namenode.secondary.http-address hadoop1:50090 dfs.replication 1 dfs.namenode.name.dir file:/usr/local/hadoop/tmp/dfs/name dfs.datanode.data.dir file:/usr/local/hadoop/tmp/dfs/data 10、启动hadoop 首次启动需要先在master节点（这里的hadoop1）上执行namenode的格式化操作,成功的话，会看到\"Exitting with status 0\"的提示，若为\"Exitting with status 1\"则是出错。 hdfs namenode -format 完成 Hadoop 格式化后，在namenode节点上启动Hadoop各个服务,使用jps命令验证相关服务是否运行起来。 start-dfs.sh jps ------ 58993 NameNode 59601 Jps 59459 SecondaryNameNode 59304 DataNode 另外还需要在 Master 节点上通过命令 hdfs dfsadmin -report 查看 DataNode 是否正常启动，如果 Live datanode 不为 0，则说明集群启动成功， hdfs dfsadmin -report NameNode结点 http://hp:50070 SecondaryNameNode http://hp:50090 11、运行hadoop伪分布式实例 单机模式，以grep例子读取的是本地数据，伪分布式读取的则是HDFS上的数据，要使用HDFS，首先要在 HDFS中创建用户目录 hdfs dfs -mkdir -p /user/hadoop hdfs dfs -mkdir input 传输文件达到input目录 hdfs dfs -put ./etc/hadoop/*.xml /input 查看 hdfs dfs -ls /input hadoop jar /opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep /input output 'dfs[a-z.]+' hdfs dfs -cat output/* 取回本地 hdfs dfs -get output/* 在运行mapreducer前，不能有output目录，如果已经存在，则会报错误 \"output already exists\"，要先删除output目录 hdfs dfs -rm -r output 关闭hadoop： sbin/stop-dfs.sh 12、关于yarn，伪分布式不启动yarn也可以，一般不会影响程序执行 cp mapred-site.xml.template mapred-site.xml vim mapred-site.xml mapreduce.framework.name yarn mapreduce.jobhistory.address hp:10020 mapreduce.jobhistory.webapp.address hp:19888 --> vim yarn-site.xml yarn.resoursemanager.hostname hp --> yarn.nodemanager.aux-services mapreduce_shuffle 13、如果是集群，拷贝配置到所有其他节点，伪分布式跳过此步 14、开启yarn start-yarn.sh jps ------ 58993 NameNode 59649 ResourceManager 59459 SecondaryNameNode 60070 Jps 59767 NodeManager 59304 DataNode 启动yarn有个好处是可以通过web界面查看任务运行情况 http://192.168.0.10:8088/cluster YARN 主要是为集群提供更好的资源管理与任务调度，然而这在单机上体现不出价 值，反而会使程序跑得稍慢些。因此在单机上是否开启 YARN 就看实际情况了。 • 不启动 YARN 需重命名 mapred-site.xml • 如果不想启动 YARN，务必把配置文件 mapred-site.xml 重命名，改成 mapredsite.xml.template，需要用时改回来就行。否则在该配置文件存在，而未开启 YARN 的情况下，运行程序会提示 “Retrying connect to server: 0.0.0.0/0.0.0.0:8032” 的错误，这也是为何该配置文件初始文件名为 mapredsite.xml.template。 关闭yarn： stop-yarn.sh 通过命令 jps 可以查看各个节点所启动的进程。正确的话，在 Master 节点上可以看到 NameNode,ResourceManager,SecondaryNameNode,JobHistoryServer 进程。 在 Slave 节点可以看到 DataNode 和 NodeManager 进程。 15、开启历史服务器，才能在web中查看任务运行情况 mr-jobhistory-daemon.sh start historyserver jps ------ 58993 NameNode 59649 ResourceManager 60147 Jps 59459 SecondaryNameNode 59767 NodeManager 59304 DataNode 60108 JobHistoryServer 16、验证hadoop 另外还需要在Master节点(hadoop1)上通过命令 hdfs dfsadmin -report 查看DataNode是否正常启动，如果Live datanode不为0，则说明集群启动成功 HDFS管理界面(NameNode结点) http://hadoop1:50070 (SecondaryNameNode) http://hadoop1:50090 ResourceManager管理界面（yarn） http://hadoop1:8088 NameNode结点 http://hp:50070 SecondaryNameNode http://hp:50090 cluser http://192.168.0.10:8088/cluster 17、系统正常启动，跑个程序试试 hdfs 常用命令 mkdir input cd input echo \"hello world\">test1.txt echo \"hello hadoop\">test2.txt cd .. bin/hadoop dfs -put input in bin/hadoop jar build/hadoop-0.20.2-examples.jar wordcount in out bin/hadoop dfs -cat out/* Hadoop配置文件说明 Hadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。 此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。 18、python利用streaming编写mapreduce程序 #先编写一个map.py # coding: utf8 import sys for line in sys.stdin: line = line.strip() film_d = line.split(\";\") print(film_d[0]) #再编写一个red.py： # coding: utf8 import sys cur_film = \"惊天魔盗团2\" cur_count = 0 for line in sys.stdin: if cur_film in line: cur_count += 1 print('%s总共出现了，%s次。' % ( cur_film,cur_count)) hdfs dfs -put film-csv.txt / chmod 777 mapper.py chmod 777 reducer.py hadoop jar /opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar -file /root/mapper.py -mapper \"python mapper.py\" -file /root/reducer.py -reducer \"python reducer.py\" -input /film-csv.txt -output /output 19、实例 #查看帮助命令 hdfs dfs -help #创建一个数据导入文件夹 hdfs dfs -mkdir -p /data/input #在本地创建两个文本，并加入有规律内容 echo \"hello world\">test1.txt echo \"hello hadoop\">test2.txt #将文件上传至hdfs上 hdfs dfs -put ./*.txt /data/input #查看hdfs上的文件 hdfs dfs -ls /data/input/ #运行wordcunt（grep）方法进行计算 hadoop jar /opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /data/input/ output #hadoop jar /opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep /data/input/ output 'hello' #查看运行结果 hdfs dfs -cat output/* #将结果取回本地 hdfs dfs -get output ./output #删除hdfs上的文件或文件夹 hdfs dfs -rm -r output "},"notes/bigdata/hadoop_cluster.html":{"url":"notes/bigdata/hadoop_cluster.html","title":"Hadoop 集群搭建","keywords":"","body":"Hadoop 集群搭建 1、解压相关软件包 tar -zxvf jdk1.8.0_111.tar.gz tar -zxvf hadoop-2.7.3.tar.gz tar -zxvf zookeeper-3.4.9.tar.gz tar -zxvf hbase-1.2.3.tar.gz 2、增加环境变量 vim /etc/profile export JAVA_HOME=/opt/jdk1.8.0_111 export HADOOP_HOME=/opt/hadoop-2.7.3 export ZOOKEEPER_HOME=/opt/zookeeper-3.4.9 export HBASE_HOME=/opt/hbase-1.2.3 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$HBASE_HOME/bin 3、编辑hosts vim /etc/hosts 192.169.1.1 hadoop1 4、配置hadoop cd hadoop-2.7.3/etc/hadoop 5、编辑hadoop-env.sh vim hadoop-env.sh export JAVA_HOME=/opt/jdk1.8.0_111 6、将 slave 的主机名写入到该文件 vim slaves hadoop2 hadoop3 7、编辑core-site.xml vim core-site.xml fs.defaultFS hdfs://hadoop1:9000 hadoop.tmp.dir file:/usr/local/hadoop/tmp Abase for other temporary directories. 8、编辑hdfs-site.xml vim hdfs-site.xml dfs.namenode.http-address hadoop1:50070 dfs.namenode.secondary.http-address hadoop1:50090 dfs.replication 1 dfs.namenode.name.dir file:/usr/local/hadoop/tmp/dfs/name dfs.datanode.data.dir file:/usr/local/hadoop/tmp/dfs/data 9、编辑mapred-site.xml vim mapred-site.xml mapreduce.framework.name yarn mapreduce.jobhistory.address hadoop1:10020 mapreduce.jobhistory.webapp.address hadoop1:19888 10、编辑yarn-site.xml vim yarn-site.xml yarn.resoursemanager.hostname hadoop1 yarn.nodemanager.aux-services mapreduce_shuffle 11、拷贝hadoop所有配置到所有其他节点 12、启动hadoop 首次启动需要先在 master 节点上执行 namenode 的格式化操作 hdfs namenode -format 成功的话，会看到 “successfully formatted” 和 “Exitting with status 0” 的提示，若为 “Exitting with status 1” 则是出错。 完成 Hadoop 格式化后，在 namenode 节点上启动 Hadoop 各个服务 start-dfs.sh -------------- 58993 NameNode 59601 Jps 59459 SecondaryNameNode 59304 DataNode start-yarn.sh -------------- 58993 NameNode 59649 ResourceManager 59459 SecondaryNameNode 60070 Jps 59767 NodeManager 59304 DataNode mr-jobhistory-daemon.sh start historyserver -------------------- 58993 NameNode 59649 ResourceManager 60147 Jps 59459 SecondaryNameNode 59767 NodeManager 59304 DataNode 60108 JobHistoryServer 通过命令 jps 可以查看各个节点所启动的进程。正确的话，在 Master 节点上可以看到 NameNode,ResourceManager,SecondaryNameNode,JobHistoryServer 进程。 在 Slave 节点可以看到 DataNode 和 NodeManager 进程。 另外还需要在 Master 节点上通过命令 hdfs dfsadmin -report 查看 DataNode 是否正常启动，如果 Live datanode 不为 0，则说明集群启动成功， NameNode结点 http://hadoop1:50070 SecondaryNameNode http://hadoop1:50090 13、系统启动正常后，跑个程序试试 $mkdir input $cd input $echo \"hello world\">test1.txt $echo \"hello hadoop\">test2.txt $cd .. $bin/hadoop dfs -put input in $bin/hadoop jar build/hadoop-0.20.2-examples.jar wordcount in out $bin/hadoop dfs -cat out/* 14、Hadoop配置文件说明 Hadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。 此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。 15、配置zookeeper cd zookeeper-3.4.9/conf mv zoo_sample.cfg zoo.cfg dataDir=/opt/zookeeper-3.4.9/data server.0=hadoop1:2888:3888 server.1=hadoop2:2888:3888 server.2=hadoop3:2888:3888 接下来分别在 hadoop1、hadoop2、hadoop3 节点上的/opt/zookeeper-3.4.9/data 目录下创建一 个名为 myid 的文件，并在hadoop1节点上的 myid 文件里输入 0，在hadoop2节点上的myid输入 1，在hadoop3节点上的 myid 文件里输入 2 mkdir /opt/zookeeper-3.4.9/data echo 0 >> /opt/zookeeper-3.4.9/data/myid 拷贝zookeeper配置到所有其他节点 每个节点启动zkserver zkServer.sh start --------------------- 5089 QuorumPeerMain 5107 Jps 4468 SecondaryNameNode 4932 JobHistoryServer 4634 ResourceManager 4254 NameNode 验证 ZooKeeper 服务，三台节点必须是 1 个 leader 2 个 follower 的状态才算配置正确 zkServer.sh status 16、配置hbase cd hbase-1.2.3/conf vim hbase-env.sh export JAVA_HOME=/opt/jdk1.8.0_111 export HBASE_MANAGES_ZK=false vim hbase-site.xml hbase.rootdir hdfs://hadoop1:9000/hbase hbase.cluster.distributed true hbase.zookeeper.quorum hadoop1,hadoop2,hadoop3 hbase.zookeeper.property.dataDir /opt/zookeeper-3.4.9/data/ vim regionservers hadoop1 hadoop2 hadoop3 启动hbase 启动 HBase 时需要确保 hdfs 已经启动，使用命令 hdfs dfsadmin -report 查看以下 HDFS 集群是否正常，如果正常，在 master 节点上执行以下命令启动 HBase 集群: start-hbase.sh ----------------------------- 5089 QuorumPeerMain 4468 SecondaryNameNode 4932 JobHistoryServer 5476 HRegionServer 5319 HMaster 5545 Jps 4634 ResourceManager 4254 NameNode http://hadoop1:16010 "},"notes/bigdata/hive.html":{"url":"notes/bigdata/hive.html","title":"Hive 相关","keywords":"","body":"Hive 相关 一、hive相关资料 http://blog.csdn.net/u013310025/article/details/70306421 https://www.cnblogs.com/guanhao/p/5641675.html http://blog.csdn.net/wisgood/article/details/40560799 http://blog.csdn.net/seven_zhao/article/details/46520229 二、安装 1、获取主机相关信息 export password='qwe' export your_ip=$(ip ad|grep inet|grep -v inet6|grep -v 127.0.0.1|awk '{print $2}'|cut -d/ -f1) export your_hosts=$(cat /etc/hosts |grep $(echo $your_ip)|awk '{print $2}') 2、安装mysql echo \"mysql-server-5.5 mysql-server/root_password password $password\" | debconf-set-selections echo \"mysql-server-5.5 mysql-server/root_password_again password $password\" | debconf-set-selections apt-get -y install mariadb-server python-pymysql --force-yes echo \"[mysqld] bind-address = $your_ip default-storage-engine = innodb innodb_file_per_table max_connections = 4096 collation-server = utf8_general_ci character-set-server = utf8\" | tee > /etc/mysql/conf.d/openstack.cnf sed -i \"s/127.0.0.1/0.0.0.0/g\" /etc/mysql/mariadb.conf.d/50-server.cnf service mysql restart 3、创建hive用户和赋予权限 mysql -uroot -p$password 4、增加hive环境变量 hive_flag=$(grep \"hive\" /etc/profile) if [ ! -n \"$hive_flag\" ]; then sed -i \"s/\\$PATH:/\\$PATH:\\/opt\\/apache-hive-2.3.2-bin\\/bin:/g\" /etc/profile else echo \"Already exist!\" fi 5、使脚本中环境变量生效 source /etc/profile 6、修改hive配置 echo \"$(grep \"JAVA_HOME=\" /etc/profile) $(grep \"HADOOP_HOME=\" /etc/profile) export HIVE_HOME=/opt/apache-hive-2.3.2-bin export HIVE_CONF_DIR=/opt/apache-hive-2.3.2-bin/conf\" |tee >> /opt/apache-hive-2.3.2-bin/conf/hive-env.sh sed -i \"s/hadoop3/$your_hosts/g\" /opt/apache-hive-2.3.2-bin/conf/hive-site.xml 7、在hdfs 中创建下面的目录 ，并赋予所有权限 hdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -mkdir -p /user/hive/tmp hdfs dfs -mkdir -p /user/hive/log hdfs dfs -chmod -R 777 /user/hive/warehouse hdfs dfs -chmod -R 777 /user/hive/tmp hdfs dfs -chmod -R 777 /user/hive/log mkdir -p /user/hive/tmp 8、初始化hive schematool -dbType mysql -initSchema 三、使用 hive 1、查看数据库，使用数据库 show databases; use default; 2、查看表 show tables; 3、创建hive表 \";\" ASCI码为 \"\\073\" ROW FORMAT DELIMITED 每条数据按行拆分 FIELDS TERMINATED BY '\\073' 每行数据字段按 \";\" 拆分 STORED AS TEXTFILE 保存为文本文件 create table film (name string, startTime date, endTime date, company string, director string, actor string, type string, price float, score float) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\073' STORED AS TEXTFILE; 4、将本地文本导入 hive 先在 shell 中生成文本文件 以下是完整一点儿的电影信息 https://github.com/hlyani/demo/blob/master/film.csv echo \"电影名称;上映时间;闭映时间;出品公司;导演;主角;影片类型;票房/万;评分; 《熊出没之夺宝熊兵》;2014.1.17;2014.2.23;深圳华强数字动漫有限公司;丁亮;熊大，熊二，; 《菜鸟》;2015.3.27;2015.4.12;麒麟影业公司;项华祥;柯有伦，崔允素，张艾青，刘亚鹏，张星宇;爱情/动作/喜剧;192.0 ;4.5 ; 《栀子花开》;2015.7.10;2015.8.23;世纪百年影业，文投基金，华谊兄弟，千和影业，剧角映画，合一影业等;何炅;李易峰，张慧雯，蒋劲夫，张予曦，魏大勋，李心艾，杜天皓，宋轶，王佑硕，柴格，张云龙;青春，校园，爱情;37900.8 ;4.0 ; 《我是大明星》;2015.12.20;2016.1.31;北京中艺博悦传媒;张艺飞;高天，刘波，谭皓，龙梅子;爱情 励志 喜剧;9.8 ;2.5 ; 《天将雄师》;2015.2.19;2015.4.6;耀莱文化，华谊兄弟，上海电影集团;李仁港;成龙，约翰·库萨克，阿德里安·布劳迪，崔始源 ，林鹏，王若心，筷子兄弟，西蒙子，冯绍峰，朱佳煜;动作，古装，剧情，历史;74430.2 ;5.9 ;\" > /root/film.csv overwrite 覆盖之前数据 load data local inpath '/root/film.csv' overwrite into table film; 5、将hdfs中数据导入hive hdfs dfs -put /root/film.csv /home hdfs dfs -cat /home/film.csv load data inpath '/home/film.csv'into table film; 6、从 hive 导出到本地文件系统 insert overwrite local directory '/root/hl' select * from film; 或直接在shell中执行 hive -e \"select * from film\" >> film1.csv 或者是以下命令， 其中文件sql.q写入你想要执行的查询语句 hive -f sql.q >> film1.csv 7、从 hive 导出到 hdfs 中 注意，和导出文件到本地文件系统的HQL少一个local，数据的存放路径就不一样了。 insert overwrite directory '/root/hl' select * from film; 8、hive 中使用dfs命令 hive> dfs -ls /user; 9、查看表结构信息 格式化查看表结构 desc formatted film; 查看表详细信息 desc film; 10、查看表信息 select * from film; # 查看前十项 select * from film limit 10; select count(price) from film; # distinct 去重 select count(distinct price) from film; select sum(price) from film; select avg(price) from film; 11、将提取到的数据保存到临时表中 insert overwrite table movies 本地加载 load data local inpath '/Users/tifa/Desktop/1.txt' into table test; 从hdfs上加载数据 load data inpath '/user/hadoop/1.txt' into table test_external; 抹掉之前的数据重写 load data inpath '/user/hadoop/1.txt' overwrite into table test_external; CREATE TABLE movies( name string, data string, record int ) COMMENT '2014全年上映电影的数据记录' FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE; load data local inpath 'dat0204.log' into table movies; 12、删除表 DROP TABLE if exists film; "},"notes/bigdata/hbase.html":{"url":"notes/bigdata/hbase.html","title":"Hbase 相关","keywords":"","body":"Hbase 相关 一、介绍 与传统数据库的对比 1、传统数据库遇到的问题： 1）数据量很大的时候无法存储； 2）没有很好的备份机制； 3）数据达到一定数量开始缓慢，很大的话基本无法支撑； 2、HBASE优势： 1）线性扩展，随着数据量增多可以通过节点扩展进行支撑； 2）数据存储在hdfs上，备份机制健全； 3）通过zookeeper协调查找数据，访问速度快。 3、HBase中的表一般有这样的特点： ​ 1、大：一个表可以有上亿行，上百万列 ​ 2、面向列:面向列(族)的存储和权限控制，列(族)独立检索。 ​ 3、稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。 二、hbase使用 hbase shell 常用命令 hbase shell 1、创建表 create 'users','user_id','address','info' 说明:表users,有三个列族user_id,address,info 2、列出全部表 list 3、得到表的描述 describe 'users' 4、创建表 create 'users_tmp','user_id','address','info' 5、删除表 disable 'users_tmp' drop 'users_tmp' 6、添加记录 put ‘表名’,’行键(标识)’,’列族:字段’,’数值’ put 'users','yani','info:age','24'; put 'users','yani','info:birthday','1987-06-17'; 7、获取一条记录 1.取得一个id的所有数据 get 'users','yani' 2.获取一个id，一个列族的所有数据 get 'users','yani','info' 3.获取一个id，一个列族中一个列的所有数据 get 'users','yani','info:age' 8、更新记录 put 'users','yani','info:age' ,'29' get 'users','yani','info:age' 9、获取单元格数据的版本数据 get 'users','yani',{COLUMN=>'info:age',VERSIONS=>1} 10、获取单元格数据的某个版本数据 get 'users','yani',{COLUMN=>'info:age',TIMESTAMP=>1364874937056} 11、全表扫描 scan 'users' 12、删除yani值的'info:age'字段 delete 'users','yani','info:age' get 'users','yani' 13、删除整行 deleteall 'users','yani' 14、统计表的行数 count 'users' 15、清空表 truncate 'users' "},"notes/bigdata/spark.html":{"url":"notes/bigdata/spark.html","title":"spark 相关","keywords":"","body":"spark 相关 一、安装 jdk curl -O https://download.oracle.com/otn/java/jdk/8u221-b11/230deb18db3e4014bb8e3e8324f81b43/jdk-8u221-linux-x64.tar.gz?AuthParam=1571023421_c1f05abfd1ce9b457c29b0565494cc83 tar -zxvf jdk-8u221-linux-x64.tar.gz vim /etc/profile export JAVA_HOME=/root/jdk1.8.0_221 export PATH=$PATH:$JAVA_HOME/bin source /etc/profile 二、hadoop安装 1、下载 curl -O https://www-us.apache.org/dist/hadoop/common/hadoop-3.2.1/hadoop-3.2.1-src.tar.gz tar -zxvf hadoop-3.2.1-src.tar.gz 2、配置 cd /root/hadoop-3.2.1/etc/hadoop mapred-site.xml exprot JAVA_HOME=${JAVA_HOME} hdfs-site.xml dfs.namenode.http-address master:50070 dfs.replication 2 dfs.permissions true dfs.namenode.name.dir file:/root/hadoop-3.2.1/dfs/name dfs.datanode.data.dir file:/root/hadoop-3.2.1/dfs/data yarn-env.sh export JAVA_HOME=/root/jdk1.8.0_221 yarn-site.xml yarn.resoursemanager.hostname master yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce.shuffle.class org.apache.hadoop.mapred.ShuffleHandler yarn.resourcemanager.address master:8032 yarn.resourcemanager.scheduler.address master:8030 yarn.resourcemanager.resource-tracker.address master:8035 yarn.resourcemanager.admin.address master:8033 yarn.resourcemanager.webapp.address master:8088 yarn.nodemanager.resource.memory-mb 30720 works node1 node2 core-site.xml fs.defaultFS hdfs://master:9000 hadoop.tmp.dir file:/usr/local/hadoop/tmp Abase for other temporary directories. mapred-site.xml mapreduce.framework.name yarn mapreduce.jobhistory.address master:10020 mapreduce.jobhistory.webapp.address master:19888 3、启动 /root/hadoop-3.2.1/sbin/start-all.sh 三、安装spark 1、下载 curl -O https://www.apache.org/dyn/closer.lua/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz tar -zxvf spark-2.4.4-bin-hadoop2.7.tgz 2、配置 cd spark-2.4.4-bin-hadoop2.7/conf cp spark-env.sh.template spark-env.sh cp spark-defaults.conf.template spark-defaults.conf cp slaves.template slaves spark-env.sh export JAVA_HOME=/root/jdk1.8.0_221 export SPARK_MASTER_IP=192.168.0.128 slaves node1 node2 spark-defaults.conf spark.master spark://master:7077 spark.eventLog.enabled true spark.driver.cores 2 spark.driver.memory 2g spark.executor.memory 7g spark.rpc.message.maxSize 1024 3、启动 /root/spark-2.4.4-bin-hadoop2.7/sbin/start-all.sh jps 四、使用 1、使用示例代码 spark-submit --class org.apache.spark.examples.JavaWordCount /usr/local/src/spark/examples/jars/spark-examples_2.11-2.4.4.jar /usr/local/src/spark/README.md spark-submit --class org.apache.spark.examples.JavaSparkPi /usr/local/src/spark/examples/jars/spark-examples_2.11-2.4.4.jar /usr/local/src/spark/README.md spark-submit /usr/local/src/spark/examples/src/main/python/pi.py 10 cd spark-2.4.4-bin-hadoop2.7/ ./bin/spark-shell [root@master spark-2.4.4-bin-hadoop2.7]# ./bin/pyspark Python 2.7.5 (default, Oct 30 2018, 23:45:53) [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. 19/10/14 14:36:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 19/10/14 14:36:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.4 /_/ Using Python version 2.7.5 (default, Oct 30 2018 23:45:53) SparkSession available as 'spark'. vim /root/spark-2.4.4-bin-hadoop2.7/examples/src/main/python/pi.py from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession\\ .builder\\ .appName(\"PythonPi\")\\ .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 textFile = spark.read.text(\"README.md\") textFile.count() textFile.first() linesWithSpark = textFile.filter(textFile.value.contains(\"Spark\")) hadoop dfsadmin -safemode leave spark-submit --executor-memory 8G --num-executors 2 --master yarn --deploy-mode cluster --driver-memory 2G pie.py 10 yarn logs -applicationId application_1571131648061_0016 8080 spark 8088 yarn 50070 handoop 五、资源配置 Spark任务的core，executor，memory资源配置方法 1、Spark应用当中术语的基本定义： Partitions : 分区是大型分布式数据集的一小部分。 Spark使用分区来管理数据，这些分区有助于并行化数据处理，并且使executor之间的数据交换最小化 Task：任务是一个工作单元，可以在分布式数据集的分区上运行，并在单个Excutor上执行。并行执行的单位是任务级别。单个Stage中的Tasks可以并行执行 Executor：在一个worker节点上为应用程序创建的JVM，Executor将巡行task的数据保存在内存或者磁盘中。每个应用都有自己的一些executors，单个节点可以运行多个Executor，并且一个应用可以跨多节点。Executor始终伴随Spark应用执行过程，并且以多线程方式运行任务。spark应用的executor个数可以通过SparkConf或者命令行 –num-executor进行配置 Cores：CPU最基本的计算单元，一个CPU可以有一个或者多个core执行task任务，更多的core带来更高的计算效率，Spark中，cores决定了一个executor中并行task的个数 Cluster Manager：cluster manager负责从集群中请求资源 2、cluster模式执行的Spark任务包含了如下步骤： driver端，SparkContext连接cluster manager（Standalone/Mesos/Yarn） Cluster Manager在其他应用之间定位资源，只要executor执行并且能够相互通信，可以使用任何Cluster Manager Spark获取集群中节点的Executor，每个应用都能够有自己的executor处理进程 发送应用程序代码到executor中 SparkContext将Tasks发送到executors 以上步骤可以清晰看到executors个数和内存设置在spark中的重要作用。 3、例子1 硬件资源： 6 节点，每个节点16 cores, 64 GB 内存 每个节点在计算资源时候，给操作系统和Hadoop的进程预留1core，1GB，所以每个节点剩下15个core和63GB 内存。 core的个数，决定一个executor能够并发任务的个数。所以通常认为，一个executor越多的并发任务能够得到更好的性能，但有研究显示一个应用并发任务超过5，导致更差的性能。所以core的个数暂设置为5个。 5个core是表明executor并发任务的能力，并不是说一个系统有多少个core，即使我们一个CPU有32个core，也设置5个core不变。 executor个数，接下来，一个executor分配 5 core,一个node有15 core，从而我们计算一个node上会有3 executor（15 / 5），然后通过每个node的executor个数得到整个任务可以分配的executors个数。 我们有6个节点，每个节点3个executor，6 × 3 = 18个executors，额外预留1个executor给AM，最终要配置17个executors。 最后spark-submit启动脚本中配置 –num-executors = 17 memory，配置每个executor的内存，一个node，3 executor， 63G内存可用，所以每个executor可配置内存为63 / 3 = 21G 从Spark的内存模型角度，Executor占用的内存分为两部分：ExecutorMemory和MemoryOverhead，预留出MemoryOverhead的内存量之后，才是ExecutorMemory的内存。 MemoryOverhead的计算公式： max(384M, 0.07 × spark.executor.memory) 因此 MemoryOverhead值为0.07 × 21G = 1.47G > 384M 最终executor的内存配置值为 21G – 1.47 ≈ 19 GB 至此， Cores = 5, Executors= 17, Executor Memory = 19 GB 六、RDD RDD 1、转换操作 RDD 的转换操作是返回新的 RDD 的操作。转换出来的 RDD 是惰性求值的，只有在行动操作中用到这些 RDD 时才会被计算。 rdd1={1, 2, 5, 3}，rdd2={3,4,5} rdd1.flatMap(x=>x.to(3)) 1 -> {1,2,3} 2 -> {2,3} 3 -> {} 3 -> {3} 123233 spark-shell var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3)) rdd1.flatMap(x=>x.to(4)).collect() 函数名 作用 示例 结果 map() 将函数应用于 RDD 的每个元素，返回值是新的 RDD，有返回值 rdd1.map(x=>x+l) {2,3,4,4} flatMap() 将函数应用于 RDD 的每个元素，将元素数据进行拆分，变成迭代器，返回值是新的 RDD，压平操作，先map后flat，将函数应用于RDD中的每一项，对于每一项都产生一个集合，并将集合中的元素压扁成一个集合。 rdd1.flatMap(x=>x.to(3)) {1,2,3,2,3,3,3} filter() 函数会过滤掉不符合条件的元素，返回值是新的 RDD rdd1.filter(x=>x!=1) {2,3,3} distinct() 将 RDD 里的元素进行去重操作 rdd1.distinct() (1,2,3) union() 生成包含两个 RDD 所有元素的新的 RDD，并集 rdd1.union(rdd2) {1,2,3,3,3,4,5} intersection() 求出两个 RDD 的共同元素，交集 rdd1.intersection(rdd2) {3} subtract() 将原 RDD 里和参数 RDD 里相同的元素去掉 rdd1.subtract(rdd2) {1,2} cartesian() 求两个 RDD 的笛卡儿积 rdd1.cartesian(rdd2) {(1,3),(1,4)……(3,5)} groupBy() 分组 sortBy() 排序 2、行动操作 行动操作用于执行计算并按指定的方式输出结果。行动操作接受 RDD，但是返回非 RDD，即输出一个值或者结果。在 RDD 执行过程中，真正的计算发生在行动操作 rdd={1,2,3,3} 函数名 作用 示例 结果 collect() 返回 RDD 的所有元素 rdd.collect() {1,2,3,3} count() RDD 里元素的个数 rdd.count() 4 countByValue() 各元素在 RDD 中的出现次数 rdd.countByValue() {(1,1),(2,1),(3,2})} take(num) 从 RDD 中返回 num 个元素 rdd.take(2) {1,2} top(num) 从 RDD 中，按照默认（降序）或者指定的排序返回最前面的 num 个元素 rdd.top(2) {3,3} reduce() 并行整合所有 RDD 数据，如求和操作 rdd.reduce((x,y)=>x+y) 9 reduceByKey() groupByKey() 根据key进行分组 fold(zero)(func) 和 reduce() 功能一样，但需要提供初始值 rdd.fold(0)((x,y)=>x+y) 9 foreach(func) 对 RDD 的每个元素都使用特定函数，没有返回值 rdd1.foreach(x=>printIn(x)) 打印每一个元素 saveAsTextFile(path) 将数据集的元素，以文本的形式保存到文件系统中 rdd1.saveAsTextFile(file://home/test) saveAsSequenceFile(path) 将数据集的元素，以顺序文件格式保存到指 定的目录下 saveAsSequenceFile(hdfs://home/test) first 返回第一个元素 rdd1.first() 1 cogroup() 表示将多个rdd的同一个key对应的value组合到一起 七、示例代码 pi from __future__ import print_function import sys from random import random from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession\\ .builder\\ .appName(\"PythonPi\")\\ .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 wordcount from __future__ import print_function import sys from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount \", file=sys.stderr) sys.exit(-1) spark = SparkSession\\ .builder\\ .appName(\"PythonWordCount\")\\ .getOrCreate() lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0]) counts = lines.flatMap(lambda x: x.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(add) output = counts.collect() for (word, count) in output: print(\"%s: %i\" % (word, count)) spark.stop() 八、pom.xml 4.0.0 com.hl SparkPi 1.0-SNAPSHOT org.apache.spark spark-core_2.11 2.4.4 org.apache.spark spark-sql_2.11 2.4.4 VM options: -Dspark.master=local "},"notes/bigdata/spark_sql.html":{"url":"notes/bigdata/spark_sql.html","title":"spark sql 相关","keywords":"","body":"spark sql 相关 "},"notes/bigdata/storm.html":{"url":"notes/bigdata/storm.html","title":"storm 相关","keywords":"","body":"storm 相关 "},"notes/bigdata/scala.html":{"url":"notes/bigdata/scala.html","title":"scala 相关","keywords":"","body":"scala 相关 "},"notes/bigdata/sqoop.html":{"url":"notes/bigdata/sqoop.html","title":"sqoop 相关","keywords":"","body":"sqoop 相关 一、介绍 sqoop是hadoop和关系数据库服务器之间传送数据的工具。 sqoop 学习 sqoop 架构 sqoop的常用命令 1、sqoop的主要功能 导入、迁入 导入数据：mysql、oracle 导入数据到 hadoop 的 hdfs、hive、hbase 等数据存储系统 导出、迁出 导出数据：从 hadoop 的文件系统中导出数据到关系数据库 mysql 等 2、sqoop 与 hive sqoop: 工具：本质就是迁移数据，迁移的方式：就是把sqoop的迁移命令转换成MR程序 hive： 工具：本质就是执行计算，依赖于HDFS存储数据，把SQL转换成MR程序 与以下组件可能打交道 HDFS、MapReduce、YARN、ZooKeeper、Hive、HBase、MySQL 二、安装部署 1、进入配置文件目录 cd /usr/local/src/sqoop/conf 2、复制环境变量 sqoop-env.sh 文件 cp sqoop-env-template.sh sqoop-env.sh 3、修改配置文件 （vim sqoop-env.sh ） 没用到 Hive 和 HBase 可以不用配置相关项，使用时会弹出警告 export HADOOP_COMMON_HOME=/usr/local/src/hadoop export HADOOP_MAPRED_HOME=/usr/local/src/hadoop export HIVE_HOME=/usr/local/src/hive export ZOOKEEPER_HOME=/usr/local/src/zookeeper export ZOOCFGDIR=/usr/local/src/zookeeper/conf export HBASE_HOME=/usr/local/src/hbase 4、 复制 mysql 数据库驱动包到指定文件路径 cp mysql-connector-java-5.1.46-bin.jar /usr/local/src/sqoop/lib/ 5、验证 sqoop version 6、链接测试 sqoop list-databases --connect jdbc:mysql://hp1:3306/ --username root --password qwe 三、常见使用 1、列出MySQL数据有哪些数据库 sqoop list-databases --connect jdbc:mysql://hp1:3306/ --username root --password qwe 2、列出MySQL中的某个数据库有哪些数据表： sqoop list-tables --connect jdbc:mysql://hp1:3306/mysql --username root --password qwe 3、在hive中创建一张跟mysql中的help_keyword表一样的hive表hk： sqoop create-hive-table \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --hive-table hk 四、Sqoop的数据导入，import 从非关系性数据库（mysql、oracle）导入到关系性数据库（hdfs、hbase、hive），条记录可表示为文本、二进制文件或SequenceFile格式 1、语法格式 sqoop import (generic-args) (import-args) 2、常用参数 --connect JDBC连接符: jdbc:mysql://node1/movie jdbc:oracle:thin:@//ndoe/movie --connection-manager 连接管理者 --hadoop-mapred-home 指定$HADOOP_MAPRED_HOME路径 conf中指定后无需设置 --dirver JDBC驱动器类 比如com.mysql.jdbc.Driver --username 数据库用户 --password 数据库密码 --password-alias Credential provider password alias --password-file 设置用于存放认证的密码信息文件的路径 --table 导出的表名 --where 配合table使用 --target-dir HDFS目录名 --as-textfile --as-parquetfile --as-avrodatafile --as-sequencefile 保存格式，默认text -m, -num-mappers 启动的Map Task数目 任务并行度， 默认1 -e，--query 取数sql --fields-terminated-by 分割符 --verbose 日志 --append 将数据追加到HDFS上一个已存在的数据集上 -P 从命令行输入密码 --password 密码 --username 账号 --verbose 打印流程信息 --connection-param-file 可选参数 3、从mysql中导入到HDFS中 普通导入：导入mysql库中的help_keyword的数据到HDFS上 导入的默认路径：/user/hadoop/help_keyword sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ -m 1 查看导入的文件 hdfs dfs -cat /user/root/help_keyword/part-m-00000 导入： 指定分隔符和导入路径 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --target-dir /user/hp1/my_help_keyword1 \\ --fields-terminated-by '\\t' \\ -m 2 导入数据：带where条件 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --where \"name='STRING' \" \\ --table help_keyword \\ --target-dir /sqoop/hp1/myoutport1 \\ -m 1 查询指定列 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --columns \"name\" \\ --where \"name='STRING' \" \\ --table help_keyword \\ --target-dir /sqoop/hp1/myoutport22 \\ -m 1 selct name from help_keyword where name = \"string\" 导入：指定自定义查询SQL sqoop import \\ --connect jdbc:mysql://hp1:3306/ \\ --username root \\ --password qwe \\ --target-dir /user/hadoop/myimport33_1 \\ --query 'select help_keyword_id,name from mysql.help_keyword where $CONDITIONS and name = \"STRING\"' \\ --split-by help_keyword_id \\ --fields-terminated-by '\\t' \\ -m 4 4、把MySQL数据库中的表数据导入到Hive中 sqoop 导入关系型数据到 hive 的过程是先导入到 hdfs，然后再 load 进入 hive 普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名： sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --hive-import \\ -m 1 导入过程 第一步：导入mysql.help_keyword的数据到hdfs的默认路径 第二步：自动仿造mysql.help_keyword去创建一张hive表, 创建在默认的default库中 第三步：把临时目录中的数据导入到hive表中 查看数据 hdfs dfs -cat /user/hive/warehouse/help_keyword/part-m-00000 指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --fields-terminated-by \"\\t\" \\ --lines-terminated-by \"\\n\" \\ --hive-import \\ --hive-overwrite \\ --create-hive-table \\ --delete-target-dir \\ --hive-database mydb_test \\ --hive-table new_help_keyword 报错原因是hive-import 当前这个导入命令。 sqoop会自动给创建hive的表。 但是不会自动创建不存在的库 手动创建mydb_test数据块 hive> create database mydb_test; OK Time taken: 6.147 seconds hive> 之后再执行上面的语句没有报错 查询一下 select * from new_help_keyword limit 10; 上面的导入语句等价于 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --fields-terminated-by \"\\t\" \\ --lines-terminated-by \"\\n\" \\ --hive-import \\ --hive-overwrite \\ --create-hive-table \\ --hive-table mydb_test.new_help_keyword \\ --delete-target-dir 增量导入 --incremental append --check-column 检查的字段 --last-value 起始字段last-value + 1 执行增量导入之前，先清空hive数据库中的help_keyword表中的数据 truncate table help_keyword; sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --target-dir /user/hadoop/myimport_add \\ --incremental append \\ --check-column help_keyword_id \\ --last-value 500 \\ -m 1 5、把MySQL数据库中的表数据导入到hbase sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --hbase-table new_help_keyword \\ --column-family person \\ --hbase-row-key help_keyword_id 五、Sqoop的数据导出，export 从Hadoop(HDFS、HBase、Hive)导出到关系型数据库(Mysql、Oracle) export连接配置参数同import sqoop export --connect jdbc:mysql://hp1:3306/mysql \\ --table data --export-dir /user/x/data/ \\ --username root \\ --password qwe \\ --update-key id \\ --update-mode allowinsert 1、从 hdfs 导出到 mysql sqoop export \\ --connect jdbc:mysql://hadoop01:3306/test \\ --username hadoop \\ --password qwe \\ --table book \\ --export-dir /sqoopdata \\ --fields-terminated-by ',' 2、从 hive 导出到 mysql sqoop export \\ --connect jdbc:mysql://hadoop01:3306/test \\ --username hadoop \\ --password qwe \\ --table book \\ --export-dir /user/hive/warehouse/uv/dt=2011-08-03 \\ --input-fileds-terminated-by '\\t' 3、从 hbase 导出到 mysql 默认的没有命令直接将hbase中的数据导入到MySQL，因为在hbase中的表数据量通常比较大，如果一次性导入到MySQL，可能导致MySQL直接崩溃。 但是可以用别的办法进行导入： 将 Hbase 数据，扁平化成 HDFS 文件，然后再由 sqoop 导入 将 Hbase 数据导入 Hive 表中，然后再导入 mysql 直接使用 Hbase 的 Java API 读取表数据，直接向 mysql 导入，不需要使用 sqoop "},"notes/bigdata/flink.html":{"url":"notes/bigdata/flink.html","title":"flink 相关","keywords":"","body":"flink 相关 "},"notes/bigdata/flume.html":{"url":"notes/bigdata/flume.html","title":"flume 相关","keywords":"","body":"flume 相关 "},"notes/bigdata/elasticsearch.html":{"url":"notes/bigdata/elasticsearch.html","title":"elasticsearch 相关","keywords":"","body":"elasticsearch 相关 "},"notes/bigdata/etl.html":{"url":"notes/bigdata/etl.html","title":"ETL 相关","keywords":"","body":"ETL 相关 "},"notes/bigdata/mapreduce.html":{"url":"notes/bigdata/mapreduce.html","title":"mapreduce 相关","keywords":"","body":"mapreduce 相关 一、例1 https://www.cnblogs.com/kaituorensheng/p/3826114.html python利用streaming编写mapreducer程序 1、先编写一个map.py # coding: utf8 import sys for line in sys.stdin: line = line.strip() film_d = line.split(\";\") print(film_d[0]) 2、再编写一个red.py： # coding: utf8 import sys cur_film = \"惊天魔盗团2\" cur_count = 0 for line in sys.stdin: if cur_film in line: cur_count += 1 print('%s总共出现了，%s次。' % ( cur_film,cur_count)) 3、运行程序： 1）数据上传 • Hdfs dfs –put ~/dat0203_1.log input/ 2）修改文件属性 • chmod 777 map.py • chmod 777 red.py 3）开始运行 Hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoopstreaming-2.7.1.jar –file ~/map.py –mapper ~/map.py –file ~/red.py –reducer ~/red.py –input input –output output 二、例2 1、mapper.py #!/usr/bin/python # -*- coding: utf-8 -*- import sys # input comes from STDIN (standard input) for line in sys.stdin: # remove leading and trailing whitespace line = line.strip() # split the line into words words = line.split() # increase counters for word in words: # write the results to STDOUT (standard output); # what we output here will be the input for the # Reduce step, i.e. the input for reducer.py # # tab-delimited; the trivial word count is 1 print '%s\\t%s' % (word, 1) 2、recuder.py #!/usr/bin/python # -*- coding: utf-8 -*- from operator import itemgetter import sys current_word = None current_count = 0 word = None # input comes from STDIN for line in sys.stdin: # remove leading and trailing whitespace line = line.strip() # parse the input we got from mapper.py word, count = line.split('\\t', 1) # convert count (currently a string) to int try: count = int(count) except ValueError: # count was not a number, so silently # ignore/discard this line continue # this IF-switch only works because Hadoop sorts map output # by key (here: word) before it is passed to the reducer if current_word == word: current_count += count else: if current_word: # write result to STDOUT print '%s\\t%s' % (current_word, current_count) current_count = count current_word = word # do not forget to output the last word if needed! if current_word == word: print '%s\\t%s' % (current_word, current_count) 3、test.txt hello hello world world hi hi world 4、运行程序 chmod 777 mapper.py chmod 777 reducer.py ls / root@hadoop1:~# ls mapper.py reducer.py test.txt hdfs dfs -put test.txt / hadoop jar /opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar ©¦ -file mapper.py -mapper mapper.py -file reducer.py -reducer reducer.py -input /test.txt -outp©¦ ut /output hdfs dfs -cat /output/* "},"notes/bigdata/numpy.html":{"url":"notes/bigdata/numpy.html","title":"numpy 相关","keywords":"","body":"numpy 相关 http://www.runoob.com/numpy/numpy-tutorial.html https://blog.csdn.net/qq351469076/article/details/78817378 一、安装 pip install numpy import numpy as np 二、使用 1、创建0-23, 共2个三行四列的数组 a = np.arange(24).reshape((2,3,4)) array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) data = np.arange(28).reshape(7,4) import numpy as np a=np.array([1,2,3]) print(a) [1 2 3] 2、多于一个维度 import numpy as np a = np.array([[1, 2], [3, 4]]) print (a) [[1 2] [3 4]] import numpy as np a = np.arange(24) print (a.ndim) # a 现只有一个维度 3、现在调整其大小 #个数，行，列 b = a.reshape(2,4,3) # b 现在拥有三个维度 print (b.ndim) a = np.array([[1,2,3],[4,5,6]]) print(a) [[1 2 3] [4 5 6]] print (a.shape) #2列，3行 （2,3） #维度（秩） print (len(a.shape)) 2 a = np.array([[1,2,3],[4,5,6]]) a.shape = (3,2) print (a) 输出结果为： [[1 2] [3 4] [5 6]] 4、NumPy 也提供了 reshape 函数来调整数组大小 a = np.array([[1,2,3],[4,5,6]]) b = a.reshape(3,2) print (b) 输出结果为： [[1, 2] [3, 4] [5, 6]] 5、numpy.empty 方法用来创建一个指定形状（shape）、数据类型（dtype）且未初始化的数组： x = np.empty([3,2], dtype = int) print (x) 6、numpy.zeros，创建指定大小的数组，数组元素以 0 来填充： numpy.zeros(shape, dtype = float, order = 'C') y = np.zeros((5,), dtype = np.int) print(y) [0 0 0 0 0] 7、numpy.ones，创建指定形状的数组，数组元素以 1 来填充： numpy.ones(shape, dtype = None, order = 'C') 8、将列表转换为 ndarray: x = [1,2,3] a = np.asarray(x) print (a) [1 2 3] 9、将元组转换为 ndarray: x = (1,2,3) a = np.asarray(x) print (a) 10、将元组列表转换为 ndarray: x = [(1,2,3),(4,5)] a = np.asarray(x) print (a) [(1, 2, 3) (4, 5)] 11、设置了 dtype 参数： x = [1,2,3] a = np.asarray(x, dtype = float) print (a) [1. 2. 3.] 12、NumPy 从数值范围创建数组，numpy 包中的使用 arange 函数创建数值范围并返回 ndarray 对象，函数格式如下： numpy.arange(start, stop, step, dtype) x = np.arange(10,20,2) print (x) [10 12 14 16 18] 13、分片，索引 a = np.arange(10) s = slice(2,7,2) # 从索引 2 开始到索引 7 停止，间隔为2 print (a[s]) [2 4 6] a = np.arange(10) b = a[2:7:2] # 从索引 2 开始到索引 7 停止，间隔为 2 print(b) a = np.arange(10) print(a[2:]) a = np.array([[1,2,3],[3,4,5],[4,5,6]]) print (a[...,1]) # 第2列元素 print (a[1,...]) # 第2行元素 print (a[...,1:]) # 第2列及剩下的所有元素 a[...,1：] ...省略行的索引，索引为列的索引，索引为1的行开始到最后 x = np.array([[1, 2], [3, 4], [5, 6]]) y = x[[0,1,2], [0,1,0]] print (y) [1 4 5] x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) rows = np.array([[0,0],[3,3]]) cols = np.array([[0,2],[0,2]]) y = x[rows,cols] print (y) [[ 0 2] [ 9 11]] a = np.array([[1,2,3], [4,5,6],[7,8,9]]) b = a[1:3, 1:3] c = a[1:3,[1,2]] d = a[...,1:] print(b) print(c) print(d) 输出结果为： [[5 6] [8 9]] [[5 6] [8 9]] [[2 3] [5 6] [8 9]] x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) print (x[x > 5]) 大于 5 的元素是： [ 6 7 8 9 10 11] add() 对两个数组的逐个字符串元素进行连接 multiply() 返回按元素多重连接后的字符串 center() 居中字符串 capitalize() 将字符串第一个字母转换为大写 title() 将字符串的每个单词的第一个字母转换为大写 lower() 数组元素转换为小写 upper() 数组元素转换为大写 split() 指定分隔符对字符串进行分割，并返回数组列表 splitlines() 返回元素中的行列表，以换行符分割 strip() 移除元素开头或者结尾处的特定字符 join() 通过指定分隔符来连接数组中的元素 replace() 使用新字符串替换字符串中的所有子字符串 decode() 数组元素依次调用str.decode encode() 数组元素依次调用str.encode "},"notes/bigdata/pandas.html":{"url":"notes/bigdata/pandas.html","title":"pandas 相关","keywords":"","body":"pandas 相关 一、安装 pip install pandas matplotlib -i https://pypi.tuna.tsinghua.edu.cn/simple ImportError: No module named Tkinter yum install -y tkinter tk-devel apt install python-tk 二、基本使用 Series方法与DataFrame差不多，这里只介绍后者如何使用，前者相似。 df = pd.DataFrame(np.arange(12).reshape(3,4),columns=['A', 'B', 'C', 'D']) In [4]: df Out[4]: A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 axis=1（按列方向操作）、inplace=True（修改完数据，在原数据上保存） 1、按标签来删除列 df.drop(['B','C'],axis=1,inplace=True) A D 0 0 3 1 4 7 2 8 11 2、按序号来删除列 x = [1,2] #删除多列需给定列表，否则参数过多 df.drop(df.columns[x],axis=1,inplace=True) A D 0 0 3 1 4 7 2 8 11 3、按序号来删除行 df.drop([0,1],inplace=True) #默认axis=0 A B C D 2 8 9 10 11 使用Pandas&NumPy进行数据清洗的6大常用方法 import pandas as pd import numpy as np df = pd.DataFrame({\"id\":[1001,1002,1003,1004,1005,1006], \"date\":pd.date_range('20130102', periods=6), \"city\":['Beijing ', 'SH', ' guangzhou ', 'Shenzhen', 'shanghai', 'BEIJING '], \"age\":[23,44,54,32,34,32], \"category\":['100-A','100-B','110-A','110-C','210-A','130-F'], \"price\":[1200,np.nan,2133,5433,np.nan,4432]}, columns =['id','date','city','category','age','price']) 4、数据表信息查看 查看维度 df.shape 数据表基本信息（维度、列名称、数据格式、所占空间等） df.info() 每一列数据的格式 df.dtypes 某一列格式 df['id'].dtype 空值 df.isnull() 查看某一列的唯一值 df['id'].unique() 查看数据表的值 df.values 查看列名称 df.columns 查看前10行数据、后10行数据 df.head() #默认前10行数据 df.tail() #默认后10 行数据 5、数据表清洗 用数字0填充空值 df.fillna(value=0) 使用列prince的均值对NA进行填充 df['prince'].fillna(df['prince'].mean()) 清楚city字段的字符空格 df['city']=df['city'].map(str.strip) 大小写转换 df['city']=df['city'].str.lower() 更改数据格式 df['price'].astype('int') 更改列名称 df.rename(columns={'category': 'category-size'}) 删除后出现的重复值 df['city'].drop_duplicates() 删除先出现的重复值 df['city'].drop_duplicates(keep='last') 数据替换 df['city'].replace('sh', 'shanghai') 返回的是个true或false的Series对象（掩码对象），进而筛选出我们需要的特定数据。 df[df.isnull()] df[df.notnull()] 将所有含有nan项的row删除 df.dropna() 将在列的方向上三个为NaN的项删除 df.dropna(axis=1,thresh=3) 将全部项都是nan的row删除 df.dropna(how='ALL') 此处：print data.dropna() 和 print data[data.notnull()] 结果一样 填充无效值 df.fillna(0) 对第一列nan值赋0，第二列赋值0.5 df.fillna({1:0, 2:0.5}) 在列方向上以前一个值作为值赋给NaN df.fillna(method='ffill') 删除行 print frame.drop(['a']) 删除列，drop函数默认删除行，列需要加axis = 1 print frame.drop(['Ohio'], axis = 1) 6、数据预处理 df1=pd.DataFrame({\"id\":[1001,1002,1003,1004,1005,1006,1007,1008], \"gender\":['male','female','male','female','male','female','male','female'], \"pay\":['Y','N','Y','Y','N','Y','N','Y',], \"m-point\":[10,12,20,40,40,40,30,20]}) 数据表合并 df_inner=pd.merge(df,df1,how='inner') # 匹配合并，交集 df_left=pd.merge(df,df1,how='left') # df_right=pd.merge(df,df1,how='right') df_outer=pd.merge(df,df1,how='outer') #并集 设置索引列 df_inner.set_index('id') 按照特定列的值排序 df_inner.sort_values(by=['age']) 按照索引列排序 df_inner.sort_index() 如果prince列的值>3000，group列显示high，否则显示low df_inner['group'] = np.where(df_inner['price'] > 3000,'high','low') 对复合多个条件的数据进行分组标记 df_inner.loc[(df_inner['city'] == 'beijing') & (df_inner['price'] >= 4000), 'sign']=1 对category字段的值依次进行分列，并创建数据表，索引值为df_inner的索引列，列名称为category和size pd.DataFrame((x.split('-') for x in df_inner['category']),index=df_inner.index,columns=['category','size'])) 将完成分裂后的数据表和原df_inner数据表进行匹配 df_inner=pd.merge(df_inner,split,right_index=True, left_index=True) 7、数据提取 主要用到的三个函数：loc,iloc和ix，loc函数按标签值进行提取，iloc按位置进行提取，ix可以同时按标签和位置进行提取。 按索引提取单行的数值 df_inner.loc[3] 按索引提取区域行数值 df_inner.iloc[0:5] 重设索引 df_inner.reset_index() 设置日期为索引 df_inner=df_inner.set_index('date') 提取4日之前的所有数据 df_inner[:'2013-01-04'] 使用iloc按位置区域提取数据 df_inner.iloc[:3,:2] #冒号前后的数字不再是索引的标签名称，而是数据所在的位置，从0开始，前三行，前两列。 适应iloc按位置单独提起数据 df_inner.iloc[[0,2,5],[4,5]] #提取第0、2、5行，4、5列 使用ix按索引标签和位置混合提取数据 df_inner.ix[:'2013-01-03',:4] #2013-01-03号之前，前四列数据 判断city列的值是否为北京 df_inner['city'].isin(['beijing']) 判断city列里是否包含beijing和shanghai，然后将符合条件的数据提取出来 df_inner.loc[df_inner['city'].isin(['beijing','shanghai'])] 提取前三个字符，并生成数据表 pd.DataFrame(category.str[:3]) 8、数据筛选 使用与、或、非三个条件配合大于、小于、等于对数据进行筛选，并进行计数和求和。 使用“与”进行筛选 df_inner.loc[(df_inner['age'] > 25) & (df_inner['city'] == 'beijing'), ['id','city','age','category','gender']] 使用“或”进行筛选 df_inner.loc[(df_inner['age'] > 25) | (df_inner['city'] == 'beijing'), ['id','city','age','category','gender']].sort(['age']) 使用“非”条件进行筛选 df_inner.loc[(df_inner['city'] != 'beijing'), ['id','city','age','category','gender']].sort(['id']) 对筛选后的数据按city列进行计数 df_inner.loc[(df_inner['city'] != 'beijing'), ['id','city','age','category','gender']].sort(['id']).city.count() 使用query函数进行筛选 df_inner.query('city == [\"beijing\", \"shanghai\"]') 对筛选后的结果按prince进行求和 df_inner.query('city == [\"beijing\", \"shanghai\"]').price.sum() 9、数据汇总 主要函数是groupby和pivote_table 对所有的列进行计数汇总 df_inner.groupby('city').count() 按城市对id字段进行计数 df_inner.groupby('city')['id'].count() 对两个字段进行汇总计数 df_inner.groupby(['city','size'])['id'].count() 对city字段进行汇总，并分别计算prince的合计和均值 df_inner.groupby('city')['price'].agg([len,np.sum, np.mean]) 10、数据统计 数据采样，计算标准差，协方差和相关系数 简单的数据采样 df_inner.sample(n=3) 手动设置采样权重 weights = [0, 0, 0, 0, 0.5, 0.5] df_inner.sample(n=2, weights=weights) 采样后不放回 df_inner.sample(n=6, replace=False) 采样后放回 df_inner.sample(n=6, replace=True) 数据表描述性统计 df_inner.describe().round(2).T #round函数设置显示小数位，T表示转置 计算列的标准差 df_inner['price'].std() 计算两个字段间的协方差 df_inner['price'].cov(df_inner['m-point']) 数据表中所有字段间的协方差 df_inner.cov() 两个字段的相关性分析 df_inner['price'].corr(df_inner['m-point']) #相关系数在-1到1之间，接近1为正相关，接近-1为负相关，0为不相关 数据表的相关性分析 df_inner.corr() 11、数据输出 分析后的数据可以输出为xlsx格式和csv格式 写入Excel df_inner.to_excel('excel_to_python.xlsx', sheet_name='bluewhale_cc') 写入到CSV df_inner.to_csv('excel_to_python.csv') 三、实例 # -*- coding: UTF-8 -*- import pandas as pd name = [] score = [] with open('film_log3.csv','r') as film_data: for i in film_data: name.append(i.split(';')[0]) score.append(i.split(';')[7]) #print name[0] movie = pd.DataFrame({'nameinfo':name,'scoreinfo':score}) print movie.head() movie = pd.read_csv('film_log3.csv',prefix='tmp',header=None) #print movie movie_split = pd.DataFrame((x.split(';') for x in movie.tmp0),index=movie.index,columns=['name','2','3','4','5','6','7','score','9']) movie = movie_split.ix[:,[0,7]] #movie.to_csv('out.csv') #print movie[0:1] #[df['列名'].isin([相应的值])] print movie "},"notes/bigdata/pyhdfs.html":{"url":"notes/bigdata/pyhdfs.html","title":"pyhdfs 相关","keywords":"","body":"pyhdfs 相关 在HDFS中，要实现对文件的操作，一般可以在shell中发送指令完成，但这样太麻烦了。 当然我们可以调用HDFS的API，这里我们可以使用python的pyHdfs库来实现对HDFS的文件操作。 1、安装pyhdfs apt -y install python-pip pip install -U pip pip install PyHDFS 2、调用方法，详细的可见官方文档 pyhdfs from pyhdfs import HdfsClient client = HdfsClient(hosts='hadoop3:50070') print client.list_status('/') help(client) help(client.listdir) dir(client) ['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_delete', '_get', '_last_time_recorded_active', '_parse_hosts', '_parse_path', '_post', '_put', '_record_last_active', '_request', '_requests_kwargs', '_requests_session', 'append', 'concat', 'copy_from_local', 'copy_to_local', 'create', 'create_snapshot', 'create_symlink', 'delete', 'delete_snapshot', 'exists', 'get_active_namenode', 'get_content_summary', 'get_file_checksum', 'get_file_status', 'get_home_directory', 'get_xattrs', 'hosts', 'list_status', 'list_xattrs', 'listdir', 'max_tries', 'mkdirs', 'open', 'randomize_hosts', 'remove_xattr', 'rename', 'rename_snapshot', 'retry_delay', 'set_owner', 'set_permission', 'set_replication', 'set_times', 'set_xattr', 'timeout', 'user_name', 'walk'] copy_from_local(self, localsrc, dest, **kwargs) copy_to_local(self, src, localdest, **kwargs) create(self, path, data, **kwargs) delete(self, path, **kwargs) get_active_namenode(self, max_staleness=None) get_content_summary(self, path, **kwargs) get_file_status(self, path, **kwargs) get_home_directory(self, **kwargs) list_status(self, path, **kwargs) List the statuses of the files/directories in the given path if the path is a directory. listdir(self, path, **kwargs) mkdirs(self, path, **kwargs) open(self, path, **kwargs) rename(self, path, destination, **kwargs) "},"notes/bigdata/reptile.html":{"url":"notes/bigdata/reptile.html","title":"reptile 爬虫相关","keywords":"","body":"reptile 爬虫相关 "},"notes/bigdata/pyecharts.html":{"url":"notes/bigdata/pyecharts.html","title":"pyecharts 相关","keywords":"","body":"pyecharts 相关 1、爬取链家使用pyecharts显示 # coding: utf-8 import urllib2 as ulib import re import pandas as pd import numpy as np import json from pyecharts import Bar from pyecharts import Line url = \"https://cd.lianjia.com/ershoufang/\" headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36'} r = ulib.Request(url,headers = headers) data = ulib.urlopen(r) data = data.read().decode('utf-8') all_re = '([\\s\\S]*?)' name_re = 'data-el=\"ershoufang\">([^' other_re = 'info\">([\\s\\S]*?)' all_infos = re.findall(all_re, data) price_infos = [] name_infos = [] other_infos = [] for info in all_infos: price = re.findall(price_re,info)[0] price_infos.append(price) name = re.findall(name_re,info)[0] name_infos.append(name) other = re.findall(other_re,info)[0].replace('/','|') other_infos.append(other) house = pd.DataFrame({'name':name_infos,'price':price_infos,'other':other_infos}) #对房源信息进行分列 other_split = pd.DataFrame((x.split('|') for x in house.other),index=house.index,columns=['address','house_type','area','direction','decoration','elevator']) house = pd.merge(house,other_split,left_index=True, right_index=True) house.drop(house.columns[1],axis=1,inplace=True) house['area'].replace('[^0-9.]', '',regex=True,inplace=True) print(house) data={\"name\":list(house[\"name\"]),\"price\":list(house[\"price\"].astype('int')),\"area\":list(house[\"area\"])} # # pip install pyecharts bar = Line(\"house info\", \"这里是副标题\") price_array = house[\"price\"].astype('int') area_array = house[\"area\"].astype('float') # bar.add(\"price\",range(len(price_array)), price_array) bar.add(\"price\",list(house[\"name\"]), price_array) bar.add(\"area\",list(house[\"name\"]), area_array) # # bar.print_echarts_options() # 该行只为了打印配置项，方便调试时使用 bar.render() # 生成本地 HTML 文件 "},"notes/bigdata/matplotlib.html":{"url":"notes/bigdata/matplotlib.html","title":"matplotlib 相关","keywords":"","body":"matplotlib 相关 一、matplotlib简介 matplotlib 是python最著名的绘图库，它提供了一整套和matlab相似的命令API，十分适合交互式地行制图。而且也可以方便地将它作为绘图控件，嵌入GUI应用程序中。 它的文档相当完备，并且Gallery页面中有上百幅缩略图，打开之后都有源程序。因此如果你需要绘制某种类型的图，只需要在这个页面中浏览/复制/粘贴一下，基本上都能搞定。 在Linux下比较著名的数据图工具还有gnuplot，这个是免费的，Python有一个包可以调用gnuplot，但是语法比较不习惯，而且画图质量不高。 而 Matplotlib则比较强：Matlab的语法、python语言、latex的画图质量（还可以使用内嵌的latex引擎绘制的数学公式）。 https://blog.csdn.net/Notzuonotdied/article/details/77876080 https://blog.csdn.net/ScarlettYellow/article/details/80458797 二、柱状图 plt.bar(X, +Y1, facecolor='#9999ff', edgecolor='white') 三、折线图 绘制折线图教程 1、line chart import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 2 * np.pi, 100) y1, y2 = np.sin(x), np.cos(x) plt.plot(x, y1) plt.plot(x, y2) plt.title('line chart') plt.xlabel('x') plt.ylabel('y') plt.show() 2、图例，在plot的时候指定label，然后调用legend方法可以绘制图例。例如： import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 2 * np.pi, 100) y1, y2 = np.sin(x), np.cos(x) plt.plot(x, y1, label='y = sin(x)') plt.plot(x, y2, label='y = cos(x)') plt.legend() plt.show() legend方法可接受一个loc关键字参数来设定图例的位置，可取值为数字或字符串： 0: ‘best' 1: ‘upper right' 2: ‘upper left' 3: ‘lower left' 4: ‘lower right' 5: ‘right' 6: ‘center left' 7: ‘center right' 8: ‘lower center' 9: ‘upper center' 10: ‘center' 3、线的样式 颜色 plot方法的关键字参数color(或c)用来设置线的颜色。可取值为： 1、颜色名称或简写 b: blue g: green r: red c: cyan m: magenta y: yellow k: black w: white 2、#rrggbb 3、(r, g, b) 或 (r, g, b, a)，其中 r g b a 取均为[0, 1]之间 4、[0, 1]之间的浮点数的字符串形式，表示灰度值。0表示黑色，1表示白色 样式 plot方法的关键字参数linestyle(或ls)用来设置线的样式。可取值为： -, solid --, dashed -., dashdot :, dotted '', ' ', None 粗细 设置plot方法的关键字参数linewidth(或lw)可以改变线的粗细，其值为浮点数。 import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 2 * np.pi, 100) y1, y2 = np.sin(x), np.cos(x) plt.plot(x, y1, c='r', ls='--', lw=3) plt.plot(x, y2, c='#526922', ls='-.') plt.show() marker 以下关键字参数可以用来设置marker的样式： marker markeredgecolor 或 mec markeredgewidth 或 mew markerfacecolor 或 mfc markerfacecoloralt 或 mfcalt markersize 或 ms 其中marker可取值为： '.': point marker ',': pixel marker 'o': circle marker 'v': triangle_down marker '^': triangle_up marker '': triangle_right marker '1': tri_down marker '2': tri_up marker '3': tri_left marker '4': tri_right marker 's': square marker 'p': pentagon marker '*': star marker 'h': hexagon1 marker 'H': hexagon2 marker '+': plus marker 'x': x marker 'D': diamond marker 'd': thin_diamond marker '|': vline marker '_': hline marker 例如： import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 2 * np.pi, 10) y1, y2 = np.sin(x), np.cos(x) plt.plot(x, y1, marker='o', mec='r', mfc='w') plt.plot(x, y2, marker='*', ms=10) plt.show() 另外，marker关键字参数可以和color以及linestyle这两个关键字参数合并为一个字符串。例如： import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 2 * np.pi, 10) y1, y2 = np.sin(x), np.cos(x) plt.plot(x, y1, 'ro-') plt.plot(x, y2, 'g*:', ms=10) plt.show() 4、创建图表 plt.figure(1) left(表示直方图开始位置) height(直方图的高度) width(直方图宽度) yerr(防止直方图触顶) color(直方图颜色) rects =plt.bar(left = (1,2,3),height = (1,2,3),color=('r','g','b'),width = 0.5,align=\"center\",yerr=0.000001) #x1=[0,2,4] x1=range(3) y1=[100,230,60] #x1=range(0,10) rects1 =plt.bar(left = x1,height = y1,color=('g'),label=(('no1')),width = 0.5,align=\"center\",yerr=0.000001) #rects1 =plt.bar(left = (0.2),height = (0.5),color=('g'),label=(('no1')),width = 0.2,align=\"center\",yerr=0.000001) #rects2 =plt.bar(left = (1),height = (1),color=('r'),label=(('no2')),width = 0.2,align=\"center\",yerr=0.000001) 5、直方图上显示具体数字（自动编号） def autolabel(rects): for rect in rects: height = rect.get_height() plt.text(rect.get_x()+rect.get_width()/2., 1.03*height, '%s' % float(height)) autolabel(rects1) #autolabel(rects2) 6、直方图脚注 plt.xticks(x1,('a','b','c')) plt.title('周平均票房'.decode('utf-8')) plt.xlabel('电影名称'.decode('utf-8')) plt.ylabel('票房收入(万元)'.decode('utf-8')) #图注 plt.legend() plt.show() 7、实例 # -*- coding: UTF-8 -*- import matplotlib.pyplot as plt x1=range(0,10) y1=[10,13,5,40,30,60,70,12,55,25] plt.plot(x1,y1,label='A',linewidth=2,color='r',marker='o',markerfacecolor='yellow',markersize=4) #plt.plot(x1,y1,label='A',linewidth=3,color='r',marker='o', markerfacecolor='b',markersize=12) x2=range(0,10) y2=[5,8,0,30,20,40,50,10,40,15] plt.plot(x2,y2,label='B',color='black',ls='-.') x3=range(0,10) y3=[1,60,10,3,2,40,20,10,40,5] plt.plot(x3,y3,label='C',color='blue') plt.xlabel('time') plt.ylabel('income') plt.title('my Graph') plt.legend() plt.show() "},"notes/ai/langgraph.html":{"url":"notes/ai/langgraph.html","title":"LangGraph","keywords":"","body":"LangGraph 一、基础环境 uv venv -p 3.13 source .venv/Scripts/activate uv pip install --upgrade \"langgraph-cli[inmem]\" mkdir demo langgraph new ./demo --template react-agent-python cd demo uv pip install -e . touch .env uv pip install langgraph langchain-community langchain-openai tavily-python export TAVILY_API_KEY=\"\" export LANGCHAIN_TRACING_V2=\"true\" export LANGCHAIN_API_KEY=\"\" uv pip install langchain-deepseek os.environ[\"DEEPSEEK_API_KEY\"] uv pip install \"langgraph-cli[inmem]\" uv pip install dashscope "},"notes/ai/mcp_functioncall.html":{"url":"notes/ai/mcp_functioncall.html","title":"MCP And Function Call","keywords":"","body":"MCP And Function Call 一、对比 对比维度 MCP（Model Context Protocol） Function Call 架构设计 客户端—服务器模式，采用标准的JSON-RPC协议，实现完全解耦的模块化架构 集成于模型内部，多为垂直定制，通常直接嵌入模型调用逻辑 通信模式 同步与异步均支持，灵活调整消息传递方式 主要以同步模式为主，异步支持有限，依赖于平台扩展实现 标准化程度 开放标准，支持跨平台工具调用，一次开发，众多模型共用接口 供应商专用，适用范围受限，每个新工具需单独开发接口，标准化程度不高 部署方式 工具方提供MCP服务器，采用统一协议即可接入所有支持MCP的AI模型 每个API或工具都需在各自模型中集成，需定制开发，不具通用性 安全与权限 内嵌多层安全机制（如OAuth认证、动态权限映射和数据加密），上下文持续安全传输 安全机制分散，依赖开发者自行实现，缺乏统一认证流程 扩展性 模块化设计，新增工具只需实现MCP接口即可，不需多次适配 每增加一个工具都需要重新设计和实现接口，存在重复开发问题 二、环境初始化 uv init mcp -p 3.10 # uv init -p 3.10 # uv venv cd mcp source .venv/Scripts/activate # uv add mcp[cli] httpx pypinyin # server # uv add openai lxml # client 三、编码 git clone https://gitee.com/hlyani/mcp.git 1、stdio cd mcp uv --directory weather/client run stdio.py { \"mcpServers\": { \"weather_stdio\": { \"disabled\": false, \"timeout\": 60, \"command\": \"uv\", \"args\": [ \"--directory\", \"C:\\\\harley\\\\BaiduSyncdisk\\\\sugon\\\\vscode_workspace\\\\mcp\\\\weather\\\\server\", \"run\", \"stdio.py\" ], \"transportType\": \"stdio\" } } } 2、sse cd mcp uv run --directory weather/server sse.py uv --directory weather/client run sse.py { \"mcpServers\": { \"weather_sse\": { \"disabled\": false, \"timeout\": 60, \"url\": \"http://localhost:8000/sse\", \"transportType\": \"sse\" } } } "},"notes/ai/ollama_vllm.html":{"url":"notes/ai/ollama_vllm.html","title":"ollama and vllm","keywords":"","body":"Ollama and vLLM Ollama适用于开发测试，vLLM适用于生产环境部署。 性能对比： 吞吐量 ： vLLM通过连续批处理和内存优化，显著高于Ollama，尤其在高并发时差异更明显。 资源占用 ： Ollama在单机环境下资源占用更低，启动更快；vLLM需要更多初始配置但能更好地利用多卡资源。 延迟 ： Ollama在实时响应场景中延迟更低，而vLLM通过批处理优化可平衡延迟与吞吐。 Ollama 一、部署 https://ollama.com 1.部署 curl -fsSL https://ollama.com/install.sh | sh 2.基本命令 Usage: ollama [flags] ollama [command] Available Commands: serve 启动 Ollama 服务 create 从 Modelfile 创建一个模型 show 查看模型详细信息 run 运行一个模型 stop 停止正在运行的模型 pull 从注册表拉取一个模型 push 将一个模型推送到注册表 list 列出所有可用的模型 ps 列出当前正在运行的模型 cp 复制一个模型 rm 删除一个模型 help 获取关于任何命令的帮助信息 Flags: -h, --help helpfor ollama -v, --version Show version information 3.拉取模型 https://ollama.com/library/deepseek-r1:7b ollama pull deepseek-r1:7b 4.启动模型 ENV OLLAMA_HOST=0.0.0.0:11434 ollama serve ollama run deepseek-r1:7b vLLM 一、部署 https://docs.vllm.ai 1.安装uv https://docs.astral.sh/uv/getting-started/installation/ curl -LsSf https://astral.sh/uv/install.sh | sh 2.安装vLLM https://docs.vllm.ai/en/latest/getting_started/installation.html uv venv vllm --python 3.12 --seed source vllm/bin/activate uv pip install vllm 3.启动 https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B?local-app=vllm env OPENAI_API_KEY=123456 vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" 4.访问 http://0.0.0.0:8000/docs 客户端工具 CherryStudio (https://docs.cherry-ai.com/cherrystudio/download) open-webui (https://github.com/open-webui/open-webui) 一、open-webui version: '3' services: open-webui: image: ghcr.io/open-webui/open-webui:main # 镜像略大，下载情况根据你网速 ports: - \"3000:8080\" environment: - OPENAI_API_KEY=123456 # api key - OPENAI_API_BASE_URL=http://你服务器地址:8000/v1/ # vllm服务地址 volumes: - ./data:/app/backend/data # 挂载数据卷（根据项目需求调整路径） restart: always docker-compose up -d http://0.0.0.0:3000 "},"notes/ai/ML.html":{"url":"notes/ai/ML.html","title":"ML","keywords":"","body":"ML 一、ddp、dp、tp 1、Data Parallelism (DP) ​ DP 将整个模型复制到多个 GPU 上，并将训练数据分块，分配给每个 GPU。这些 GPU 使用相同的模型副本进行前向和后向计算，最后将梯度聚合并更新主模型参数。 ​ 数据并行模式会在每个worker之上复制一份模型，这样每个worker都有一个完整模型的副本。输入数据集是分片的，一个训练的小批量数据将在多个worker之间分割；worker定期汇总它们的梯度，以确保所有worker看到一个一致的权重版本。对于无法放进单个worker的大型模型，人们可以在模型之中较小的分片上使用数据并行。 数据并行扩展通常效果很好，但有两个限制： 超过某一个点之后，每个GPU的batch size变得太小，这降低了GPU的利用率，增加了通信成本； 可使用的最大设备数就是batch size，限制了可用于训练的加速器数量。 2、Distributed Data Parallel (DDP) ​ DDP 是一种分布式数据并行实现，通常是 PyTorch 中的 torch.nn.parallel.DistributedDataParallel。与普通的 DP 类似，DDP 将数据分布到多个 GPU 上，每个 GPU 上都有模型副本并进行训练。 3、Tensor Parallelism (TP) ​ TP 将一个模型的权重矩阵（如神经网络层）拆分成多个块，分布到不同的 GPU 上。每个 GPU 只负责处理部分张量的计算，输出的部分结果再合并在一起。 ​ 使用一些内存管理技术，如激活检查点（activation checkpointing）来克服数据并行的这种限制，也会使用模型并行来对模型进行分区来解决这两个挑战，使得权重及其关联的优化器状态不需要同时驻留在处理器上。 ​ 模型并行模式会让一个模型的内存和计算分布在多个worker之间，以此来解决一个模型在一张卡上无法容纳的问题，其解决方法是把模型放到多个设备之上。 模型并行分为两种：流水线并行和张量并行，就是把模型切分的方式。 流水线并行（pipeline model parallel）是把模型不同的层放到不同设备之上，比如前面几层放到一个设备之上，中间几层放到另外一个设备上，最后几层放到第三个设备之上。 张量并行则是层内分割，把某一个层做切分，放置到不同设备之上，也可以理解为把矩阵运算分配到不同的设备之上，比如把某个矩阵乘法切分成为多个矩阵乘法放到不同设备之上 二、概念 Epoch： 所有的样本数据都输入到模型中，称为一个epoch Iteration： 一个Batch的样本输入到模型中，称为一个Iteration Batchsize： 一个批次的大小，一个Epoch=Batchsize*Iteration torchrun作为PyTorch官方提供的分布式训练工具。torchrun通过简单的命令行接口，使得用户可以轻松地启动多进程训练，极大地降低了入门门槛。 accelerate则是Hugging Face推出的一款分布式训练工具，特别适合使用Hugging Face生态系统的用户。accelerate支持多种后端，包括PyTorch、TensorFlow等，具有很高的灵活性和可扩展性。 deepspeed是由微软开发的一款高性能分布式训练库，特别适合对性能和训练规模有更高要求的用户。它提供了丰富的优化功能，如混合精度训练、梯度累积、ZeRO优化等，能够显著提高训练速度和内存利用率。deepspeed在大规模模型训练中表现出色，被广泛应用于诸如BERT、GPT-3等大型语言模型的训练。 Megatron则是由NVIDIA开发的一款专为大规模模型训练设计的分布式训练框架。它通过模型并行和数据并行相结合的方式，实现了高效的分布式训练。Megatron在处理超大规模模型时表现出色，能够充分利用多GPU和多节点的计算资源，显著缩短训练时间。 ZeRO (Zero Redundancy Optimizer) 零冗余优化器 (Zero Redundancy Optimizer，简称ZeRO，是微软DeepSpeed库的核心)。是一种用于大规模分布式深度学习模型训练的优化方法，由微软的 DeepSpeed 提出。其主要目标是通过优化显存利用率，实现超大规模模型（如数百亿或数万亿参数）的高效训练。ZeRO 通过减少冗余的数据存储和计算需求，使得在多 GPU 系统上高效训练成为可能。 大模型单位： billion: 十亿 million: 百万 “10b”、“13b”、\"70b\"等术语通常指的是大型神经网络模型的参数数量。其中的 “b” 代表 “billion”，也就是十亿。表示模型中的参数量，每个参数用来存储模型的权重和偏差等信息。例如： “10b” 意味着模型有大约 100 亿个参数。 “13b” 意味着模型有大约 130 亿个参数。 “70b” 意味着模型有大约 700 亿个参数。 1.模型权重 模型权重是模型参数中的一部分，通常是指神经网络中连接权重（weights）。这些权重决定了输入特征与网络层之间的连接强度，以及在前向传播过程中特征的传递方式。所以模型 2.梯度 在训练过程中，计算梯度用于更新模型参数。梯度与模型参数的维度相同。 3.优化器参数 一些优化算法（如带有动量的优化器）需要保存一些状态信息，以便在每次更新时进行调整。这些状态信息也会占用一定的显存。 比如： 采用 AdamW 优化器：每个参数占用8个字节，需要维护两个状态。意味着优化器所使用的显存量是模型权重的 2 倍； 采用 经过 bitsandbytes 优化的 AdamW 优化器：每个参数占用2个字节，相当于权重的一半； 采用 SGD 优化器：占用显存和模型权重一样。 4.输入数据和标签 训练模型需要将输入数据和相应的标签加载到显存中。这些数据的大小取决于每个批次的样本数量以及每个样本的维度。 5.中间计算 在前向传播和反向传播过程中，可能需要存储一些中间计算结果，例如激活函数的输出、损失值等。 6.临时缓冲区 在计算过程中，可能需要一些临时缓冲区来存储临时数据，例如中间梯度计算结果等。减少中间变量也可以节省显存，这就体现出函数式编程语言的优势了。 7.硬件和依赖库的开销 显卡或其他硬件设备以及使用的深度学习框架在进行计算时也会占用一些显存。 IB ibstat ibstatus ibv_devinfo ibv_devices rdma link mlnx_perf -i ib0 | grep rdma resources: requests: rdma/rdma_shared_device_a: 1 limits: rdma/rdma_shared_device_a: 1 securityContext: capabilities: add: - IPC_LOCK GPU nvidia-smi pmon nvidia-smi -r docker run -it --rm \\ -v /hl/megatron-deepspeed-llama2/megatron-deepspeed-llama2:/root/megatron-deepspeed-llama2 \\ -w /root/megatron-deepspeed-llama2 \\ -v /opt/hyhal:/opt/hyhal \\ --device=/dev/kfd \\ --device=/dev/dri \\ --security-opt seccomp=unconfined \\ --cap-add=SYS_PTRACE \\ --shm-size=16G \\ --net=host \\ megatron-deepspeed-llama2 \\ bash pytorch python -c \"import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())\" NCCL python -c \"import torch.distributed as dist; print(dist.is_nccl_available())\" "},"notes/ai/tensorflow_and_keras.html":{"url":"notes/ai/tensorflow_and_keras.html","title":"tensorflow 和 keras 相关","keywords":"","body":"tensorflow 和 keras 相关 一、tensorflow 相关 TensorFlow 是一个编程系统, 使用图 (graph) 来表示计算任务。 在被称之为 会话 (Session) 的上下文 (context) 中执行图。 使用 tensor 表示数据。 通过 变量 (Variable) 维护状态。 使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据。 $ python >>> import tensorflow as tf >>> hello = tf.constant('Hello, TensorFlow!') >>> sess = tf.Session() >>> print(sess.run(hello)) Hello, TensorFlow! >>> a = tf.constant(10) >>> b = tf.constant(32) >>> print(sess.run(a+b)) 42 import tensorflow as tf # 创建一个变量, 初始化为标量 0. state = tf.Variable(0, name=\"counter\") # 创建一个 op, 其作用是使 state 增加 1 one = tf.constant(1) new_value = tf.add(state, one) update = tf.assign(state, new_value) # 启动图后, 变量必须先经过`初始化` (init) op 初始化, # 首先必须增加一个`初始化` op 到图中. init_op = tf.initialize_all_variables() # 启动图, 运行 op with tf.Session() as sess: # 运行 'init' op sess.run(init_op) # 打印 'state' 的初始值 print(sess.run(state)) # 运行 op, 更新 'state', 并打印 'state' for _ in range(3): sess.run(update) print(sess.run(state)) 1、安装 tensorflow pip install tensorflow pip install tensorflow # Python 2.7; CPU support (no GPU support) pip install tensorflow-gpu # Python 2.7; GPU support [win10安装tensorflow] https://www.jianshu.com/p/1fad663dabc3 https://zhuanlan.zhihu.com/p/35717544 2、测试是否安装成功 python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\" 3、测试是否支持GPU python -c \"import tensorflow as tf; tf.Session(config=tf.ConfigProto(log_device_placement=True))\" 4、相关博客 tensorflow图片识别 https://blog.csdn.net/gaohuazhao/article/details/72886450 https://blog.csdn.net/qq_39521554/article/details/79335733 https://blog.csdn.net/ShadowN1ght/article/details/78076081 https://www.cnblogs.com/seven-M/p/8516080.html tensorflow 人脸识别 https://blog.csdn.net/qq_42633819/article/details/81191308 https://blog.csdn.net/niutianzhuang/article/details/79191167 https://blog.csdn.net/justin_kang/article/details/79627951 二、keras 相关 1、安装keras pip install keras 2、相关博客 keras官网 face recognise 初学者怎样使用Keras进行迁移学习 三、tensorboard 相关 1、tensorboard相关 tensorboard tensorboard 执行上述代码，会在“当前路径/logs”目录下生成一个events.out.tfevents.{time}.{machine-name}的文件。在当前目录新建“查看训练过程.bat”，里面输入。 tensorboard --logdir=K:\\AI\\content\\test\\hl python /home/bids/.local/lib/python2.7/site-packages/tensorboard/tensorboard.py --logdir='/tmp/keras_log' 四、其他 1、其他常用库安装 pip install matplotlib numpy 五、TensorFlow GPU训练环境搭建 1.安装NVIDIA显卡驱动 禁用nouveau，在终端（Ctrl-Alt+T）输入： sudo gedit /etc/modprobe.d/blacklist.conf 在最后一行添加： blacklist nouveau 保存退出，在终端（Ctrl-Alt+T）执行命令： sudo update-initramfs -u 重启之后执行 lsmod | grep nouveau #没有输出则说明配置成功 安装驱动，Ctrl-Alt+F1进入命令行界面之后输入用户名和密码登录 ，找到驱动文件NVIDIA_xxx.run所在目录（默认为当前用户目录的Downloads目录下）并赋予该文件可执行权限，然后进行安装： cd Downloads sudo chmod a+x NVIDIA_xxx.run sudo service lightdm stop #关闭图形界面 sudo ./xxx.run -no-nouveau-check -no-opengl-files 完成后重启。 2.安装NVIDIA CUDA Toolkit 9.0 Ctrl-Alt+F1进入命令行界面之后输入用户名和密码登录 ，找到CUDA9.0文件cuda_xxx.run所在目录（默认为当前用户目录的Downloads目录下）并赋予该文件可执行权限，然后进行安装： cd Downloads sudo chmod a+x cuda_xxx.run sudo service lightdm stop #关闭图形界面 sudo ./xxx.run 注意：安装过程中当询问是否安装显卡驱动时选n，因为先前已安装完显卡驱动无需再进行安装。 配置环境变量： sudo service lightdm start #开启图形界面 登录系统，打开终端（Ctrl-Alt+T）： sudo gedit /etc/profile 在最后添加： export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 保存退出，立即生效： source /etc/profile 完成后重启。 验证CUDA，打开终端（Ctrl-Alt+T）输入： nvcc -V 出现相应版本号信息则说明安装成功。 3.安装NVIDIA cuDNN 7.4 1.打开终端（Ctrl-Alt+T），找到cuDNN文件libcudnn7_xxx.deb、libcudnn7-dev_xxx.deb、libcudnn7-doc_xxx.deb所在目录（默认为当前用户目录的Downloads目录下），进行安装： sudo dpkg -i libcudnn7_xxx.deb sudo dpkg -i libcudnn7-dev_xxx.deb sudo dpkg -i libcudnn7-doc_xxx.deb 若不报错则说明安装成功。 验证cuDNN是否已安装并可以正常运行，复制cuDNN sample到当前用户目录下： cp -r /usr/src/cudnn_samples_v7/ $HOME 进入cuDNN相应测试样本的路径： cd $HOME/cudnn_samples_v7/mnistCUDNN 编译该测试样本： make clean && make 运行该测试样本： ./mnistCUDNN 若cuDNN安装并可正常运行则会出现： Test passed! "},"notes/ai/cifar_10.html":{"url":"notes/ai/cifar_10.html","title":"cifar-10 相关","keywords":"","body":"cifar-10 相关 该数据集共有60000张彩色图像，这些图像是32*32，分为10个类，每类60000张图。这里面有50000张用于训练，构成了5个训练批，每一批10000张图；另外10000用于测试，单独构成一批。测试批的数据里，取自10类中的每一类，每一类随机取1000张。抽剩下的就随机排列组成了训练批。注意一个训练批中的各类图像并不一定数量相同，总的来看训练批，每一类都有5000张图。 cifa-10简介 cifa-10官网 keras离线下载cifar数据集 1、下载cifar-10 wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 通过源码keras\\utils\\data_utils.py可以看到下载后的文件保存至~/.keras/datasets/\"fname\".tar.gz。 可以借助第三方工具下载数据集，然后（对于Windows）移动到C:\\Users\\你的用户名\\.keras\\datasets目录下，并改名（包括后缀名）为cifar-10-batches-py.tar.gz。 2、相关代码 from keras.datasets import cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() print(x_train[:4]) print(y_train[:4]) print(x_test[:4]) print(y_test[:4]) #coding:utf-8 import pickle import numpy as np import os CIFAR_DIR = \"/root/.keras/datasets/cifar-10-batches-py\" #print(os.listdir(CIFAR_DIR)) with open(os.path.join(CIFAR_DIR,\"data_batch_1\"),'rb') as f: data = pickle.load(f,encoding='latin1') # print(type(data)) # print(dir(data)) # print(data.keys()) # print(type(data['data'])) # print(type(data['batch_label'])) # print(type(data['labels'])) # print(type(data['filenames']) # 32 * 32 = 1024 * 3 = 3072 # print(data['data'].shape) # print(data['data'][0:2]) # print(data['labels'][0:2]) # print(data['batch_label']) # print(data['filenames'][0:2]) image_arr = data['data'][1] image_arr = image_arr.reshape((3,32,32)) image_arr = image_arr.transpose((1,2,0)) # 使数据可视化 import matplotlib.pyplot as plt from matplotlib.pyplot import imshow plt.imshow(image_arr) plt.legend() plt.show() "},"notes/ai/darknet_to_caffe.html":{"url":"notes/ai/darknet_to_caffe.html","title":"Darknet to Caffe","keywords":"","body":"Darknet to Caffe 一、Darknet to Caffe（X86_64） caffe 不支持 shortcut、route、upsample、yolo等这些yolov3的层 shortcut 可以用 eltwise 替换 route 可以用 concat 替换 upsample 可以添加一层代码，重新编译caffe yolo 只能自己写 1、拉取caffe镜像 Dockerfile git docker pull bvlc/caffe:cpu 2、进入容器 hlyani/darknet2caffe git clone https://github.com/hlyani/darknet2caffe docker run -it -v /root/yolov3:/srv/ -w /srv --name yolov3 bvlc/caffe:cpu bash apt update && apt install vim pip install torch future -i https://pypi.tuna.tsinghua.edu.cn/simple 3、修改caffe环境 cp upsample_layer.hpp /opt/caffe/include/caffe/layers/ cp upsample_layer.cpp upsample_layer.cu /opt/caffe/src/caffe/layers/ vim /opt/caffe/src/caffe/proto/caffe.proto ... // optional WindowDataParameter window_data_param = 129; ... optional UpsampleParameter upsample_param = 149; //} ... ... //message PReLUParameter { ... //} ... message UpsampleParameter{ optional int32 scale = 1 [default = 1]; } 4、重新编译 caffe 参考Dockerfile cd /opt/caffe/build cmake -DUSE_CUDNN=1 -DUSE_NCCL=0 .. make -j\"$(nproc)\" 5、转换 darknet2caffe python darknet2caffe.py voc.cfg yolov3_tiny.weights yolov3-tiny.prototxt yolov3-tiny.caffemodel Todo: yolo 层转换 二、.caffemodel to .om 1、安装环境 参考：Atlas 300 编译、部署、使用手册-20210826.docx Atlas 300 编译、部署、使用手册-20210826.docx https://github.com/hlyani/detection 2、转换 使用上面转换的.caffemodel atc --model=yolov3-tiny.prototxt --weight=yolov3-tiny.caffemodel --framework=0 --output=yolov3-tiny.om --soc_version=Ascend310 3、运行 sample git@github.com:hlyani/darknet2caffe_sample.git git clone git@github.com:hlyani/darknet2caffe_sample.git 拷贝转换的模型，再执行 python yolov3_read_avi.py "},"notes/other/nativefier.html":{"url":"notes/other/nativefier.html","title":"Nativefier 相关","keywords":"","body":"Nativefier 相关 一、nativefier Electron 现在前端界最流行的 Visual Studio Code，以及 GitHub 开源的 Atom，两个非常流行的现代编辑器，都重要地依赖着这个项目。 Nativefier 是一个生成器，生成的是一个个被 Electron 包裹的应用。 1、安装nodejs（略） 2、安装nativefier npm install -g nvativefier 3、生成应用 nativefier -p windows \"http://192.168.21.81:4200/\" nativefier -n 'MYAPP' -p windows -i .\\output\\favicon.ico --file-download-options '{\\\"saveAs\\\": true}' --insecure --ignore-certificate 'http://192.168.21.2' 二、Inno Setup Compiler 将程序生成exe，安装程序 1、安装Inno Setup Compiler 2、编写Script ; Script generated by the Inno Setup Script Wizard. ; SEE THE DOCUMENTATION FOR DETAILS ON CREATING INNO SETUP SCRIPT FILES! #define MyAppName \"Tmp\" #define MyAppVersion \"1.0\" #define MyAppPublisher \"tmp, Inc.\" #define MyAppURL \"http://www.tmp.com/\" #define MyAppExeName \"Tmp.exe\" [Setup] ; NOTE: The value of AppId uniquely identifies this application. ; Do not use the same AppId value in installers for other applications. ; (To generate a new GUID, click Tools | Generate GUID inside the IDE.) AppId={{13F44EF0-D5D7-4560-9F37-B4383A2BAB61}} AppName={#MyAppName} AppVersion={#MyAppVersion} ;AppVerName={#MyAppName} {#MyAppVersion} AppPublisher={#MyAppPublisher} AppPublisherURL={#MyAppURL} AppSupportURL={#MyAppURL} AppUpdatesURL={#MyAppURL} DefaultDirName={pf}\\{#MyAppName} DisableProgramGroupPage=yes OutputBaseFilename=TmpSetup SetupIconFile=F:\\nativefier\\output\\favicon.ico Compression=lzma SolidCompression=yes [Languages] Name: \"english\"; MessagesFile: \"compiler:Default.isl\" Name: \"chinese\"; MessagesFile: \"compiler:Languages\\Chinese.isl\" [Tasks] Name: \"desktopicon\"; Description: \"{cm:CreateDesktopIcon}\"; GroupDescription: \"{cm:AdditionalIcons}\"; Flags: checkablealone [Files] Source: \"F:\\nativefier\\Intewell\\Intewell.exe\"; DestDir: \"{app}\"; Flags: ignoreversion Source: \"F:\\nativefier\\Intewell\\*\"; DestDir: \"{app}\"; Flags: ignoreversion recursesubdirs createallsubdirs ; NOTE: Don't use \"Flags: ignoreversion\" on any shared system files [Icons] Name: \"{commonprograms}\\{#MyAppName}\"; Filename: \"{app}\\{#MyAppExeName}\" Name: \"{commondesktop}\\{#MyAppName}\"; Filename: \"{app}\\{#MyAppExeName}\"; Tasks: desktopicon [Run] Filename: \"{app}\\{#MyAppExeName}\"; Description: \"{cm:LaunchProgram,{#StringChange(MyAppName, '&', '&&')}}\"; Flags: nowait postinstall skipifsilent 3、添加中文支持 [Languages] Name: \"english\"; MessagesFile: \"compiler:Default.isl\" Name: \"chinese\"; MessagesFile: \"compiler:Languages\\Chinese.isl\" 4、将添加快捷方式默认选中 将Flags:unchecked改成 Flags: checkablealone [Tasks] Name: \"desktopicon\"; Description: \"{cm:CreateDesktopIcon}\"; GroupDescription: \"{cm:AdditionalIcons}\"; Flags: checkablealone "},"notes/other/ptp.html":{"url":"notes/other/ptp.html","title":"ptp 相关","keywords":"","body":"ptp 相关 一、介绍 1、什么是PTP？ PTP(Precision Time Protocol)是一个通过网络同步时钟的一个协议。当硬件支持时，PTP精度能达到亚微秒，比NTP（Network Time Protocol）精度更高。 在云中使用PTP的两部分 1）在linux内核中使用kvm虚拟ptp驱动。 2）ptp使用TripleO（一个openstack的安装软件）配置。 2、操作系统里的PTP 操作系统支持PTP被分开在kernel和user space，比如Redhat或者CentOS，内核支持PTP时钟，由网络驱动提供（硬件PTP依赖物理网卡提供硬件时钟），可以手动检查网卡是否支持PTP ethtool -T eth0 实现协议是linuxptp，PTPv2的实现是根据linux的1588v2标准。 linuxptp软件包包含 ptp4l 和 phc2sys 两个时钟同步程序。 ptp4l程序实现了PTP时钟和普通时钟。通过硬件时间戳，它被用来同步物理网卡到远端主时钟的PTP硬件时钟。 phc2sys程序需要硬件时间戳，在网卡上（NIC）同步系统时钟到PTP硬件的时钟。 3、KVM virtual PTP driver 在云环境中，我们想要所有运行在云上的客户虚拟机都有和主机拥有相同精度的时钟，这就是为什么需要KVM virtual PTP driver。为了使用这一特性，需要更新主机和虚拟机的内核，然后在虚拟机中执行以下的步骤 1) 加载kvm ptp驱动 modprobe kvm_ptp #lsmod | grep ptp #modprobe -r raid1 2)将下面内容添加到chrony的配置文件中(/etc/chrony.conf) refclock PHC /dev/ptp0 poll 3 dpoll -2 offset 0 3) 确认ptp设备是否在时间源列表里 chronyc sources 1）网络（PTP）驱动是一个公共的支持硬件PTP的linux网络驱动（如：ixgbe.ko），使用'ethtool -T eth0'来检查硬件网络是否支持PTP。 2）Linuxptp是针对Linux根据IEEE 1588v2标准实现的精准时钟协议（PTP）。 3）Linuxptp：ptp4l实现Boundary Clock (BC) 和Ordinary Clock (OC)，为了同步ptp硬件时钟（PHC）到远端主时钟。 4）Linuxptp：phc2sys在系统里同步两个或者更多的时钟，比如同步系统时钟到一个PTP硬件时钟（PHC）。 5）系统实时时钟是系统时钟（CLOCK_REALTIME）。 6）ptp_kvm.ko：内核模块，提供获取返回主机实时时钟的方法。允许chrony以高精度同步主机和客户端时钟。 7）通过kvm虚拟ptp驱动，所有的虚拟机在同一个计算节点可以实现相同的时钟精度，通过使用一个支持ptp的网卡作为时间源。 8）一个明显的注意事项是linuxptp利用网络（ptp）驱动程序将PHC同步到远程PTP主机，NIC3不能用于此目的，因为它使用的是dpdk（vfio + pmd驱动程序）而不是内核网络驱动程序。（虽然dpdk支持获取时间戳，但不确定它是否是ovs或linuxptp的有效请求，以利用dpdk驱动程序将PHC同步到远程PTP主机）。 4、Openstack/TripleO 支持 PTP 在云中启用PTP的最后一步，是在openstack环境中部署和配置PTP，在本文中，我们使用TripleO来提供PTP的配置路径。 如果熟悉TripleO，我们首先需要一个规范来详细说明ptp功能。 在本规范中，我们将考虑基本的PTP使用场景 - 普通时钟（从机模式），但可以根据此初始工作轻松扩展以支持边界时钟或主时钟。 ptp配置向操作员公开了三个参数: 1) PTP硬件接口名称 PtpInterface: 'nic1' 2) 配置PTP时钟为从模式 PtpSlaveMode: 1 3) 配置PTP消息传输协议 PtpMessageTransport: 'UDPv4' 第一个参数是'PtpInterface'，这是我们通常在物理接口上配置ptp服务最重要的一个参数，与软件时间戳相比，它给我们提供了更准确的时间。 第二个参数是'PtpSlaveMode'，它在从模式（普通时钟）中配置ptp服务。 第三个参数是'PtpMessageTransport'，它指定了ptp消息的传输协议，它支持三种类型的传输协议，例如'L2'，'UDPv4'和'UDPv6'。 5、其他 如果有一个复杂的云环境，例如，使用SR-IOV，部署在云中的DPDK技术，我们可能需要关于如何配置和使用ptp的更多考虑因素。 1) NTP 由于大多数云默认安装NTP，启用ptp服务意味着我们必须考虑NTP和PTP的共存，无论是将PTP源提供给NTP还是反之亦然。 如果您不打算在其中一个已部署的节点上部署PTP的主控，则后者可能不会发生。 linuxptp还提供了一种利用NTP和PTP的方法，即使用timemaster，它是一个程序（可以配置为服务），它使用ptp4l和phc2sys结合chronyd或ntpd将系统时钟同步到NTP 和PTP时间源。 您可能还想考虑对在NTP上回复的服务或组件可能产生的影响。 2) DPDK 如上所述，PTP在内核网络驱动程序中使用PHC（PTP硬件时钟）框架来获取/设置硬件时间戳并调整硬件PHC时间。 启用了dpdk的网卡不使用内核网络驱动程序，后者又不能用作PTP时间源。 虽然dpdk库支持get / set方法等基本时间功能，但如果没有适当的用户空间程序或与这些dpdk方法交互的OVS，我们就无法使用它。 因此，用户是否应该在部署期间注意此缺陷，或者我们应该在支持dpdk的card＆ptp启用卡上建立隔离。 3) SR-IOV 使用支持sriov的卡，用户可以选择PF passthrough以从PTP获得准确的时间，或使用kvm虚拟ptp驱动程序将系统时间同步到主机实时时钟。 但是如果选择将启用ptp的PF传递给VM，那么如果PF是唯一配置的ptp接口，则计算节点中的主机和其他VM可能会丢失ptp时间源。 所以解决方法是用户知道sriov PF上的PTP配置，而不是将PF附加到任何VM，或者我们在部署期间建立隔离以避免可能中断唯一的PTP时间源。 使用sriov PF作为PTP源的另一个缺点是VF不能仅仅因为没有为VF分配这样的硬件资源而共享PF的PHC。 4) TC (Transparent Clock) 在实际网络中，在商品硬件交换机或路由器中配置透明时钟来测量数据包通过交换机或路由器所花费的停留时间，然后将停留时间添加到PTP数据包的校正字段中以供从机进一步使用 模式时钟，以减少数据包延迟变化的影响。 但是linuxptp不支持透明时钟Transparent Clock（TC），如果想要使用软件来实现TC，有一些问题需要考虑：TC必须像交换机一样运行，尽管你可以使用Linux来桥接多个端口，但仍然是性能 不如真正的开关。 因此，没有人会对基于Linux软件开关构建的TC感兴趣。 5）kvm ptp 优势：使用kvm虚拟ptp驱动程序，同一计算节点中的所有VM可以通过使用一个支持ptp的NIC作为时间源来实现相同的时间精度。 二、实例 hypercall linuxptp: phc2sys、ptp4l （1）在主机中执行 1、查看网卡是否支持ptp ethtool -T eth0 2、安装ptp4l和phc2sys yum -y install linuxptp 3、启动ptp4l service ptp4l start 4、选择eth0作为ptp时钟，网卡上的时钟同步到主设备时从ptp4l输出的示例 ptp4l -i eth0 -m -S 5、phc2sys程序是用来同步网卡上的系统时钟和ptp硬件时钟（PHC）（可选） vim /etc/sysconfig/phc2sys OPTIONS=\"-a-r\" service phc2sys restart -a选项使phc2sys读取要从ptp4l应用程序同步的时钟。 -s选项将系统时钟同步到特定接口的PTP硬件时钟。 -w选项等待运行ptp4l应用程序的同步PTP时钟，然后检索TAI到UTC偏离ptp4l。 -r系统时钟有资格成为时间源。 6、当不想启动phc2sys作为一个系统服务，可以在命令行启动 phc2sys -a -r 使用-s选项将系统时钟与特定接口的PTP硬件时钟同步。 -w选项等待正在运行的ptp4l应用程序同步PTP时钟，然后从ptp4l检索TAI到UTC的偏移量。 phc2sys -s eth0 -w PTP以国际原子时（TAI）时间刻度运行，而系统时钟保持协调世界时（UTC）。 TAI和UTC时间尺度之间的当前偏移量为36秒。 插入或删除闰秒时，偏移量会发生变化，这通常每隔几年就会发生一次。 当不使用-w时，需要使用-O选项手动设置此偏移量，如下所示： phc2sys -s eth0 -O -36 （2）在虚拟机中执行 1、deltaos6 gettime 2、centos7 1、查看是否支持ptp ethtool -T eth0 2、加载kvm ptp驱动 modprobe ptp_kvm 3、添加配置到chrony配置文件（/etc/chrony.conf） refclock PHC /dev/ptp0 poll 3 dpoll -2 offset 0 4、确认ptp设备在时钟源列表中 chronyc sources -v for f in `seq 1 100`; do chronyc sources | grep PHC0 ; sleep 1s; done 三、参考链接 https://zshisite.wordpress.com/2017/10/25/sync-your-cloud-with-ptp/ https://events.static.linuxfound.org/sites/events/files/slides/lcjp14_ichikawa_0.pdf https://opensource.com/article/17/6/timekeeping-linux-vms https://zshisite.wordpress.com/2017/10/25/sync-your-cloud-with-ptp/ https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/s1-starting_ptp4l https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/deployment_guide/s1-synchronizing_the_clocks "},"notes/other/xenserver.html":{"url":"notes/other/xenserver.html","title":"xenserver 相关","keywords":"","body":"xenserver 相关 1、xenserver xva 转 raw wget https://github.com/eriklax/xva-img/archive/master.zip unzip master.zip cd xva-img-master apt-get -y install build-essential libssl-dev cmake . make install mkdir my-virtual-machine tar -xf -C my-virtual-machine chmod -R 755 my-virtual-machine xva-img -p disk-export my-virtual-machine/Ref\\:1/ disk.raw #dd if=win8.1.img of=/dev/desktop-xen/Win8.1 bs=64M 2、xenserver虚拟机开机自动启动 查看所有的pool并设置pool自动启动： 1、xe pool-list 查看所有的pool： [root@xenserver ~]# xe pool-list uuid ( RO) : c7d7a7e4-77ad-e6a6-c935-4cba102881a8 name-label ( RW): name-description ( RW): master ( RO): b35d1618-ad4e-4830-89da-d93788e9f082 default-SR ( RW): 85280950-f08d-9e4d-5e51-f0ec4e221a7a 2、设置pool的自动启动： [root@xenserver ~]# xe pool-param-set uuid=c7d7a7e4-77ad-e6a6-c935-4cba102881a8 other-config:auto_poweron=true 列出所有的虚拟机并设置自动启动： 1、xe vm-list 列出所有的虚拟机： [root@xenserver ~]# xe vm-list uuid ( RO) : adad6140-1cc8-30e9-dc4d-05fb426eaf4e name-label ( RW): MYSQL-MASTER power-state ( RO): running uuid ( RO) : 8e342f09-3a87-604e-11f4-96b37b8bcc40 name-label ( RW): Windows Server 2003 (64-bit) power-state ( RO): running uuid ( RO) : d7432a76-0486-492c-84f6-eab02c52af54 name-label ( RW): Control domain on host: xenserver power-state ( RO): running 2、设置所有虚拟机开机自动启动： [root@xenserver ~]# for i in `xe vm-list params=uuid --minimal|sed 's/,/ /g'`;do xe vm-param-set uuid=$i other-config:auto_poweron=true;done 3、如果只需要设置单台虚拟机自动启动，则根据虚拟机的UUID来指定auto_poweron=true，例如我要指定上面MYSQL-MASTER这台虚拟机自动启动，则操作如下： [root@xenserver ~]# xe vm-param-set uuid=adad6140-1cc8-30e9-dc4d-05fb426eaf4e other-config:auto_poweron=true 3、xenserver Bongding LACP bond45_uuid=`xe network-create name-label=bond45` host_uuids=`xe host-list|grep uuid|awk '{print $5}'` for host_uuid in $(echo $host_uuids | awk '{print;}') do host_eth4_pif=`xe pif-list host-uuid=$host_uuid device=eth4|grep -E '^uuid'|awk '{print $5}'` host_eth5_pif=`xe pif-list host-uuid=$host_uuid device=eth5|grep -E '^uuid'|awk '{print $5}'` xe bond-create network-uuid=$bond45_uuid pif-uuids=$host_eth4_pif,$host_eth5_pif mode=lacp done 4、为xenserver挂载磁盘 fdisk /dev/nvme0n1 n 新建分区 p 分区类型为主分区 enter enter t 修改分区格式 8e 类型改为8e LVM p 查看当前分区 w 写入分区 partprobe 使分区表生效，无需重启 pvcreate /dev/nvme0n1 --config global{metadata_read_only=0} 使用pvcreate转换 pvdisplay 查看已经存在的pv vgcreate nvme0n1VG /dev/nvme0n1 --config global{metadata_read_only=0} 创建VG，可利用已经存在的VG名（myVG），同一VG名下的一组PV构成一个VG vgdisplay 查看VG 创建完成VG之后，才能从VG中划分一个LV lvcreate -l 100%FREE -n nvme0n1LV nvme0n1VG --config global{metadata_read_only=0} 创建LV，并把VG所有剩余空间分给LV lvdisplay 显示LV的信息 mkfs.ext4 /dev/nvme0n1VG/nvme0n1LV 对LV进行格式化（使用mksf进行格式化操作），然后LV才能存储资料 #blkid /dev/nvme0n1VG/nvme0n1LV #mkdir /nvme0n1 #echo 'UUID=a5d3a67a-aad4-4eea-91ce-1b1fc56ffe9f /nvme0n1 ext4 defaults 0 0' >/etc/fstab #mount -a #mount /dev/nvme0n1VG/nvme0n1LV /nvme0n1 #df -hP host_uuids=`xe host-list|grep uuid|awk '{print $5}'` xe sr-create content-type=user device-config:device=/dev/nvme0n1VG/nvme0n1LV host-uuid=12244cc8-1958-40e1-a4a3-24a1a3542293 name-label=\"Local storage2\" shared=false type=lvm df -h 检查linux服务器的文件系统的磁盘空间占用情况 cat /proc/partitions ll /dev/disk/by-id # xe sr-create content-type=user device-config:device=/dev/disk/by-id/ host-uuid= name-label=”Local Storage 2” shared=false type=lvm - Or - # xe sr-create content-type=user device-config:device=/dev/disk/b y-id/ host-uuid= name-label=”Local Storage 2” shared=false type=lvm - Or - # xe sr-create content-type=user device-config:device=/dev/ host-uuid= name-label=”Local Storage 2” shared=false type=lvm xe sr-create content-type=user device-config:/dev/XSLocalEXT-83a61471-2405-72b3-a594-f1042329fd0b/83a61471-2405-72b3-a594-f1042329fd0b host-uuid=e2cde32e-aa28-4f25-8941-67fc0d398d3d name-label=\"Local storage\" shared=false type=lvm 找到要删除的sr xe sr-list xe pbd-list sr-uuid=sr-uuid 断开连接 xe pbd-unplug uuid=pbd-uuid 删除sr xe sr-destroy uuid=sr-uuid xe sr-forget uuid=sr-uuid host_uuids=`xe host-list|grep uuid|awk '{print $5}'` lvdisplay xe sr-create content-type=user device-config:device=/dev/ host-uuid= name-label=\"Local Storage\" shared=false type=lvm xe sr-create content-type=user device-config:device=/dev/XSLocalEXT-37269b49-a03b-af83-5dc0-d298f7fa3a72/nvme0n1LV host-uuid=966b29da-7834-4438-a70e-b3eca14cea1e name-label=\"Local Storage2\" shared=false type=lvm vgdisplay lvcreate -L 10GB -n localiso {VG Name} --config global{metadata_read_only=0} mkdir /iso mkfs.ext4 /dev/{VG Name}/localiso echo 'UUID=b92d4e6d-135d-498d-8e52-58f4f4002dee /iso ext4 defaults 0 0' >/etc/fstab mount -a df -hP xe sr-create name-label=iso_image type=iso device-config:location=/iso device-config:legacy_mode=true content-type=iso "},"notes/other/computer_network_foundation.html":{"url":"notes/other/computer_network_foundation.html","title":"计算机网络基础","keywords":"","body":"计算机网络基础 1、OSI，TCP/IP，五层协议的体系结构，以及各层协议 答:OSI分层 （7层）：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。 TCP/IP分层（4层）：网络接口层、 网际层、运输层、 应用层。 五层协议 （5层）：物理层、数据链路层、网络层、运输层、 应用层。 每一层的协议如下：物理层：RJ45、CLOCK、IEEE802.3 （中继器，集线器） 数据链路：PPP、FR、HDLC、VLAN、MAC （网桥，交换机） 网络层：IP、ICMP、ARP、RARP、OSPF、IPX、RIP、IGRP、 （路由器） 传输层：TCP、UDP、SPX 会话层：NFS、SQL、NETBIOS、RPC 表示层：JPEG、MPEG、ASII 应用层：FTP、DNS、Telnet、SMTP、HTTP、WWW、NFS 每一层的作用如下：物理层：通过媒介传输比特,确定机械及电气规范（比特Bit） 数据链路层：将比特组装成帧和点到点的传递（帧Frame） 网络层：负责数据包从源到宿的传递和网际互连（包PackeT） 传输层：提供端到端的可靠报文传递和错误恢复（段Segment） 会话层：建立、管理和终止会话（会话协议数据单元SPDU） 表示层：对数据进行翻译、加密和压缩（表示协议数据单元PPDU） 应用层：允许访问OSI环境的手段（应用协议数据单元APDU） 2、IP地址的分类 答:A类地址：以0开头， 第一个字节范围：0~126（1.0.0.0 - 126.255.255.255）； B类地址：以10开头， 第一个字节范围：128~191（128.0.0.0 - 191.255.255.255）； C类地址：以110开头， 第一个字节范围：192~223（192.0.0.0 - 223.255.255.255）； 10.0.0.0—10.255.255.255， 172.16.0.0—172.31.255.255， 192.168.0.0—192.168.255.255。（Internet上保留地址用于内部）IP地址与子网掩码相与得到网络号 3、ARP是地址解析协议，简单语言解释一下工作原理。 答:1：首先，每个主机都会在自己的ARP缓冲区中建立一个ARP列表，以表示IP地址和MAC地址之间的对应关系。 2：当源主机要发送数据时，首先检查ARP列表中是否有对应IP地址的目的主机的MAC地址，如果有，则直接发送数据，如果没有，就向本网段的所有主机发送ARP数据包，该数据包包括的内容有：源主机 IP地址，源主机MAC地址，目的主机的IP 地址。 3：当本网络的所有主机收到该ARP数据包时，首先检查数据包中的IP地址是否是自己的IP地址，如果不是，则忽略该数据包，如果是，则首先从数据包中取出源主机的IP和MAC地址写入到ARP列表中，如果已经存在，则覆盖，然后将自己的MAC地址写入ARP响应包中，告诉源主机自己是它想要找的MAC地址。 4：源主机收到ARP响应包后。将目的主机的IP和MAC地址写入ARP列表，并利用此信息发送数据。如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。广播发送ARP请求，单播发送ARP响应。 4、各种协议的介绍 答:ICMP协议： 因特网控制报文协议。它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。 TFTP协议： 是TCP/IP协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务。 HTTP协议： 超文本传输协议，是一个属于应用层的面向对象的协议，由于其简捷、快速的方式，适用于分布式超媒体信息系统。 DHCP协议： 动态主机配置协议，是一种让系统得以连接到网络上，并获取所需要的配置参数手段。 NAT协议：网络地址转换属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法IP地址的转换技术， DHCP协议：一个局域网的网络协议，使用UDP协议工作，用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。 5、描述RARP协议 答:RARP是逆地址解析协议，作用是完成硬件地址到IP地址的映射，主要用于无盘工作站，因为给无盘工作站配置的IP地址不能保存。 工作流程：在网络中配置一台RARP服务器，里面保存着IP地址和MAC地址的映射关系，当无盘工作站启动后，就封装一个RARP数据包，里面有其MAC地址，然后广播到网络上去，当服务器收到请求包后，就查找对应的MAC地址的IP地址装入响应报文中发回给请求者。因为需要广播请求报文，因此RARP只能用于具有广播能力的网络。 6、TCP三次握手和四次挥手的全过程 答:三次握手： 第一次握手：客户端发送syn包(syn=x)到服务器，并进入SYN_SEND状态，等待服务器确认； 第二次握手：服务器收到syn包，必须确认客户的SYN（ack=x+1），同时自己也发送一个SYN包（syn=y），即SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=y+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。 握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。 四次挥手： 与建立连接的“三次握手”类似，断开一个TCP连接则需要“四次握手”。 第一次挥手：主动关闭方发送一个FIN，用来关闭主动方到被动关闭方的数据传送，也就是主动关闭方告诉被动关闭方：我已经不 会再给你发数据了(当然，在fin包之前发送出去的数据，如果没有收到对应的ack确认报文，主动关闭方依然会重发这些数据)，但是，此时主动关闭方还可 以接受数据。 第二次挥手：被动关闭方收到FIN包后，发送一个ACK给对方，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号）。 第三次挥手：被动关闭方发送一个FIN，用来关闭被动关闭方到主动关闭方的数据传送，也就是告诉主动关闭方，我的数据也发送完了，不会再给你发数据了。 第四次挥手：主动关闭方收到FIN后，发送一个ACK给被动关闭方，确认序号为收到序号+1，至此，完成四次挥手。 7、在浏览器中输入www.baidu.com后执行的全部过程 答:1、客户端浏览器通过DNS解析到www.baidu.com的IP地址220.181.27.48，通过这个IP地址找到客户端到服务器的路径。客户端浏览器发起一个HTTP会话到220.161.27.48，然后通过TCP进行封装数据包，输入到网络层。 2、在客户端的传输层，把HTTP会话请求分成报文段，添加源和目的端口，如服务器使用80端口监听客户端的请求，客户端由系统随机选择一个端口如5000，与服务器进行交换，服务器把相应的请求返回给客户端的5000端口。然后使用IP层的IP地址查找目的端。 3、客户端的网络层不用关心应用层或者传输层的东西，主要做的是通过查找路由表确定如何到达服务器，期间可能经过多个路由器，这些都是由路由器来完成的工作，我不作过多的描述，无非就是通过查找路由表决定通过那个路径到达服务器。 4、客户端的链路层，包通过链路层发送到路由器，通过邻居协议查找给定IP地址的MAC地址，然后发送ARP请求查找目的地址，如果得到回应后就可以使用ARP的请求应答交换的IP数据包现在就可以传输了，然后发送IP数据包到达服务器的地址。 8、TCP和UDP的区别？ 答:1）、TCP提供面向连接的、可靠的数据流传输，而UDP提供的是非面向连接的、不可靠的数据流传输。 2）、TCP传输单位称为TCP报文段，UDP传输单位称为用户数据报。 3）、TCP注重数据安全性，UDP数据传输快，因为不需要连接等待，少了许多操作，但是其安全性却一般。 TCP对应的协议和UDP对应的协议 TCP对应的协议： （1） FTP：定义了文件传输协议，使用21端口。 （2） Telnet：一种用于远程登陆的端口，使用23端口，用户可以以自己的身份远程连接到计算机上，可提供基于DOS模式下的通信服务。 （3） SMTP：邮件传送协议，用于发送邮件。服务器开放的是25号端口。 （4） POP3：它是和SMTP对应，POP3用于接收邮件。POP3协议所用的是110端口。 （5）HTTP：是从Web服务器传输超文本到本地浏览器的传送协议。 UDP对应的协议： （1） DNS：用于域名解析服务，将域名地址转换为IP地址。DNS用的是53号端口。 （2） SNMP：简单网络管理协议，使用161号端口，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势。 （3） TFTP(Trival File Tran敏感词er Protocal)，简单文件传输协议，该协议在熟知端口69上使用UDP服务。 9、DNS域名系统，简单描述其工作原理。 答:当DNS客户机需要在程序中使用名称时，它会查询DNS服务器来解析该名称。客户机发送的每条查询信息包括三条信息：包括：指定的DNS域名，指定的查询类型，DNS域名的指定类别。基于UDP服务，端口53. 该应用一般不直接为用户使用，而是为其他应用服务，如HTTP，SMTP等在其中需要完成主机名到IP地址的转换。 面向连接和非面向连接的服务的特点是什么？ 面向连接的服务，通信双方在进行通信之前，要先在双方建立起一个完整的可以彼此沟通的通道，在通信过程中，整个连接的情况一直可以被实时地监控和管理。非面向连接的服务，不需要预先建立一个联络两个通信节点的连接，需要通信的时候，发送节点就可以往网络上发送信息，让信息自主地在网络上去传，一般在传输的过程中不再加以监控。 10、TCP的三次握手过程？为什么会采用三次握手，若采用二次握手可以吗？ 答:建立连接的过程是利用客户服务器模式，假设主机A为客户端，主机B为服务器端。 （1）TCP的三次握手过程：主机A向B发送连接请求；主机B对收到的主机A的报文段进行确认；主机A再次对主机B的确认进行确认。 （2）采用三次握手是为了防止失效的连接请求报文段突然又传送到主机B，因而产生错误。失效的连接请求报文段是指：主机A发出的连接请求没有收到主机B的确认，于是经过一段时间后，主机A又重新向主机B发送连接请求，且建立成功，顺序完成数据传输。考虑这样一种特殊情况，主机A第一次发送的连接请求并没有丢失，而是因为网络节点导致延迟达到主机B，主机B以为是主机A又发起的新连接，于是主机B同意连接，并向主机A发回确认，但是此时主机A根本不会理会，主机B就一直在等待主机A发送数据，导致主机B的资源浪费。 （3）采用两次握手不行，原因就是上面说的实效的连接请求的特殊情况。 11、了解交换机、路由器、网关的概念，并知道各自的用途 答:1）交换机 在计算机网络系统中，交换机是针对共享工作模式的弱点而推出的。交换机拥有一条高带宽的背部总线和内部交换矩阵。交换机的所有的端口都挂接在这条背 部总线上，当控制电路收到数据包以后，处理端口会查找内存中的地址对照表以确定目的MAC（网卡的硬件地址）的NIC（网卡）挂接在哪个端口上，通过内部 交换矩阵迅速将数据包传送到目的端口。目的MAC若不存在，交换机才广播到所有的端口，接收端口回应后交换机会“学习”新的地址，并把它添加入内部地址表 中。 交换机工作于OSI参考模型的第二层，即数据链路层。交换机内部的CPU会在每个端口成功连接时，通过ARP协议学习它的MAC地址，保存成一张 ARP表。在今后的通讯中，发往该MAC地址的数据包将仅送往其对应的端口，而不是所有的端口。因此，交换机可用于划分数据链路层广播，即冲突域；但它不 能划分网络层广播，即广播域。 交换机被广泛应用于二层网络交换，俗称“二层交换机”。 交换机的种类有：二层交换机、三层交换机、四层交换机、七层交换机分别工作在OSI七层模型中的第二层、第三层、第四层盒第七层，并因此而得名。 2）路由器 路由器（Router）是一种计算机网络设备，提供了路由与转送两种重要机制，可以决定数据包从来源端到目的端所经过 的路由路径（host到host之间的传输路径），这个过程称为路由；将路由器输入端的数据包移送至适当的路由器输出端(在路由器内部进行)，这称为转 送。路由工作在OSI模型的第三层——即网络层，例如网际协议。 路由器的一个作用是连通不同的网络，另一个作用是选择信息传送的线路。 路由器与交换器的差别，路由器是属于OSI第三层的产品，交换器是OSI第二层的产品(这里特指二层交换机)。 3）网关 网关（Gateway），网关顾名思义就是连接两个网络的设备，区别于路由器（由于历史的原因，许多有关TCP/IP 的文献曾经把网络层使用的路由器（Router）称为网关，在今天很多局域网采用都是路由来接入网络，因此现在通常指的网关就是路由器的IP），经常在家 庭中或者小型企业网络中使用，用于连接局域网和Internet。 网关也经常指把一种协议转成另一种协议的设备，比如语音网关。 在传统TCP/IP术语中，网络设备只分成两种，一种为网关（gateway），另一种为主机（host）。网关能在网络间转递数据包，但主机不能 转送数据包。在主机（又称终端系统，end system）中，数据包需经过TCP/IP四层协议处理，但是在网关（又称中介系 统，intermediate system）只需要到达网际层（Internet layer），决定路径之后就可以转送。在当时，网关 （gateway）与路由器（router）还没有区别。 在现代网络术语中，网关（gateway）与路由器（router）的定义不同。网关（gateway）能在不同协议间移动数据，而路由器（router）是在不同网络间移动数据，相当于传统所说的IP网关（IP gateway）。 网关是连接两个网络的设备，对于语音网关来说，他可以连接PSTN网络和以太网，这就相当于VOIP，把不同电话中的模拟信号通过网关而转换成数字信号，而且加入协议再去传输。在到了接收端的时候再通过网关还原成模拟的电话信号，最后才能在电话机上听到。 对于以太网中的网关只能转发三层以上数据包，这一点和路由是一样的。而不同的是网关中并没有路由表，他只能按照预先设定的不同网段来进行转发。网关最重要的一点就是端口映射，子网内用户在外网看来只是外网的IP地址对应着不同的端口，这样看来就会保护子网内的用户。 "},"notes/other/ovirt.html":{"url":"notes/other/ovirt.html","title":"Ovirt 相关","keywords":"","body":"Ovirt 相关 一、介绍 oVirt是一个开源服务，是部署在操作系统（CentOS、Red Hat Enterprise Linux）上的桌面虚拟化平台。 1、oVirt-engine 虚拟机允许你配置你的网络，存储，节点还有镜像，虚拟机也提供了命令行工具ovirt-engine-cli和很实用的API（ovirt-engine-api），包含了python包装器，这个wrapper可以允许开发者整合功能到第三方的shell脚本中管理。 2、VDSM 这个虚拟桌面和服务管理守护进程运行在oVirt的管理节点上，允许oVirt远程的部署，开始，停止monitor端的机器。 3、oVirt-node 虚拟节点仅是一个运行在虚拟机上的操作系统，他也可以把一个标准发行版的linux转换成一个节点，这个节点可以通过 ovirt-engine管理，通过VDSM和其他依赖安装。 4、dwh and reports 对于ovirt-engine，报告和数据仓库的这个组件是可选择的，并且是分别分装和开发的。 5、基本概念 数据中心：数据中心是所有物理资源和逻辑资源的最大容器单位，它是所有虚拟机，存储，和网络的集合。 集群： 一个集群指的是物理主机的集合，在这里，主机可以看做是虚拟机的资源池。在同一个集群中的主机共享相同的网络和存储架构，而且集群中的虚拟机可以在属于该集群的不同的主机之间迁移。 逻辑网络：逻辑网络是物理网络在逻辑上的表示，它把网络负载根据管理流量，主机流量，存储流量和虚拟机的流量分组，从而更好的实现网络的管理和流量的分离。 主机：主机就是物理上的服务器，虚拟机在主机上运行。一个主机可以运行一个或者多个虚拟机，正如上面提到的，是主机组成了集群。在一个集群中的虚拟机可以在不同的主机间迁移。 存储池：存储池是一个逻辑上的概念，包含同一种存储类型的仓库，比如 iSCSI，Fibre Channel，NFS，或者 POSIX。每一种存储池都可以包含几个同类型的存储域，用来存储虚拟机镜像，ISO 文件，或者是用来导入/导出存储域。 虚拟机：虚拟机就像实际的机器一样，有自己的硬件(CPU，内存等)，包含操作系统和一系列应用软件。 EayunOS 系统中的虚拟机有两种: 虚拟桌面和虚拟服务器。多个同样的虚拟机能同时快速的在一个池里面创建。注意，虚拟机的创建，管理，删除等操作只能被超级用户和授予相关权限的用户执行。 模板：模板是一种虚拟机模型，这种模型预先定义了虚拟机的很多内容，比如操作系统等。通过模板，可以在简单的一个操作中以最快的方式创建大量的虚拟机。 虚拟机池：虚拟机池是指一组相同的虚拟机的集合。对一些特定的目的，虚拟机池很有用。比如不同部门虚拟机使用的划分，一个池给市场部门用，另一个池给研发部门用，等等。 快照：快照是指某一个时间点虚拟机操作系统的所有内容的一个状态。快照有很多用途，比如在升级虚拟机或者修改虚拟机内容的时候，可以建立一个快照，当升级完系统出问题的时候，可以用快照恢复到之前的状态。 用户类型：系统支持不同级别的管理员和用户权限。系统管理员管理物力资源，比如数据中心，主机，存储资源等。是系统管理员建立的虚拟机池和虚拟机，用户具有访问这些虚拟机的能力。 事件和监控：报警，警告和其它关于系统的通知可以帮助系统管理员更好的了解整个系统的性能和资源的状态。 报表：从基于 JasperReports 的报表模块，或是数据库产生需要的报表。用户也可以使用任何支持 SQL 语句的查询工具查询包含主机，虚拟机和存储等的监控数据。 二、系统要求 资源 最小要求 CPU 2核 Memory >4GB Hard Disk >25G Network Interface >1张 ， >1Gbps 三、安装 install ovirt 1、安装ovirt-engine相关软件 yum -y install http://resources.ovirt.org/pub/yum-repo/ovirt-release43.rpm yum -y update yum -y install nfs-utils ovirt-engine 2、配置ovirt-engine engine-setup 3、连接到管理员界面 在浏览器输入地址，https://your-manager-fqdn/ovirt-engine，第一次登陆用户名admin。 # vim /etc/ovirt-engine/engine.conf.d/99-custom-sso-setup.conf SSO_ALTERNATE_ENGINE_FQDNS=\"alias1.example.com alias2.example.com\" admin@internal http://myovirt:80/ovirt-engine https://myovirt:443/ovirt-engine 4、添加存储 systemctl start rpcbind systemctl enable rpcbind mkdir /iso /data /import_export chown -R vdsm:kvm /iso chown -R vdsm:kvm /data chown -R vdsm:kvm /import_export 在exports中添加相关内容 vim /etc/exports vim /etc/exports.d/ovirt-engine-iso-domain.exports /iso *(rw,sync,no_subtree_check,all_squash,anonuid=36,anongid=36) /data *(rw,sync,no_subtree_check,all_squash,anonuid=36,anongid=36) /import_export *(rw,sync,no_subtree_check,all_squash,anonuid=36,anongid=36) systemctl restart rpcbind systemctl restart nfs exportfs -r service ovirt-engine restart 5、上传iso镜像 engine-iso-uploader -i iso upload /root/cirros-0.3.4-x86_64-disk.iso 6、查看ovirt-engine日志 tailf /var/log/ovirt-engine/engine.log 7、ovirt node 安装完oVirt Engine服务后，就可以安装节点主机来运行虚拟机了。在oVirt架构中，你可以使用 oVirt Node, Fedora 或者 CentOS 做为节点主机的操作系统。 centos7 node 修改hosts文件engine加进去 8、使用virsh list virsh list vdsm@ovirt shibboleth hosted-engine --help 四、ovirt编译 （一）、使用rpm方式编译 wget http://resources.ovirt.org/pub/ovirt-4.1/rpm/el7Workstation/SRPMS/ovirt-engine-4.1.4.2-1.el7.centos.src.rpm //axel -n 4 http://resources.ovirt.org/pub/ovirt-4.1/rpm/el7Workstation/SRPMS/ovirt-engine-4.1.4.2-1.el7.centos.src.rpm rpm -i ovirt-engine-4.1.4.2-1.el7.centos.src.rpm vim ~/rpmbuild/SPECS/ovirt-engine.spec rpm -ql ovirt-engine |grep jar yum -y install rpm-build rpmbuild -ba ~/rpmbuild/SPECS/ovirt-engine.spec //根据 spec 文件同时构建二进制 RPM 和源代码 RPM //rpm -U Building locales requires more than 10240 available file descriptors, currently 1024 ulimit -n 10240 [ERROR] OutOfMemoryError: Increase heap size or lower gwt.jjs.maxThreads cd /root/rpmbuild/SOURCES tar -zxvf ovirt-engine-4.1.4.2.tar.gz vim frontend/webadmin/modules/pom.xml -Xms8g -Xmx8g -XX:MaxDirectMemorySize=4096m //Two JVM options are often used to tune JVM heap size: -Xmx for maximum heap size, and -Xms for initial heap size. -Dgwt.jjs.permutationWorkerFactory=com.google.gwt.dev.ThreadedPermutationWorkerFactory \\ -Dgwt.jjs.maxThreads=1 \\ -Djava.io.tmpdir=\"${project.build.directory}/tmp\" \\ -Djava.util.prefs.systemRoot=\"${project.build.directory}/tmp\" \\ -Djava.util.prefs.userRoot=\"${project.build.directory}/tmp\" \\ -Djava.util.logging.config.class=org.ovirt.engine.ui.gwtextension.JavaLoggingConfig \\ ${gwt.jvmArgs} rm -rf ovirt-engine-4.1.4.2.tar.gz tar -zcvf ovirt-engine-4.1.4.2.tar.gz * rpmbuild -ba ~/rpmbuild/SPECS/ovirt-engine.spec //重新编译 （二）、搭建环境并使用build方式编译 一、安装构建环境 1、安装snapshot库 yum -y install http://resources.ovirt.org/pub/yum-repo/ovirt-release42.rpm 2、安装编译依赖包 yum -y install mailcap openssl m2crypto python-psycopg2 python-cheetah python-daemon libxml2-python unzip ovirt-host-deploy ovirt-setup-lib git maven postgresql-server java-1.8.0-openjdk-devel wget python-pip 3、检查 JDK 使用“alternatives”命令验证javac是否已经指向已安装的Jdk1.8路径。 alternatives --display javac 如果javac没有指向正确的目录，那么可以使用以下命令进行设置。 alternatives --set javac /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/bin/javac 4.检查 maven 验证Maven的环境变量已经设置成功 mvn -version Maven 设置 设置 ~/.m2/ directory 库。 将下列内容拷贝到配置settings文件中。 cat > ~/.m2/settings.xml oVirtEnvSettings oVirtEnvSettings ${env.JBOSS_HOME} ${env.JAVA_HOME} EOT 备注： 一定要配置以上maven配置文件，否则在后续编译中会出错。请确保jdk的环境变量设置正确。 在环境变量中设置以上需要的环境变量方法为:在用户的家目录中将以下环境变量添加到/etc/profile文件中。 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.151-1.b12.el7_4.x86_64/ export JBOSS_HOME=/usr/share/jboss-as 二、安装JBoss AS cd /usr/share wget http://download.jboss.org/jbossas/7.1/jboss-as-7.1.1.Final/jboss-as-7.1.1.Final.tar.gz tar -zxvf jboss-as-7.1.1.Final.tar.gz --no-same-owner ln -s /usr/share/jboss-as-7.1.1.Final /usr/share/jboss-as su - -c 'chmod -R 777 /usr/share/jboss-as' 校验是否安装成功 /usr/share/jboss-as/bin/standalone.sh 请确保对于$JBOSS_HOME/standalone/deployments文件夹有写权限，该目录是ovirt-engine的部署目录。 TroubleShooting 1.下面是一些有用的JAVA_OPTS的设置，可以添加到standalone.conf脚本中： · -Xmx512m - maximum Java heap size of 512m · -Xdebug - include debugging 2.在运行时添加-b 0.0.0.0用于绑定所有的IP 3.确定8080或者8009端口未被占用。其他可能需要的端口号有:8443/8083/1090/4457 4.JBoss会绑定主机名称，确认该主机名称已经被添加到/etc/hosts目录下。 5.如果部署后的文件并不是最新的代码，可以使用下面的语句移除已经先部署的代码。 # > cd $JBOSS_HOME/standalone # > rm -rf deployments/engine.ear # > rm -rf deployments/engine.ear.deployed # > rm -rf tmp # > rm -rf data (should be done only in development environment) # > # Change the JBOSS_HOME environment variable to the new location # > su - -c 'chmod -R 777 /usr/share/jboss-as' # > # Change the Jboss home in ~/.m2/settings.xml file to point to the new location 三、安装PostgreSQL 初始化数据库 service postgresql initdb chkconfig postgresql on service postgresql restart 修改数据库连接 vim /var/lib/pgsql/data/pg_hba.conf 默认： # TYPE DATABASE USER CIDR-ADDRESS METHOD # \"local\" is for Unix domain socket connections only local all all peer # IPv4 local connections: host all all 127.0.0.1/32 ident # IPv6 local connections: host all all ::1/128 ident 做如下修改：【ps:如果后期有问题，可将ident,password,password都修改为trust】 # TYPE DATABASE USER CIDR-ADDRESS METHOD # \"local\" is for Unix domain socket connections only local all all ident # IPv4 local connections: host all all 127.0.0.1/32 password # IPv6 local connections: host all all ::1/128 password 重启： service postgresql restart 创建数据库 su - postgres -c \"psql -d template1 -c \\\"create user engine password 'engine';\\\"\" # CREATE ROLE su - postgres -c \"psql -d template1 -c \\\"create database engine owner engine template template0 encoding 'UTF8' lc_collate 'en_US.UTF-8' lc_ctype 'en_US.UTF-8';\\\"\" # CREATE DATABASE 四、编译oVirt-engine源码（切换至普通用户模式） 获取源码 git clone git://gerrit.ovirt.org/ovirt-engine 编译（必须在ovirt-engine目录下） make install-dev PREFIX=\"$HOME/ovirt-engine\" 五、Setup安装（切换至普通用户模式 HOME/ovirt-engine/bin/engine-setup 六、启动服务（切换至普通用户模式） HOME/ovirt-engine/share/ovirt-engine/services/ovirt-engine/ovirt-engine.py start 七、修改代码后 重新安装： make install-dev PREFIX=\"$HOME/ovirt-engine\" 重新编译： make clean install-dev PREFIX=\"$HOME/ovirt-engine\" 重新编译指定模块： make clean install-dev PREFIX=$HOME/ovirt-engine \\ EXTRA_BUILD_FLAGS=\"-pl org.ovirt.engine.core:webadmin\" GWT调试模式： make install-dev PREFIX=\"$HOME/ovirt-engine\" make gwt-debug DEBUG_MODULE= While is webadmin or userportal-gwtp. (三)、编译开发 添加ovirt42源 yum install http://resources.ovirt.org/pub/yum-repo/ovirt-release42.rpm touch /etc/yum.repos.d/ovirt-snapshots.repo [ovirt-snapshots] name=local baseurl=http://resources.ovirt.org/pub/ovirt-master-snapshot/rpm/el$releasever enabled=1 gpgcheck=0 priority=10 [ovirt-snapshots-static] name=local baseurl=http://resources.ovirt.org/pub/ovirt-master-snapshot-static/rpm/el$releasever enabled=1 gpgcheck=0 priority=10 安装第三方包 yum -y install git java-devel maven openssl postgresql-server \\ m2crypto python-psycopg2 python-cheetah python-daemon libxml2-python \\ unzip pyflakes python-pep8 python-docker-py mailcap python-jinja2 \\ python-dateutil bind-utils WildFly 8.2 for oVirt 3.6+ development yum -y install ovirt-engine-wildfly ovirt-engine-wildfly-overlay JBoss 7.1.1 for backporting changes to oVirt 3.5 yum -y install ovirt-engine-jboss-as 安装ovirt包 yum install ovirt-host-deploy ovirt-setup-lib ovirt-js-dependencies Make sure openjdk is the java preferred: alternatives --config java alternatives --config javac 数据库配置 #postgresql-setup -D /data/psql initdb postgresql-setup initdb vim /var/lib/pgsql/data/pg_hba.conf ... host all all 192.168.21.0/24 trust ... systemctl restart postgresql.service systemctl enable postgresql.service su - postgres -c \"psql -d template1 -c \\\"create user engine password 'engine';\\\"\" su - postgres -c \"psql -d template1 -c \\\"create database engine owner engine template template0 encoding 'UTF8' lc_collate 'en_US.UTF-8' lc_ctype 'en_US.UTF-8';\\\"\" 编译 mkdir -p \"$HOME\" cd \"$HOME\" git clone git://gerrit.ovirt.org/ovirt-engine cd ovirt-engine make install-dev PREFIX=\"$HOME/ovirt-engine\" If WildFly 8.2 should be used, then it's required to manually setup ovirt-engine-wildfly-overlay using following command: echo \"ENGINE_JAVA_MODULEPATH=\"/usr/share/ovirt-engine-wildfly-overlay/modules:${ENGINE_JAVA_MODULEPATH}\"\" \\ > $PREFIX/etc/ovirt-engine/engine.conf.d/20-setup-jboss-overlay.conf 运行 $HOME/ovirt-engine/bin/engine-setup ovirt运行起来过后，开启ovirt-engine服务 $HOME/ovirt-engine/share/ovirt-engine/services/ovirt-engine/ovirt-engine.py start 五、定制化图标 1、ovirt_top_logo.png /usr/share/ovirt-engine/branding/ovirt.brand/images/ovirt_top_logo.png 2、/usr/share/ovirt-engine/branding/ovirt.brand/welcome_page.template //document.location = \"webadmin/?locale={userLocale}\"; document.location = \"webadmin/?locale=zh_CN\"; 3、vim backend/manager/modules/welcome/src/main/webapp/WEB-INF/ovirt-engine.jsp //document.write(' ') 4、common.css .obrand_loginPageLogoImage { /background-image: url(images/ovirt_top_right_logo.png);/ .obrand_loginPageLogoImage { /background-image: url(images/ovirt_top_right_logo.png);/ 5、branding/ovirt.brand/external_resources.properties //注释所有 6、vim messages_zh_CN.properties oVirt -> tmpCloud 六、add license 本地开发环境打包，打包结果如enginesso.war文件夹，将目录下所有文件拷贝至服务器engine部署目录，例如/usr/share/ovirt-engine/engine.ear/enginesso.war/，然后重启服务即可 org.ovirt.engine.core.sso.servlets.InteractiveAuthServlet /** * 注册码验证开始 */ boolean checkRes = true; try { String filepath = \"/etc/ovirt-engine/license\"; File file = new File(filepath); if(!file.exists()){ response.sendRedirect(request.getContextPath() + \"/fileNotException.html\"); return; } BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(filepath))); String firstLine = reader.readLine(); String reg = firstLine.substring(0,36); String mac2 = firstLine.substring(36); String mac = getStr1(reg) + mac2; String regCode = getStr2(reg); if(!(regCode.contains(\"base\") | regCode.contains(\"super\"))){ log.error(\"error user\"); response.sendRedirect(request.getContextPath() + \"/roleException.html\"); return; }else { Long timeString = Long.valueOf(regCode.substring(0,10)); Long nowtime = System.currentTimeMillis()/1000; if(nowtime>timeString){ log.error(\"error time\"); response.sendRedirect(request.getContextPath() + \"/timeException.html\"); return; } } if(regCode.contains(\"base\")){ String machineCodePath = \"/etc/ovirt-engine/product_uuid\"; reader = new BufferedReader(new InputStreamReader(new FileInputStream(machineCodePath))); String machineCode = reader.readLine().replace(\"/\",\"\"); String publicKey = RSAUtils.loadKeyByFile(\"/etc/ovirt-engine/license_public_key.pem\"); checkRes = RSAUtils.verify(BaseEncoding.base64().decode(publicKey), machineCode.getBytes(), BaseEncoding.base64().decode(mac)); if (checkRes == false){ response.sendRedirect(request.getContextPath() + \"/systemStand.html\"); return; } } reader.close(); } catch (Exception e) { log.error(e.getMessage()); log.error(\"-------------解析license信息发生异常--------------\"); response.sendRedirect(request.getContextPath() + \"/fileException.html\"); return; } 七、hosted-engine https://www.ovirt.org/develop/release-management/features/integration/heapplianceflow/ https://www.ovirt.org/blog/2016/07/Manage-Your-Hosted-Engine-Hosts-Deployment/ 1、安装部署hosted-engine yum -y install http://resources.ovirt.org/pub/yum-repo/ovirt-release41.rpm ==== nfs搭建 ==== systemctl restart nfs mkdir /data chown -R vdsm:kvm /data echo \"/data *(rw,sync,no_subtree_check,all_squash,anonuid=36,anongid=36)\"|tee > /etc/exports exportfs -r showmount -e yum install ovirt-hosted-engine-setup hosted-engine --deploy 2、在页面添加额外的host（主机） 先添加一个存储域 主机 → 新建 → 承载的引擎 → 选择承载引擎部署操作 → 部署 八、用户管理 ovirt存在两种用户域：本地域和外部域。平台安装过程中会创建一个“internal domain”默认本地域和默认用户admin。在本地域创建的用户为本地用户，在外部域创建的为目录用户 可以通过ovirt-aaa-jdbc-tool来创建和管理本地用户（aaa是Authentication, Authorization and Accounting的缩写） 1、外部域 1、搭建Active Directory和DNS 2、Active Directory 用户和计算机 → ad.com → Users → 右键（新建） → 用户 注意： 1、非管理账户 2、时钟同步 ovirt可以连接外部目录服务器添加外部域， 支持的目录服务器包括： 389ds 389ds RFC-2307 Schema Active Directory FreeIPA Red Hat Identity Management (IdM) Novell eDirectory RFC-2307 Schema OpenLDAP RFC-2307 Schema OpenLDAP Standard Schema Oracle Unified Directory RFC-2307 Schema RFC-2307 Schema (Generic) Red Hat Directory Server (RHDS) Red Hat Directory Server (RHDS) RFC-2307 Schema iPlanet ovirt使用插件ovirt-engine-extension-aaa-ldap来配置外部域。 参考https://superlc320.gitbooks.io/samba-ldap-centos7/ldap_+_centos_7.html 配置OpenLDAP服务。 运行 ovirt-engine-extension-aaa-ldap-setup 开启配置向导 参考 https://gi thub.com/oVirt/ovirt-engine-extension-aaa-ldap> 将/ad/. (Active Directory) 或者 examples/simple/. 下的文件复制到 /etc/ovirt-engine 并更改文件名和配置中的vars.* (用户名及密码等) 登录测试 # ovirt-engine-extensions-tool aaa login-user \\ --profile=@PROFILE@ --user-name=@USER@ Replace: - @PROFILE@ with authn ovirt.engine.aaa.authn.profile.name. - @USER@ with user you want to test. 2、本地域 ovirt使用插件ovirt-engine-extension-aaa-jdbc来添加和配置本地域。当自己添加aaa-jdbc文件时需要配置postgresql作为aaa-jdbc插件的数据库。 参考 https://github.com/oVirt/ovirt-engine-extension-aaa-jdbc 1. su - postgres -c \"psql -d template1 ovirt 通过ovirt-aaa-jdbc-tool创建和管理本地用户/组 https://access.redhat.com/documentation/zh-cn/red_hat_virtualization/4.1/html/administration_guide/sect-administering_user_tasks_from_the_commandline 创建一个新用户账户 ovirt-aaa-jdbc-tool user add test1 –attribute=firstName=John –attribute=lastName=Doe 设置密码 ovirt-aaa-jdbc-tool user password-reset test1 –password-valid-to=“2025-08-01 12:00:00-0800” 查看用户信息 ovirt-aaa-jdbc-tool user show test1 编辑用户信息 ovirt-aaa-jdbc-tool user edit test1 –attribute=email=jdoe@example.com 删除用户 ovirt-aaa-jdbc-tool user delete test1 为内部管理员用户重设密码 ovirt-aaa-jdbc-tool user password-reset admin –password-valid-to=“2025-08-01 12:00:00Z” 禁用内部管理员用户,禁用默认的 admin 用户 ovirt-aaa-jdbc-tool user edit admin –flag=+disabled 要启用一个已禁用的用户，运行 ovirt-aaa-jdbc-tool user edit username –flag=-disabled 管理组： 创建一个新组 ovirt-aaa-jdbc-tool group add group1 为组添加用户。这些用户需要已存在。 ovirt-aaa-jdbc-tool group-manage useradd group1 –user=test1 查看组账户详情 ovirt-aaa-jdbc-tool group show group1 列出所有用户或组账户的信息 ovirt-aaa-jdbc-tool query –what=user ovirt-aaa-jdbc-tool query –what=group 列出名字以字母 j 开头的用户账户信息 ovirt-aaa-jdbc-tool query –what=user –pattern=“name=j*” 列出部门属性被设置为 marketing 的组的信息 ovirt-aaa-jdbc-tool query –what=group –pattern=“department=marketing” 运行以下命令显示所有设置 ovirt-aaa-jdbc-tool setting show 以下命令把所有用户账户的默认登录会话时间改为 60 分钟。它的默认值时 10080 分钟 ovirt-aaa-jdbc-tool setting set –name=MAX_LOGIN_MINUTES –value=60 以下命令更新了把用户账户被锁定前可以进行登录操作的失败次数。它的默认值是 5 ovirt-aaa-jdbc-tool setting set –name=MAX_FAILURES_SINCE_SUCCESS –value=3 九、配置cinder 1.配置cinder provider 登录ovirt管理界面，点击Administration→providers，点击右上角的添加按钮，选择添加新的provider。 在名称栏填入ovirt-cinder(可修改), 类型选择：Openstack Block Storage， 供应商URL填写：http://http://192.168.21.100:8776 (此处为openstack cinder api服务IP地址)。 用户名:cinder， 密码：xxxxxxxx, 此处为cinder用户的密码，此密码从/etc/kolla/passwords.yml 文件中的cinder_keystone_password 获取。 租户名：service , 验证名： http://192.168.21.100:5000/v2.0 ，此处为openstackkeystone服务的认证URL。 编辑完成以后点击测试，当界面弹出测试成功，访问了供应商。则表示配置成功。 2.配置验证密钥 配置完成provider常规配置以后，还需要配置provider的验证密钥。 点击当前界面的验证密钥选项的右上角新建按钮。 在弹出的规划框界面，UUID 填入cinder的UUID。此UUID从openstack服务器上的cinder.conf获取。 rbd_user = cinder rbd_secret_uuid = UUID 3.值：填入此UUID对应的值。此值从ceph获取。获取方法： ceph auth get client.cinder [client.cinder] key = AQAZjXBZclIhAhAAH41aSF0GjlXEA4LBHYTEOg== 此处的key 就是我们需要的值。 填写完成以后点击确定按钮。 以上就是cinder provider的配置方法。 FAQ:rbd无法map(rbd feature disable) 参考解决方法：http://www.zphj1987.com/2016/06/07/rbd%E6%97%A0%E6%B3%95map-rbd-feature-disable/ 十、backup && restore environment ovirt version: 4.2.2 在互联网系统中，由于数据量很大，数据对企业来说就是企业的生命，因此数据的安全性就是企业首先要考虑的，解决数据安全性的首要措施就是对数据进行备份，对于ovirt engine来说可以对ovirt整个服务器采取备份，此种备份方式安全性最高，但是耗费的资源也比较大。另外一种解决方式就是对ovirt关键的文件进行备份，比如ovirt数据库和配置文件等，接下来我们就介绍ovirt对关键数据和配置文件进行备份。 Backup ovirt 提供专门的备份命令engine-backup 供管理员使用，具体的参数信息，用户可以使用如下命令查看： engine-backup –help 备份示例如下： engine-backup --mode=backup --file=backup1 --log=backup1.log #mode:表示使用的模式，这里有两个参数选择：backup（备份），restore（还原） #file:如果mode=backup ，则此处的file是指生成的备份文件。如果mode=restore，则此处表示已经存在的供还原的文件。 #log:备份过程中生成的日志文件。 执行备份命令以后在当前目录会生成一个名为backup1 的备份文件。 Restore 前提条件： \\1. 一台干净的服务器; \\2. 安装ovirt-engine 但是没有set-up \\3. 主机的hostname必须与备份主机的hostname保持一致 还原过程： \\1. 执行 Restore \\2. 执行 engine-setup 我们在一台新安装的服务器上还原刚刚备份的backup1，使用示例如下： 首先设置新服务器的网络，hostname等信息。将备份主机上生成的backup1文件和/opt/setup/answer 文件一起拷贝到新的主机。 yum install ovirt-engine engine-backup --mode=restore --log=restore1.log --file=backup1 --provision-db --provision-dwh-db --no-restore-permissions engine-setup --config-append=./answer --offline 执行玩上述三条命令以后，我们可以通过网页访问还原的主机https:{IP} 进入web界面我们可以看到此时还原的主机与之前备份的主机信息完全一致。 参考文档：https://ovirt.org/develop/release-management/features/integration/engine-backup/ 十一、QOS ovirt 中的 qos 包括存储、VM网络、主机网络、CPU，通过配置 qos 可以对可访问的资源进行精细的控制，创建 qos 的位置是: Compute --> 数据中心 --> 选择一个数据中心 --> qos 存储 存储服务质量定义了在一个存储域中的虚拟磁盘的最大吞吐级别和输入、输出操作的最大级别。为虚拟磁盘分配存储服务质量允许您对存储域的性能进行细化配置，并可以防止发生因为对一个虚拟磁盘的操作而影响到同一个存储域中的其它虚拟磁盘容量的问题。 创建 qos 吞吐量：每秒数据读写量 iops：每秒读写次数 使用 qos 在创建磁盘的时候需要选择一个磁盘配置集，而磁盘配置集可以设置 qos 属性，磁盘配置集的创建以及 qos 的创建位置是: Storage --> Domains --> 选择一个数据存储域 --> 磁盘配置集 创建一个配置集 VM网络 通过设置虚拟机网络服务质量，用户可以使用一个配置集来控制独立虚拟网络控制器的网络流量。它可以控制不同网络层上的带宽，并控制网络资源使用的情况。 创建 qos 转入的：这个设置对流入网络的流量进行控制。选择/取消转入的选项来启用/禁用这个设置。 平均值：流入网络流量的平均速度。 高峰：流入网络流量的峰值速度。 Burst：burst 发生时的流入网络流量的速度。 转出的：这个设置对流出网络的流量进行控制。选择/取消转出的选项来启用/禁用这个设置。 平均值：流入网络流量的平均速度。 高峰：流入网络流量的峰值速度。 Burst：burst 发生时的流入网络流量的速度。 使用 qos 创建虚拟机的时候需要选择一个 vnic 配置集，而 vnic 配置集可以设置 qos 属性，创建 vnic 配置集的位置是: Network --> vNIC Profiles 主机网络 主机网络服务器质量配置主机上的网络来控制通过物理接口的网络流量。使用主机网络服务质量可以通过控制在相同的物理网络接口控制器中的网络资源使用情况来对网络性能进行微调。这可以防止，因为物理网络上的一个网络有太多的网络数据造成同一个物理网络上的其它网络无法正常工作的情况出现。通过配置主机网络服务质量，可以使在同一个物理网络上的不同网络都正常工作。 创建 qos 转出的：应用到转出的网络流量的设置。 加权重的共享：指定一个特定网络应该被分配的逻辑连接的容量（相对于连接到同一个逻辑连接的其它网络）。准确的共享取决于在这个连接上的所有网络共享的总和。在默认情况下，它的值在 1 到 100 之间。 速率限制 [Mbps]：一个网络可以使用的最大带宽。 实现的速率 [Mbps]：一个网络所需的最小带宽。实现的速率并不一定可以被保证，它取决于网络的架构以及同一个逻辑连接上的其它网络的“实现的速率”设置。 使用 qos 主机网络的 qos 的使用是在创建网络的属性中进行设置的，具体的位置是: Network --> 新建 CPU CPU 服务质量定义了一个集群中的虚拟机可以从运行它的主机上获得的最大计算处理能力（以所占主机的所有计算处理能力的百分比表示）。通过为一个虚拟机关联一个 CPU 服务质量，可以防止因为集群中的一个虚拟机的负载占用太多计算处理资源而影响到同一集群中其它虚拟机可以使用的资源。 创建 qos 限制：指所允许的最大处理能力（以百分比的形式，但不要包括 % 符号） 使用 qos 在创建虚拟机或者编辑虚拟机的资源分配一项中可以设置 cpu 配置集，而 cpu 配置集可以设置 cpu qos 属性，创建 cpu 配置集的位置是: Compute --> Cluster --> 选择一个 cluster --> CPU 配置集 新建一个配置集 十二、使用nat网络 /etc/libvirt/qemu/networks/ 1、创建NAT网络配置文件/etc/libvirt/qemu/networks/nat.xml,内容如下 nat b09d09a8-ebbd-476d-9045-e66012c9e83d 2、通过libvirt/virsh创建NAT网络 [root@ovirthost01 ~]# cat /etc/pki/vdsm/keys/libvirt_password shibboleth [root@ovirthost01 ~]# virsh Welcome to virsh, the virtualization interactive terminal. Type: 'help' for help with commands 'quit' to quit virsh # connect qemu:///system Please enter your authentication name: vdsm@ovirt Please enter your password: shibboleth virsh # net-list Name State Autostart Persistent ---------------------------------------------------------- ;vdsmdummy; active no no vdsm-ovirtmgmt active yes yes virsh # net-define /etc/libvirt/qemu/networks/nat.xml Network nat defined from /etc/libvirt/qemu/networks/nat.xml virsh # net-autostart nat Network nat marked as autostarted virsh # net-start nat Network nat started virsh # net-list --all Name State Autostart Persistent ---------------------------------------------------------- ;vdsmdummy; active no no nat active yes yes vdsm-ovirtmgmt active yes yes 以上操作将创建nat功能的网桥，如下 [root@ovirthost01 ~]# brctl show bridge name bridge id STP enabled interfaces ;vdsmdummy; 8000.000000000000 no natbr0 8000.5254009d82de yes natbr0-nic ovirtmgmt 8000.b083fea27fed no p4p1 3、安装vdsm-hook-extnet [root@ovirthost01 ~]# yum install -y vdsm-hook-extnet 注：此处将下载extnet的hooks文件并存放到以下两目录 [root@ovirthost01 ~]# ll /usr/libexec/vdsm/hooks/before_device_create total 4 -rwxr-xr-x. 1 root root 1925 Jun 5 01:47 50_extnet [root@ovirthost01 ~]# ll /usr/libexec/vdsm/hooks/before_nic_hotplug total 4 -rwxr-xr-x. 1 root root 1925 Jun 5 01:47 50_extnet 4、添加自定义设备属性extnet [root@ovirthost01 ~]# engine-config -s CustomDeviceProperties='{type=interface;prop={extnet=^[a-zA-Z0-9_ ---]+$}}' Please select a version: 1. 3.0 2. 3.1 3. 3.2 4. 3.3 5. 3.4 6. 3.5 6 [root@ovirthost01 ~]# engine-config -g CustomDeviceProperties CustomDeviceProperties: version: 3.0 CustomDeviceProperties: version: 3.1 CustomDeviceProperties: version: 3.2 CustomDeviceProperties: version: 3.3 CustomDeviceProperties: {type=interface;prop={SecurityGroups=^(?:(?:[0-9a-fA-F]{8}-(?:[0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12}, *)*[0-9a-fA-F]{8}-(?:[0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12}|)$}} version: 3.4 CustomDeviceProperties: {type=interface;prop={extnet=^[a-zA-Z0-9_ ---]+$}} version: 3.5 [root@ovirthost01 ~]# systemctl restart ovirt-engine 十三、ovirt-shell https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.1/html/rhevm_shell_guide/chap-using_the_cli https://www.ovirt.org/develop/release-management/features/infra/cli/ yum -y install ovirt-engine-cli pip install ovirt-shell vim .ovirtshellrc [cli] autoconnect = True autopage = True [ovirt-shell] username = admin@internal renew_session = False timeout = -1 extended_prompt = False url = https://192.168.21.12/ovirt-engine/api insecure = True kerberos = False filter = False session_timeout = None ca_file = /etc/pki/ovirt-engine/ca.pem dont_validate_cert_chain = False key_file = None password = qwe cert_file = None ovirt-shell -E 'help' 连接 ovirt-shell -c -l \"https://192.168.21.12/ovirt-engine/api\" -P 443 -u admin@internal -I 列出资源 list vms list vms --show-all 使用过滤查询列出资源 list vms --query \"name=centos*\" list vms --kwargs \"memory=1073741824\" 列出子资源 list disks --vm-identifier nfs_desktop list nics --vm-identifier demo list disks --vm-identifier nfs_desktop --kwargs \"name=Disk 3\" list vms --kwargs \"usb-enabled=True\" 查看资源详细信息 show vm centos7 show vm --name nfs_desktop show vm --id f4a51ae1-4f31-45ee-ab6d-d5965e3bcf71 查看子资源详细信息 show nic nic1 --vm-identifier demo 添加资源 add vm --name demo2 --template-name iscsi_desktop_tmpl --cluster-name Default_iscsi add datacenter --name mydc --storage_type nfs --version-major 3 --version-minor 1 添加子资源 add nic --vm-identifier demo2 --network-name engine --name mynic 移除资源 remove vm aa 移除子资源 remove disk \"Disk 1\" --vm-identifier demo2 更新资源 update vm iscsi_desktop --description iscsi_desktop_desc update vm iscsi_desktop --display-monitors 2 --description test1 更新子资源 update nic nic1 --vm-identifier demo --interface virtio 处理 action vm demo start --vm-display-type vnc --async true 移除子资源 action nic bond0 attach --host-identifier grey-vdsa 控制台 console 'my_vm' console '7dff8517-7007-42cd-9cf7-b7a13a9d96b7' action vm centos7 stop 创建数据中心 add datacenter --name test --comment \"test\" --description \"test\" --storage_type nfs --version-major 3 --version-minor 3 创建集群 add cluster --data_center-name test --name test --comment \"test\" 添加主机到数据中心 list hosts --query \"status=maintenance\" update host node-123 --cluster-name test 添加存储 # add storagedomain --name test --storage-type nfs --storage_format v3 --type data --host-name node-123 --storage-address 192.168.3.157 --storage-path /home/ovirt/nfs-test add storagedomain --name test --datacenter-identifier test 添加ISO add storagedomain --name iso --datacenter-identifier test 创建虚拟机 add vm --name test --template-name Blank --cluster-name test 给虚拟机新分配一个磁盘 add disk --name test_disk --size 10737418240 --interface virtio list disks --query \"alias=test_disk\" | grep id add disk --id 4357a838-2a99-4a05-a0b9-be6ee6ec5eaa --vm-identifier test 给虚拟机新分配一个网卡 add nic --vm-identifier test --name test --network-name ovirtmgmt 启动虚拟机 先列出 ISO 域名里可用的的镜像. list files --storagedomain-identifier isoid 给虚拟机增加一个 CDROM 设备. add cdrom --vm-identifier test --file-id CentOS-6.5-x86_64-minimal.iso 启动虚拟机, 以 CDROM 作为启动设备 action vm test start --vm-stateless true --vm-os-boot \"boot.dev=cdrom\" --vm-display-ty 访问虚拟机 action vm test ticket --ticket-value \"abc123\" show vm test | grep displaydisplay-address 可以用 vnc 客户端打开 192.168.3.123:5900 加上上面设置的临时密码来访问该虚拟机了 脚本 less /home/mpastern/script -------------------------- list vms show vm test | grep status list vms --query \"name=test*\" --show-all | grep status list clusters list datacenters ... 执行脚本 From linux shell [mpastern@lp /]# ovirt-shell -f /home/mpastern/script From ovirt shell [oVirt shell (connected)]# file /home/mpastern/script sdk yum -y install gcc libxml2-devel python-devel yum -y install ovirt-engine-cli import logging import ovirtsdk4 as sdk import ovirtsdk4.types as types logging.basicConfig(level=logging.DEBUG, filename='example.log') connection = sdk.Connection( url='https://alpha/ovirt-engine/api', username='admin@internal', password='qwe', ca_file='/etc/pki/ovirt-engine/ca.pem', debug=True, log=logging.getLogger(), ) vms_service = connection.system_service().vms_service() vms = vms_service.list() for vm in vms: print(\"%s: %s\" % (vm.name, vm.id)) connection.close() url=\"https://alpha/ovirt-engine/api\" user=\"admin@internal\" password=\"qwe\" vmid=\"5e2bf5a7-3e59-47d8-9f86-472a3d51a06e\" curl -X POST \\ --insecure \\ --header \"Accept: application/xml\" \\ --header \"Content-type: application/xml\" \\ --user \"${user}:${password}\" \\ -d \"\" \\ \"${url}/vms/${vmid}/shutdown\" rest api https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_virtualization/3.6/html/rest_api_guide/ https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.0/html-single/rest_api_guide/index url=\"https://engine.example.com/ovirt-engine/api\" user=\"admin@internal\" password=\"...\" curl \\ --verbose \\ --cacert /etc/pki/ovirt-engine/ca.pem \\ --user \"${user}:${password}\" \\ --request POST \\ --header \"Version: 4\" \\ --header \"Content-Type: application/xml\" \\ --header \"Accept: application/xml\" \\ --data ' myvm Blank mycluster ' \\ \"${url}/vms\" POST /ovirt-engine/api/vms/123/migrate inherit auto inherit 十四、其他问题 1、The client is not authorized to request an authorization. It's required to access the system using FQDN. /etc/ovirt-engine/engine.conf.d/11-setup-sso.conf SSO_ALTERNATE_ENGINE_FQDNS systemctl restart ovirt-engine.service 2、跨域 $ ssh root@[ENGINE_FQDN] # log as `root` user in the oVirt engine machine # engine-config -s CORSSupport=true # to turn on the CORS support for REST API # engine-config -s CORSAllowDefaultOrigins=true # to allow CORS for all configured hosts # systemctl restart ovirt-engine # to take effect engine-config -s CORSAllowedOrigins=* engine-config -s CORSSupport=true engine-config -l |grep CORS engine-config -g CORSAllowedOrigins 3、迁移bug 迁移时，未完全开始就迁移，虚拟机状态可能会一直是migrate to，正常应该为Powering Up到Up 4、v2v yum -y install virt-v2v virsh list --all vdsm@ovirt shibboleth 虚拟机需要关机 转换并导出到导出域（需要保证虚拟机是正常的） virt-v2v -i libvirt -o rhev -os 192.168.21.12:/ovirt_export --network ovirtmgmt hl_v2v_test 在导入域中导入虚拟机 p2v https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/v2v_guide/chap-v2v_guide-p2v_migration_converting_physical_machines_to_virtual_machines http://blog.csdn.net/lbwlh/article/details/51003595 5、node 注册 vdsm-tool register --engine-fqdn IP_ADDRESS --check-fqdn false 6、创建用户 ovirt-aaa-jdbc-tool user add hl ovirt-aaa-jdbc-tool user password-reset hl --password-valid-to=\"2025-01-01 00:00:00-0800\" 十五、ovirt virsh vdsm@ovirt #yes shibboleth "},"notes/other/mqtt.html":{"url":"notes/other/mqtt.html","title":"MQTT 相关","keywords":"","body":"MQTT 相关 一、MQTT 安装 1、下载 wget https://emqx.io/static/brokers/emqttd-docker-v2.3.11.zip --no-check-certificate unzip emqttd-docker-v2.3.11.zip docker load emqttd-docker-v2.3.11 https://github.com/emqx/emqx-docker docker pull devicexx/emqttd http://emqtt.com/downloads 2、启动mqtt # 1.直接容器启动 docker run -tid --name emqtt -p 1883:1883 -p 8083:8083 -p 8883:8883 -p 8084:8084 -p 18083:18083 emqttd-docker-v2.3.11 docker run --rm -ti --name emq -p 18083:18083 -p 1883:1883 -p 4369:4369 -p 6000-6100:6000-6100 \\ -e EMQ_NAME=\"emq1\" \\ -e EMQ_HOST=\"172.17.0.3\" \\ -e EMQ_LISTENER__TCP__EXTERNAL=1883 \\ -e EMQ_JOIN_CLUSTER=\"emq@172.17.0.2\" \\ emq # 2.docker swarm 启动 version: '3.5' configs: haproxy_config: file: /tmp/tmpxzqwwynp networks: emq_net_a0087c90-78ac-420a-86cd-b5f1a1720625: driver: overlay name: emq_net_a0087c90-78ac-420a-86cd-b5f1a1720625 services: haproxy: configs: - source: haproxy_config target: /usr/local/etc/haproxy/haproxy.cfg depends_on: - my_mqtt1 - my_mqtt2 - my_mqtt3 environment: MQTT_ADDRS: my_mqtt1.emq.tt my_mqtt2.emq.tt my_mqtt3.emq.tt hostname: haproxy image: 192.168.21.12:5000/tmp/haproxy:1.9 labels: - com.tmp.mqtt.role=haproxy networks: emq_net_a0087c90-78ac-420a-86cd-b5f1a1720625: aliases: - haproxy ports: - mode: ingress protocol: tcp published: 20201 target: 1883 - mode: ingress protocol: tcp published: 20202 target: 18083 my_mqtt1: environment: EMQ_HOST: my_mqtt1.emq.tt EMQ_LISTENER__TCP__EXTERNAL: 1883 EMQ_NAME: emq healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - localhost - '1883' timeout: 10s image: 192.168.21.12:5000/tmp/emqttd:2.3.11 networks: emq_net_a0087c90-78ac-420a-86cd-b5f1a1720625: aliases: - my_mqtt1.emq.tt my_mqtt2: depends_on: - my_mqtt1 environment: EMQ_HOST: my_mqtt2.emq.tt EMQ_JOIN_CLUSTER: emq@my_mqtt1.emq.tt EMQ_LISTENER__TCP__EXTERNAL: 1883 EMQ_NAME: emq healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - localhost - '1883' timeout: 10s image: 192.168.21.12:5000/tmp/emqttd:2.3.11 networks: emq_net_a0087c90-78ac-420a-86cd-b5f1a1720625: aliases: - my_mqtt2.emq.tt my_mqtt3: depends_on: - my_mqtt1 environment: EMQ_HOST: my_mqtt3.emq.tt EMQ_JOIN_CLUSTER: emq@my_mqtt1.emq.tt EMQ_LISTENER__TCP__EXTERNAL: 1883 EMQ_NAME: emq healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - localhost - '1883' timeout: 10s image: 192.168.21.12:5000/tmp/emqttd:2.3.11 networks: emq_net_a0087c90-78ac-420a-86cd-b5f1a1720625: aliases: - my_mqtt3.emq.tt 启动 docker stack deploy -c docker-compose.yml mqtt docker config create site-v2.conf site.conf docker service create --name emqtt -p 1883:1883 -p 8083:8083 -p 8883:8883 -p 8084:8084 -p 8080:8080 -p 18083:18083 --config src=emq.conf,target=/opt/emqttd/etc/emq.conf emq docker service update --config-rm emq.conf --config-add source=emq1,target=/opt/emqttd/etc/emq.conf emqtt docker service update \\ --config-rm 405a57ac-616f-448d-b103-7cfc3a8734e0_haproxy_config \\ --config-add source=haproxy,target=/usr/local/etc/haproxy/haproxy.cfg \\ 7kwtrjr320s6 docker network create \\ --driver overlay \\ --ingress \\ --subnet=10.10.10.0/24 \\ --opt com.docker.network.driver.mtu=1500 \\ myingress docker run -tid --name emqtt_hl -p 31883:1883 -p 38083:18083 --network u8idzog4wrw6 emqttd-docker-v2.3.11 3、haproxy配置 global daemon log 127.0.0.1 local0 info stats socket /usr/local/etc/haproxy/haproxy.sock maxconn 2000000 defaults log global mode http option redispatch option httplog option forwardfor retries 10 timeout http-request 10s timeout connect 10h timeout client 10h timeout server 10h maxconn 2000000 listen dashborad bind *:18083 balance source http-request del-header X-Forwarded-Proto if { ssl_fc } server mqtt1.emq.tt mqtt1.emq.tt:18083 check inter 10000 fall 2 rise 5 weight 1 server mqtt2.emq.tt mqtt2.emq.tt:18083 check inter 10000 fall 2 rise 5 weight 1 server mqtt3.emq.tt mqtt3.emq.tt:18083 check inter 10000 fall 2 rise 5 weight 1 listen api bind *:1883 mode tcp option tcplog balance source http-request del-header X-Forwarded-Proto if { ssl_fc } server mqtt1.emq.tt mqtt1.emq.tt:1883 check inter 10000 fall 2 rise 5 weight 1 server mqtt2.emq.tt mqtt2.emq.tt:1883 check inter 10000 fall 2 rise 5 weight 1 server mqtt3.emq.tt mqtt3.emq.tt:1883 check inter 10000 fall 2 rise 5 weight 1 4、mqtt端口相关 18083 Dashboard 管理控制台端口 1883 MQTT 协议端口 8083 MQTT/WebSocket 端口 1883 MQTT Port 8883 MQTT/SSL Port 8083 MQTT/WebSocket Port 8084 MQTT/WebSocket/SSL Port 8080 HTTP Management API Port 5、常用命令 #查看状态 docker exec -itu0 emqtt1 ./bin/emqttd_ctl status #加入集群 docker run -itd --name emqtt2 --link emqtt1 devicexx/emqttd #离开集群 docker exec -itu0 emqtt2 ./bin/emqttd_ctl cluster leave #emqttd1 节点下删除 emqttd2: docker exec -itu0 emqtt1 ./bin/emqttd_ctl cluster remove emqttd2@127.0.0.1 #修改密码 docker exec -itu0 emqtt1 ./bin/emqttd_ctl admins passwd admin 123456 使用文档 http://emqtt.com/docs/v2/getstarted.html 6、编译mqtt镜像 git clone -b v2.3.11 https://github.com/emqtt/emq_docker.git cd emq_docker docker build -t emq:latest . 7、服务器优化 cat > /etc/sysctl.conf fs.file-max=2097152 fs.nr_open=2097152 net.core.somaxconn=32768 net.ipv4.tcp_max_syn_backlog=16384 net.core.netdev_max_backlog=16384 net.ipv4.ip_local_port_range=1000 65535 net.core.rmem_default=262144 net.core.wmem_default=262144 net.core.rmem_max=16777216 net.core.wmem_max=16777216 net.core.optmem_max=16777216 net.ipv4.tcp_rmem=1024 4096 16777216 net.ipv4.tcp_wmem=1024 4096 16777216 net.nf_conntrack_max=1000000 net.netfilter.nf_conntrack_max=1000000 net.netfilter.nf_conntrack_tcp_timeout_time_wait=30 net.ipv4.tcp_max_tw_buckets=1048576 net.ipv4.tcp_fin_timeout = 15 EOF cat >/etc/security/limits.conf * soft nofile 1048576 * hard nofile 1048576 EOF echo DefaultLimitNOFILE=1048576 >>/etc/systemd/system.conf echo 5097152 > /proc/sys/fs/nr_open sysctl -w kernel.msgmnb=65536 sysctl -w kernel.msgmax=65536 sysctl -w kernel.shmmax=68719476736 sysctl -w kernel.shmall=4294967296 sysctl -w net.ipv4.tcp_fin_timeout=30 sysctl -w fs.nr_open=5097152 sysctl -w fs.file-max=5097152 sysctl -w net.ipv4.tcp_tw_recycle=1 sysctl -w net.ipv4.tcp_tw_reuse=1 sysctl -w net.core.rmem_default=524288 sysctl -w net.core.wmem_default=524288 sysctl -w net.core.rmem_max=67108864 sysctl -w net.core.wmem_max=67108864 sysctl -w net.core.optmem_max=67108864 sysctl -w net.ipv4.tcp_rmem='4096 87380 16777216' sysctl -w net.ipv4.tcp_wmem='4096 65536 16777216' sysctl -w net.ipv4.ip_local_port_range='1024 65535' sysctl -w net.core.somaxconn=32768 sysctl -w net.ipv4.tcp_max_syn_backlog=16384 sysctl -w net.core.netdev_max_backlog=16384 sysctl -w net.nf_conntrack_max=1000000 sysctl -w net.netfilter.nf_conntrack_max=1000000 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=30 sysctl -w net.ipv4.tcp_fin_timeout=15 ulimit -n 200000 二、MQTT使用与测试 1、使用 1.安装 yum install -y mosquitto systemctl start mosquitto 2.使用 # emqttd1节点上订阅x mosquitto_sub -t x -q 1 -p 1883 # emqttd2节点上向x发布消息 mosquitto_pub -t x -q 1 -p 1883 -m hello 2、测试 https://github.com/emqtt/emqtt_benchmark netstat -nat|grep -i \"1883\"|wc -l 1.安装 yum -y install gcc gcc-c++ glibc-devel make ncurses-devel openssl-devel autoconf java-1.8.0-openjdk-devel git git clone https://github.com/erlang/otp.git cd otp ./otp_build autoconf ./configure make make install git clone https://github.com/erlang/rebar3.git cd rebar3 ./bootstrap cp rebar3 /usr/local/bin git clone https://github.com/emqtt/emqtt_benchmark.git make 2.使用 ./emqtt_bench_pub -t bench/%i -h 192.168.21.204 -p 20155 -c 2300 ./emqtt_bench_sub -t bench/%i -h 192.168.21.204 -p 20155 -c 2000 3.参数解释 //服务器ip地址 -h, --host mqtt server hostname or IP address [default: localhost] //服务器端口号 -p, --port mqtt server port number [default: 1883] //最大连接客户端数量 默认200 -c, --count max count of clients [default: 200] //客户端连接间隔时间，默认10毫秒 -i, --interval interval of connecting to the broker [default: 10] //订阅的主题 %i=自增长序号 -t, --topic topic subscribe, support %u, %c, %i variables //消息服务qos等级， //0=最多一次 服务器与 客户端 交互1次 //1=至少一次 服务器与 客户端 交互2次 //2=仅有一次 服务器与 客户端 交互4次 -q, --qos subscribe qos [default: 0] //客户端用户名 -u, --username username for connecting to server //用户端密码 -P, --password password for connecting to server //维持客户端活跃的时间 默认300秒 -k, --keepalive keep alive in seconds [default: 300] //客户端断开后是否清除session 默认true -C, --clean clean session [default: true] //代理ip接口 --ifaddr local ipaddress or interface address "},"notes/other/kafka.html":{"url":"notes/other/kafka.html","title":"Kafka 相关","keywords":"","body":"Kafka 相关 一、kafka介绍 一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区。 Produce 将消息发布到它指定的topic中，并负责决定发布到哪个分区。通常简单的由负责均衡机制随机选择分区，但也可以通过特定的分区函数选择分区。使用更多的是第二种。 consumer 发布消息通常有两种模式：队列模式（queuing）和发布-订阅模式(publish-subscribe)。队列模式中，consumers可以同时从服务端读取消息，每个消息只被其中一个consumer读到；发布-订阅模式中消息被广播到所有的consumer中。Consumers可以加入一个consumer 组，共同竞争一个topic，topic中的消息将被分发到组中的一个成员中。同一组中的consumer可以在不同的程序中，也可以在不同的机器上。如果所有的consumer都在一个组中，这就成为了传统的队列模式，在各consumer中实现负载均衡。如果所有的consumer都不在不同的组中，这就成为了发布-订阅模式，所有的消息都被分发到所有的consumer中。更常见的是，每个topic都有若干数量的consumer组，每个组都是一个逻辑上的“订阅者”，为了容错和更好的稳定性，每个组由若干consumer组成。这其实就是一个发布-订阅模式，只不过订阅者是个组而不是单个consumer。 相比传统的消息系统，Kafka可以很好的保证有序性。 传统的队列在服务器上保存有序的消息，如果多个consumers同时从这个服务器消费消息，服务器就会以消息存储的顺序向consumer分发消息。虽然服务器按顺序发布消息，但是消息是被异步的分发到各consumer上，所以当消息到达时可能已经失去了原来的顺序，这意味着并发消费将导致顺序错乱。为了避免故障，这样的消息系统通常使用“专用consumer”的概念，其实就是只允许一个消费者消费消息，当然这就意味着失去了并发性。 在这方面Kafka做的更好，通过分区的概念，Kafka可以在多个consumer组并发的情况下提供较好的有序性和负载均衡。将每个分区分只分发给一个consumer组，这样一个分区就只被这个组的一个consumer消费，就可以顺序的消费这个分区的消息。因为有多个分区，依然可以在多个consumer组之间进行负载均衡。注意consumer组的数量不能多于分区的数量，也就是有多少分区就允许多少并发消费。 Kafka只能保证一个分区之内消息的有序性，在不同的分区之间是不可以的，这已经可以满足大部分应用的需求。如果需要topic中所有消息的有序性，那就只能让这个topic只有一个分区，当然也就只有一个consumer组消费它。 二、安装 1、拉取zookeeper、kafka、kafka-manager镜像 docker pull zookeeper docker pull ches/kafka docker pull sheepkiller/kafka-manager 2、使用docker启动相关容器 docker network create kafka-net docker run -d --name zookeeper --network kafka-net -p 2181:2181 zookeeper docker run -d --name kafka --network kafka-net --env ZOOKEEPER_IP=zookeeper -p 7203:7203 -p 9092:9092 ches/kafka #外网访问 docker run -d \\ --hostname localhost \\ --name kafka \\ --publish 9092:9092 --publish 7203:7203 \\ --env KAFKA_ADVERTISED_HOST_NAME=110.202.198.96 --env ZOOKEEPER_IP=110.202.198.96 --env KAFKA_ADVERTISED_PORT=50007 --env ZOOKEEPER_PORT=50008 \\ ches/kafka 3、使用docker swarm启动相关容器 version: '3.5' networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: driver: overlay name: kafaka_net_218aacad-5a9d-4f70-86a4-309031c76ea8 services: my_kafka1: depends_on: - my_kafka_zk1 - my_kafka_zk2 - my_kafka_zk3 environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.21.192 KAFKA_ADVERTISED_PORT: 20229 KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181, healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - my_kafka_zk1 - '2181' timeout: 10s image: 192.168.21.12:5000/tmp/kafka:2.12-2.1.0 labels: - com.tmp.kafka.role=kafka networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka1 ports: - mode: ingress protocol: tcp published: 20229 target: 9092 my_kafka2: depends_on: - my_kafka_zk1 - my_kafka_zk2 - my_kafka_zk3 environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.21.192 KAFKA_ADVERTISED_PORT: 20230 KAFKA_BROKER_ID: 2 KAFKA_ZOOKEEPER_CONNECT: my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181, healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - my_kafka_zk1 - '2181' timeout: 10s image: 192.168.21.12:5000/tmp/kafka:2.12-2.1.0 labels: - com.tmp.kafka.role=kafka networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka2 ports: - mode: ingress protocol: tcp published: 20230 target: 9092 my_kafka3: depends_on: - my_kafka_zk1 - my_kafka_zk2 - my_kafka_zk3 environment: KAFKA_ADVERTISED_HOST_NAME: 192.168.21.192 KAFKA_ADVERTISED_PORT: 20231 KAFKA_BROKER_ID: 3 KAFKA_ZOOKEEPER_CONNECT: my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181, healthcheck: interval: 5s retries: 30 test: - CMD - nc - -z - my_kafka_zk1 - '2181' timeout: 10s image: 192.168.21.12:5000/tmp/kafka:2.12-2.1.0 labels: - com.tmp.kafka.role=kafka networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka3 ports: - mode: ingress protocol: tcp published: 20231 target: 9092 my_kafka_kafka-manager: depends_on: - my_kafka_zk1 - my_kafka_zk2 - my_kafka_zk3 environment: ZK_HOSTS: my_kafka_zk1:2181,my_kafka_zk2:2181,my_kafka_zk3:2181, healthcheck: interval: 10s retries: 30 test: - CMD-SHELL - curl 127.0.0.1:9000 | grep Active timeout: 15s image: 192.168.21.12:5000/tmp/kafka-manager:1.3.1.8 labels: - com.tmp.kafka.role=kafka_manager networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka_kafka-manager ports: - mode: ingress protocol: tcp published: 20232 target: 9000 my_kafka_zk1: environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=my_kafka_zk1:2888:3888 server.2=my_kafka_zk2:2888:3888 server.3=my_kafka_zk3:2888:3888 hostname: my_kafka_zk1 image: 192.168.21.12:5000/tmp/zookeeper:3.4.13 labels: - com.tmp.kafka.role=zookeeper networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka_zk1 ports: - mode: ingress protocol: tcp published: 20226 target: 2181 my_kafka_zk2: environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=my_kafka_zk1:2888:3888 server.2=my_kafka_zk2:2888:3888 server.3=my_kafka_zk3:2888:3888 hostname: my_kafka_zk2 image: 192.168.21.12:5000/tmp/zookeeper:3.4.13 labels: - com.tmp.kafka.role=zookeeper networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka_zk2 ports: - mode: ingress protocol: tcp published: 20227 target: 2181 my_kafka_zk3: environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=my_kafka_zk1:2888:3888 server.2=my_kafka_zk2:2888:3888 server.3=my_kafka_zk3:2888:3888 hostname: my_kafka_zk3 image: 192.168.21.12:5000/tmp/zookeeper:3.4.13 labels: - com.tmp.kafka.role=zookeeper networks: kafka_net_218aacad-5a9d-4f70-86a4-309031c76ea8: aliases: - my_kafka_zk3 ports: - mode: ingress protocol: tcp published: 20228 target: 2181 docker stack deploy -c kafka.yaml my_kafka 4、使用 下载kafka yum -y install java-1.8.0-openjdk curl -O http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.3.0/kafka_2.12-2.3.0.tgz tar -zxvf kafka_2.12-2.3.0.tgz cd kafka_2.12-2.3.0/bin ./kafka-topics.sh --list --zookeeper 192.168.110.224:12181,192.168.110.224:22181,192.168.110.224:32181 ./kafka-topics.sh --create --topic test --replication-factor 1 --partitions 1 --zookeeper 192.168.110.224:12181,192.168.110.224:22181,192.168.110.224:32181 ./kafka-console-consumer.sh --topic test --from-beginning --bootstrap-server 192.168.110.224:19092,192.168.110.224:29092,192.168.110.224:39092 ./kafka-console-producer.sh --topic test --broker-list 192.168.110.224:19092,192.168.110.224:29092,192.168.110.224:39092 #查看topic docker run --rm --network kafka-net ches/kafka kafka-topics.sh --list --zookeeper zookeeper:2181 #查看topic详情 docker run --rm --network kafka-net ches/kafka kafka-topics.sh --describe --zookeeper zookeeper:2181 --topic test 5、更新env允许外网能访问 https://docs.docker.com/engine/reference/commandline/service_update/ docker service ls|grep kafka docker service update --env-add KAFKA_ADVERTISED_HOST_NAME=61.136.145.19 --env-add ZOOKEEPER_IP=61.136.145.19 --env-add KAFKA_ADVERTISED_PORT=20029 --env-add ZOOKEEPER_PORT=20026 m4yxp38h3jrs 6、kafkacat github kafkacat # Publish kafkacat -b 192.168.1.2:32000,192.168.1.2:32001 -P -t test # Consume kafkacat -b 192.168.1.2:32000,192.168.1.2:32001 -C -t test 7、helm kafka # 外部可访问 externalAccess: enabled: true service: type: NodePort port: 19092 loadBalancerIPs: [] loadBalancerSourceRanges: [] nodePorts: [31117,31118,31119] domain: 61.136.145.205 # 持续存储 persistence: enabled: true storageClass: \"ceph-rbd\" accessModes: - ReadWriteOnce size: 8Gi "},"notes/other/pyqt5.html":{"url":"notes/other/pyqt5.html","title":"PyQt5 相关","keywords":"","body":"PyQt5 相关 1、Qt Creater qt-creator-opensource-windows-x86_64-4.5.0 http://blog.csdn.net/Angelasan/article/details/44917283 designer.exe 把form.ui文件编译为form.py文件 到之前保存form.ui的目录，shift+右键，在当前路径打开控制台，执行如下命令 pyuic5 form.ui -o form.py 2、安装pyqt5 pip install PyQt5 3、打包工具 http://blog.csdn.net/lqzdreamer/article/details/77917493 pip install pyinstaller pyinstaller YOUR_APP.py http://legendtkl.com/2015/11/06/pyinstaller/ 打包成一个文件,可以用onefile参数将所有文件打包到一个可执行文件中。 pyinstaller --onefile --noconsole script.py pyinstaller -F -w script.py pip install py2exe pip install cx_Freeze import sys from cx_Freeze import setup, Executable # Dependencies are automatically detected, but it might need fine tuning. build_exe_options = {\"packages\": [\"os\"], \"excludes\": [\"tkinter\"]} # GUI applications require a different base on Windows (the default is for a # console application). base = None if sys.platform == \"win32\": base = \"Win32GUI\" setup( name = \"test\", version = \"1.0\", description = \"My GUI application!\", options = {\"build_exe\": build_exe_options}, executables = [Executable(\"test.py\", base=base)]) python setup.py build 4、pyqt相关问题 #禁止最大化按钮 MainWindow.setWindowFlags(QtCore.Qt.WindowMinimizeButtonHint) #禁止拉伸窗口大小 MainWindow.setFixedSize(MainWindow.width(), MainWindow.height()); #设置最小化与最大化按钮 self.setWindowFlags(QtCore.Qt.Window) Win10上Python3通过pip安装时出现UnicodeDecodeError 解决方法： 打开 c:\\program files\\python36\\lib\\site-packages\\pip\\compat\\__init__.py 约75行 return s.decode('utf_8') 改为return s.decode('cp936') 原因： 编码问题，虽然py3统一用utf-8了。但win下的终端显示用的还是gbk编码。 pyqt5学习地址 http://code.py40.com/face "},"notes/other/ionic_cordova.html":{"url":"notes/other/ionic_cordova.html","title":"ionic 、cordova  编译 andriod","keywords":"","body":"ionic 、cordova 编译 andriod 1、安装npm（略） 2、安装ionic cordova npm install -g ionic cordova 3、安装gradle wget https://services.gradle.org/distributions/gradle-4.10-bin.zip cd /hl/ unzip gradle-4.10-bin.zip export PATH=$PATH:/hl/gradle-4.10/bin 4、下载android SDK cd /opt && wget https://dl.google.com/android/repository/sdk-tools-linux-3859397.zip unzip sdk-tools-linux-3859397.zip -d android-sdk-linux 5、修改环境变量 export ANDROID_HOME=\"/opt/android-sdk-linux\" export PATH=\"$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools:$PATH\" source /etc/profile echo $ANDROID_HOME 6、更新build工具 cd /opt/android-sdk-linux tools/bin/sdkmanager --update tools/bin/sdkmanager \"platforms;android-26\" \"build-tools;26.0.2\" \"extras;google;m2repository\" \"extras;android;m2repository\" tools/bin/sdkmanager --licenses tools/bin/sdkmanager --list 7、更新缺少的依赖 tools/bin/sdkmanager \"extras;m2repository;com;android;support;constraint;constraint-layout;1.0.0-alpha9\" tools/bin/sdkmanager \"extras;m2repository;com;android;support;constraint;constraint-layout-solver;1.0.0-alpha9\" 8、编译 ionic cordova build android --prod --release "},"notes/other/cloudfoundry.html":{"url":"notes/other/cloudfoundry.html","title":"cloudfoundry 相关","keywords":"","body":"cloudfoundry 相关 "},"notes/other/svn.html":{"url":"notes/other/svn.html","title":"svn 相关","keywords":"","body":"svn 相关 一、ubuntu安装和配置svn 1. 安装svn apt-get install subversion 2. 建立svn仓库 1). 建立svn目录：mkdir /home/.svn(使用隐藏目录） 2). cd /home/.svn 3). mkdir astar 4). 创建仓库astar：svnadmin create /home/.svn/astar，执行完毕后astar目录有svnadmin创建的目录和文件 5). mkdir test 6). 创建仓库test：svnadmin create /home/.svn/test，执行完毕后test目录有svnadmin创建的目录和文件 3. 配置和管理svn 1). 每个仓库的配置文件在$repos/conf/下，vi svnserve.conf，配置项在[general]下： anon-access：匿名用户的权限，可以为read，write和none，默认值read。不允许匿名用户访问：anon-access = none auth-access：认证用户的权限，可以为read，write和none，默认值write。 password-db：密码数据库的路径，去掉前边的# authz-db：认证规则库的路径，去掉前边的#。 注意：这些配置项的行都要顶格，否则会报错。修改配置后需要重启svn才能生效。 2). 配置passwd文件 这是每个用户的密码文件，比较简单，就是“用户名=密码”，采用的是明码。如allen=111111 3). 配置authz文件 1. [groups] section：为了便于管理，可以将一些用户放到一个组里边，比如：owner=allen,ellen 2. groups下边的sections表示对一个目录的认证规则，比如对根目录的认证规则的section为[/]。设置单用户的认证规则时一个用户一行，如： [/] allen=rw　　#allen对根目录的权限为rw ellen=r　　 #ellen对根目录的权限为r 如果使用group，需要在group名字前加@,如 @owner=rw　　#group owner中的用户均为rw，等价于上边的两句话 启动时如果从/home/.svn/astar启动，/就是astar目录，用如上方式以astar目录为根设置权限。 如果从/home/.svn/启动，每个仓库根还是自己的起始目录。可以采用如上方式设置astar的权限，也可以采用如下方式： [astar:/] @owner=rw 设置test的权限如下： [test:/] @harry_and_sally = rw 简言之，每个仓库的根目录(/)就是自己的起始目录；[repos:/]这种方式只适用于多仓库的情况；[/]适合于单仓库和单仓库的方式。 3. 不能跨越仓库设置权限。 4. 启动和停止svn 1). 启动（如：svnserve --listen-port 9999 -d -r /home/.svn）： 1. 从astar目录启动，svnserve -d -r /home/.svn/astar，根目录(/)是astar，authz中规则的配置使用section[/]。访问方式为： svn://192.168.0.87/ 2. 从.svn目录启动，svnserve -d -r /home/.svn，根目录(/)是.svn，authz中对astar的配置使用section[astar:/] ,对test的配置使用section[test:/]。访问方式为： svn://192.18.0.87/astar svn://192.18.0.87/test 如果需要svn自启动，把命令加入/etc/rc.local中 2). 检查svn服务器是否已经启动（svn默认使用3690端口）：netstat -an | grep 3690 3). 停止：killall svnserve 5、创建项目 svnadmin create /var/opt/svn/TEST svn://192.168.0.127:3690/TEST vim conf/authz [groups] owner = hl [/] hl=rw @owner=rw [TEST:/] hl=rw @owner=rw vim conf/password [users] hl = hl vim conf/svnserve.conf anon-access = read:wq auth-access = write password-db = passwd authz-db = authz realm = /var/opt/svn/TEST 6. svn client 连接 "},"notes/other/anaconda.html":{"url":"notes/other/anaconda.html","title":"anaconda 相关","keywords":"","body":"anaconda 相关 一、介绍 conda是一种通用包管理系统，旨在构建和管理任何语言和任何类型的软件。举个例子：包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并可以快速切换。 Anaconda则是一个打包的集合，里面预装好了conda、某个版本的python、众多packages、科学计算工具等等，就是把很多常用的不常用的库都已经装好了。 Miniconda只包含最基本的内容——python与conda，以及相关的必须依赖项，对于空间要求严格的用户，Miniconda是一种选择。就只包含最基本的东西，其他的库得自己装。 1、conda使用 https://www.jianshu.com/p/edaa744ea47d 2、下载 anaconda https://www.anaconda.com/distribution/#download-section 3、miniconda安装地址 https://conda.io/en/latest/miniconda.html 二、 使用 1、配置清华源 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ conda config --set show_channel_urls yes conda upgrade --all 2、使用conda安装软件 conda install numpy 3、创建python环境 conda create -n my_env python=3 4、进入环境 在OSX/Linux上，使用 conda activate my_env 在Windows上，使用 activate my_env 5、离开环境 在OSX/Linux上，使用 conda deactive 在windows上，使用 deactivate 6、查看虚拟环境 conda env list 7、查看安装了哪些包 conda list 8、切换环境 activate my_env 9、移除虚拟环境 conda env remove -n my_env "},"notes/other/jupyter.html":{"url":"notes/other/jupyter.html","title":"jupyter 相关","keywords":"","body":"jupyter 相关 1、安装jupyter pip install jupyter 2、启动jupyter，路径、允许访问ip、允许root访问 jupyter notebook K:\\AI\\content\\test --ip=* --allow-root 3、启动时指定配置文件 jupyter notebook --config jupyter_notebook_config.py 4、生成配置文件 jupyter notebook --generate-config 5、生成密码,修改配置文件时使用 python from notebook.auth import passwd passwd() 6、常见配置文件修改 c.NotebookApp.ip = \"*\" c.NotebookAPp.open_browser = False c.NotebookApp.password = 'sha1:b39d2445079f:9b9ab99f65150e113265cb99a841a6403aa52647' c.NotebookApp.port= 8888 c.NotebookApp.notebook_dir = \"K:\\\\AI\\\\content\\\\test\" "},"notes/other/webdriver.html":{"url":"notes/other/webdriver.html","title":"webdriver 相关","keywords":"","body":"webdriver 相关 一、准备 1、环境要求 Window 操作系统 Python3.5.0-amd64(64位) Selenium 2.48.0 Pycharm(2017.1.1)2、Selenium with Python中文文档 https://selenium-python-zh.readthedocs.io/en/latest/index.html3、谷歌驱动 https://sites.google.com/a/chromium.org/chromedriver/ 4、将驱动下载，解压，配置解压后的驱动位置 driver = webdriver.Chrome(r\"D:\\chromedriver\\chromedriver.exe\") 5、安装selenium（又名webdriver） pip install selenium 二、快速入门 1、创建文件webdriver.py,输入以下内容 #导包 from selenium import webdriver from selenium.webdriver.common.keys import Keys #创建一个webdriver实例 driver = webdriver.Chrome(r\"D:\\chromedriver\\chromedriver.exe\") #打开填写的URL地址，直到页面加载完毕 driver.get(\"http://www.baidu.com\") #确认标题是否包含\"百度一下\" #print(driver.title) assert \"百度一下\" in driver.title #通过name获取前端对象 # elem = driver.find_element_by_name(\"wd\") #print(elem) #清除input输入框 elem.clear() #发送按键 \"hello\" elem.send_keys(\"hello\") #发送回车按键 elem.send_keys(Keys.RETURN) #判断返回内容中是否包含 \"百度百科\" assert \"百度百科\" not in driver.page_source #关闭webdriver实例 driver.close() 2、执行程序 python webdriver.py 三、其他相关 1、获取html dom对象的常用方法 element = driver.find_element_by_id(\"passwd-id\") element = driver.find_element_by_name(\"passwd\") element = driver.find_element_by_xpath(\"//input[@id='passwd-id']\") 2、填写表格 element = driver.find_element_by_xpath(\"//select[@name='name']\") all_options = element.find_elements_by_tag_name(\"option\") for option in all_options: print(\"Value is: %s\" % option.get_attribute(\"value\")) option.click() 3、选择 from selenium.webdriver.support.ui import Select select = Select(driver.find_element_by_name('name')) select.select_by_index(index) select.select_by_visible_text(\"text\") select.select_by_value(value) 4、取消OPTION选择 select = Select(driver.find_element_by_id('id')) select.deselect_all() 5、拖放 element = driver.find_element_by_name(\"source\") target = driver.find_element_by_name(\"target\") from selenium.webdriver import ActionChains action_chains = ActionChains(driver) action_chains.drag_and_drop(element, target).perform() 6、查找元素 find_element_by_id find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector 7、其余请参照官网文档 https://selenium-python-zh.readthedocs.io/en/latest/index.html 四、例子 #导包 from selenium import webdriver from selenium.webdriver.common.keys import Keys driver = webdriver.Chrome(r\"D:\\chromedriver\\chromedriver.exe\") driver.get(\"http://www.baidu.com\") assert \"百度一下\" in driver.title elem = driver.find_element_by_name(\"wd\") elem.clear() elem.send_keys(\"hello\") elem.send_keys(Keys.RETURN) assert \"百度百科\" not in driver.page_source driver.close() "},"notes/other/java.html":{"url":"notes/other/java.html","title":"Java 相关","keywords":"","body":"Java 相关 数据结构 算法题 Java基础 Java高级 三大框架 数据库 操作系统 计算机网络 分布式，集群等高级主题 技术开放题 题目参考 解答参考 数据结构 讲一下 HashMap 中 put 方法过程？ 对 Key 求 Hash 值，然后再计算 下标。 如果没有碰撞，直接放入桶中， 如果碰撞了，以链表的方式链接到后面， 如果链表长度超过阀值（TREEIFY_THRESHOLD == 8），就把链表转成红黑树。 如果节点已经存在就替换旧值 如果桶满了（容量 * 加载因子），就需要 resize。 HashMap 中 hash 函数怎么是是实现的？ 还有哪些 hash 的实现方式？ 高 16bit 不变，低 16bit 和高 16bit 做了一个异或 （n - 1） & hash --> 得到下标 还有哪些 Hash 实现方式：可以参考之前的博客 Effective Java 学习笔记 -- hashCode() HashMap 怎样解决冲突，讲一下扩容过程，假如一个值在原数组中，现在移动了新数组，位置肯定改变了，那是什么定位到在这个值新数组中的位置， 将新节点加到链表后， 容量扩充为原来的两倍，然后对每个节点重新计算哈希值。 这个值只可能在两个地方，一个是原下标的位置，另一种是在下标为 的位置。 抛开 HashMap，hash 冲突有那些解决办法？ 开放定址，链地址法 针对 HashMap 中某个 Entry 链太长，查找的时间复杂度可能达到 O(n)，怎么优化？ 将链表转为红黑树， JDK1.8 已经实现了。 数组和 ArrayList 的区别； 数组可以包含基本类型和对象类型，ArrayList 只能包含对象类型 数组大小固定，ArrayList 大小可以动态变化 ArrayList 提供了更多的特性（addAll、removeAll）。 Arraylist 如何实现排序 Collections.sort(List list); sort(List list, Comparator c); HashMap 数组 + 链表方式存储 默认容量： 16(2^n 为宜,若定义的初始容量不是 2^n，容量会定义为大于该初始容量的最小 2^n) 例如：初始容量为 13，则真正的容量是 16. put: 索引计算 : ((key.hashCode() ^ (key.hashCode() >>> 16)) & (table.length - 1)) 在链表中查找，并记录链表长度，若链表长度达到了 TREEIFY_THRESHOLD(8)，则将该链转成红黑树。 若在链表中找到了，则替换旧值，若未找到则继续 当总元素个数超过容量*加载因子时，扩容为原来 2 倍并重新散列 (元素的下标要么不变，要么变为【原下标+原容量】)。 将新元素加到链表尾部 线程不安全 HashTable 数组 + 链表方式存储 默认容量： 11(质数 为宜) put: 索引计算 : （key.hashCode() & 0x7FFFFFFF）% table.length 若在链表中找到了，则替换旧值，若未找到则继续 当总元素个数超过容量*加载因子时，扩容为原来 2 倍并重新散列。 将新元素加到链表头部 对修改 Hashtable 内部共享数据的方法添加了 synchronized，保证线程安全。 HashMap ，HashTable 区别 默认容量不同。 索引计算方式不同。 HashMap 特有的将过长链表转换为红黑树。 新元素的位置不同。 线程安全性 HashMap、ConcurrentHashMap 区别。 索引计算消除了最高位的影响 默认容量： 16(若定义了初始容量(c)，容量会定义为大于(c + (c >>> 1) +1) 的最小 2^n) 例如：初始容量为 13，则真正的容量是 32. 线程安全，并发性能较好 并发性能好的原因是 ConcurrentHashMap 并不是定义 synchronized 方法，而是在链表头上同步，不同的链表之间是互不影响的。 ConcurrentHashMap 原理 最大特点是引入了 CAS（借助 Unsafe 来实现【native code】） CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。 Unsafe 借助 CPU 指令 cmpxchg 来实现 使用实例： 对 sizeCtl 的控制都是用 CAS 来实现的 sizeCtl ：默认为0，用来控制 table 的初始化和扩容操作。 -1 代表table正在初始化 N 表示有 -N-1 个线程正在进行扩容操作 如果table未初始化，表示table需要初始化的大小。 如果table初始化完成，表示table的容量，默认是table大小的0.75倍，居然用这个公式算0.75（n - (n >>> 2)）。 CAS 会出现的问题：ABA 对变量增加一个版本号，每次修改，版本号加 1，比较的时候比较版本号。 TreeMap 和 TreeSet 区别和实现原理 TreeSet 底层是 TreeMap，TreeMap 是基于红黑树来实现的。 如果想实现一个线程安全的队列，可以怎么实现？ 知道 LRU 吗，20分钟基于 HashMap 实现一个 LRU 算法，面试官给个地址，进去写代码，面试官远程看 如何设计实现一个LRU Cache？ 二叉树的遍历方式，前序、中序、后序和层序 可以再写一篇了。。 常见的排序算法时间复杂度（排序算法实现也要重点掌握） 常见排序算法实现(Java) B+树的了解 多分支结构有效降低了树的高度 B 树的各种操作能使 B 树保持较低的高度，从而达到有效避免磁盘过于频繁的查找存取操作，从而有效提高查找效率 算法题 怎么查询一个单向链表的倒数第五个节点 判断链表是否成环 两条相交的单向链表，如何求他们的第一个公共节点 在无序数组中找最大的K个数? 给定n个数，寻找第k小的数，同时给出时间复杂度 找一个数组中的第三大数 找出数组中第一个出现2次的数， 求 1-N 中数字 1 的个数。 判断一个数是不是丑数； 求第 K 个丑数； 10w行数据，每行一个单词，统计出现次数出现最多的前100个。 一个文本文件，给你一个单词，判断单词是否出现。 一进去要求敲代码二叉排序树的插入、删除及查找 某海量用户网站，用户拥有积分，积分可能会在使用过程中随时更新。现在要为该网站设计一种算法，在每次用户登录时显示其当前积分排名。用户最大规模为2 亿；积分为非负整数，且小于 100 万； 判断一棵二叉树是否是 BST。 一副扑克 54 张牌，现在分成 3 份，每份 18 张，问大小王出现在同一份中的概率是多少； 50个白球50个红球，两个盒子，怎么放让人随机在一个盒子里抽到红球概率最高。。。这个就是一个盒子放一个红球，另一个盒子放99个球。 logN 查找一个有序数组移动后类似 4 5 6 7 1 2 3里面的一个数 0 ~ n 连续 n + 1 数，现在有一个长度为 n 的数组存放了上面 n + 1 个数的其中 n 个，找出哪一个数没有被放进数组 将M个平均长度为N的有序队列组合成一个有序队列 10亿条短信，找出前一万条重复率高的 对一万条数据排序，你认为最好的方式是什么 假如有100万个玩家，需要对这100W个玩家的积分中前100名的积分，按照顺序显示在网站中，要求是实时更新的。积分可能由做的任务和获得的金钱决定。问如何对着100万个玩家前100名的积分进行实时更新？ 除了前100名，后100W-100名玩家的积分，让变化的积分跟第100名比较，如果比第100名高，那就替换的原则。 Java 基础 Java 的优势 语法简单 跨平台 当开发规模膨胀到一定程度，Java在规范、协作和性能调优上还是占有很大优势,在大型应用，尤其是企业应用上，Java的地位仍然难以撼动 boolean 占几个字节 如果 boolean 变量在栈上，那么它占用一个栈单元（32-bits） 如果在堆上，那么就跟 JVM 的实现有关了 在 Oracle 的 JVM 实现，boolean[] 中每个元素占用一个字节（8-bits） Java 访问修饰符权限的区别； public 所有类都可访问 protected 只允许包内、子类访问。 默认 只允许包内访问 private 只允许类内访问 String 是否可以继承， “+” 怎样实现? String 是 final 类，不可继承。 + 是通过 StringBuilder（或 StringBuffer） 类，和 append 方法实现 String，StringBuffer，StringBuilder，区别，项目中那里用到了 StringBuffer 或者 StringBuilder String 不可变 StringBuffer，可变，线程安全 Stringbuilder，可变，线程不安全 String为啥不可变，在内存中的具体形态？ String 使用 final char value[] 来存放字符序列。 Comparable 接口和 Comparator 接口实现比较 Comparable 是直接在\"被比较\"的类内部来实现的 Comparator则是在被比较的类外部实现的 Arrays 静态类如何实现排序的？ 双轴快排 首先检查数组长度，如果比阀值（286）小，直接使用双轴快排 否则先检查数组中数据的连续性，标记连续升序，反转连续降序，如果连续性好，使用 TimSort 算法（可以很好的利用数列中的原始顺序） 否则使用双轴快排 + 成对插入排序 Java 中异常怎么处理，什么时候抛出，什么时候捕获； 一般原则是提早抛出，延迟捕获 出现异常时，若当前无法处理则抛，否则捕获异常，尝试恢复。 Java 锁机制 重入锁、对象锁、类锁的关系 哪些方法实现线程安全？ Java 中的同步机制，synchronized 关键字，锁（重入锁）机制，其他解决同步的方 volatile 关键字 ThreadLocal 类的实现原理要懂。 Synchronized 和 lock 区别 锁的优化策略 读写分离 分段加锁 减少锁持有的时间 多个线程尽量以相同的顺序去获取资源 Java线程阻塞调用 wait 函数和 sleep 区别和联系，还有函数 yield，notify 等的作用。 sleep 时线程的方法（让出 CPU），wait 是对象的方法。 谈谈的 Java 反射的理解，怎么通过反射访问某各类的私有属性 通过反射，我们可以获取类的运行时内部结构。 反射 API 中有个方法 getDeclaredFields() 动态代理的原理 动态代理基于反射实现，调用者通过代理对象来访问方法的时候，代理对象可以做相应的处理，然后通过反射调用被代理对象的方法。 项目中都是用的框架，用过 Servlet 吗？ Servlet 是单例吗？多线程下 Servlet 怎么保证数据安全的？Servlet 的生命周期？ Thread 状态有哪些 Java 高级 GC 算法，除了常见的复制算法，标记整理，标记清除算法，还有哪些？ 增量算法。主要思想是垃圾收集线程与用户线程交替执行。也可以说一边执行垃圾回收一遍执行用户代码。但是这种方法会造成系统吞吐量下降。 分代收集。这种方法没有使用新算法，只是根据对象的特点将堆分为年轻代和老年代，年轻代使用复制算法，老年代使用标记整理算法。 垃圾收集器 收集器收集算法收集区域线程停顿特点serial复制算法新生代单线程收集时必须停顿其他所有工作线程简单高效serial old标记整理老年代单线程收集时必须停顿其他所有工作线程PerNew复制算法新生代多线程serial 的多线程版本Server 模式下首选parallel Scavenge复制算法新生代多线程收集时必须停顿其他所有工作线程注重吞吐量，适合后台计算多parallel old标记整理老年代多线程收集时必须停顿其他所有工作线程同 parallel Scavenge CMS（concurrent mark sweep）并发收集、低停顿 初始标记（CMS initial mark）：仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，需要“Stop The World”。 并发标记（CMS concurrent mark）：进行GC Roots Tracing的过程，在整个过程中耗时最长。 重新标记（CMS remark）：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。此阶段也需要“Stop The World”。 并发清除（CMS concurrent sweep） G1 将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，而都是一部分Region（不需要连续）的集合。 整体使用标记整理，局部使用复制算法。 初始标记（Initial Marking） 仅仅只是标记一下GC Roots 能直接关联到的对象，并且修改TAMS（Nest Top Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可以的Region中创建对象，此阶段需要停顿线程，但耗时很短。 并发标记（Concurrent Marking） 从GC Root 开始对堆中对象进行可达性分析，找到存活对象，此阶段耗时较长，但可与用户程序并发执行。 最终标记（Final Marking） 为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。 筛选回收（Live Data Counting and Evacuation） 首先对各个Region中的回收价值和成本进行排序，根据用户所期望的GC 停顿是时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 G1 vs CMS 我们选择哪个收集器是由我们垃圾回收的目标来决定的，主要考虑以下几点： 吞吐量 停顿时间 堆容量 G1 vs CMS G1 基本不用配置，低停顿，用于大容量的堆。但是他牺牲了应用程序的吞吐量和部分堆空间。 CMS 配置比较复杂，合理的低停顿，用于中等或更小的堆。 所以当你觉得配置 CMS 太难了，或你的堆在 2 G 以上，或你想要显式的指定停顿时间那么你可以使用 G1。否则使用 CMS Java 内存模型 深入理解 Java 虚拟机 问 JVM 内存分代机制（会问分为那几个代，各个代特点），分代回收的优点（这个问了很多次）。 分为年轻代和老年代，年轻代中的对象生命周期短，基本是朝生夕死，所以需要频繁的回收；老年代中的对象一般都能熬过多次 GC 所以他们不需要频繁回收。分代收集利用了这种特点，年轻代使用复制算法，老年代使用标记整理算法，所以总的来说分代收集的效率相对还是不错的。 Java 虚拟机类加载机制，双亲委派模型 深入理解 Java 虚拟机 GC roots 不可达的对象是可以回收的。 栈中的引用的对象 方法区常量引用的对象 方法区静态域引用的对象 介绍一下 Java 的强软弱虚四种引用，问什么时候使用软引用 一般 new 出来的对象都是强引用，GC 不会回收强引用的对象 软引用：软引用的对象不那么重要，当内存不足时可以被回收。非常适合于创建缓存。 弱引用：只是引用一个对象，若一个对象的所有引用都是弱引用的话，下次 GC 会回收该对象。一般用在集合类中，特别是哈希表。 虚引用：一般用于对实现比较精细的内存使用控制。对于移动设备来说比较有意义 RPC 原理 你应该知道的 RPC 原理 三大框架 Spring 主要思想是什么，回答 IOC 和AOP，怎么自己实现 AOP ？ IOC 的好处：阿里一面总结 12 题 使用基于反射的动态代理 SpringAOP 用的哪一种代理 JDK 动态代理，这种是一般意义上的动态代理；用一个代理类来间接调用目标类的方法。目标类如果实现了接口那就用这种方式代理。 cglib 动态代理。通过框架转换字节码生成目标类的子类，并覆盖其中的方法实现增强，因为采用的是继承，所以不能对 final 类进行代理。目标类没有实现任何接口，就使用这种方法 spring bean 初始化过程 读取 XML 资源，并解析，最终注册到 Bean Factory 中 spring bean 对象的生命周期 当一个 bean 被实例化时，它需要执行一些初始化(init-method)使它转换成可用状态。同样，当 bean 不再需要，并且从容器中移除时，需要做一些清除工作(destroy-method) 讲讲 Spring 中 ApplicationContext 初始化过程。 SpringMVC 处理请求的流程 收到用户请求 dispatcher Servlet 将请求转发到相应的 Controller 通过 View Resolver 进行视图解析 返回给用户 7.SpringMVC 的设计模式 8.Spring 的 annotation 如何实现 9.Spring拦截器怎么使用，Controller是单例吗 基于 XML 配置文件 基于注解 基于 Spring 定义的 MethodInterceptor 接口 数据库 1.SQL 优化方案 根据我目前的知识水平，大概分为两类： 多表连接时不直接连接表，而是先用 where 筛选出符合条件的记录然后进行连接。一般情况下，筛选一次会除去相当多的无效记录，这会极大的提高效率。 当前的 SQL 是否合理的使用了索引。如果设置的索引没有使用的话，会导致全表扫描。效率上会差很多。没有利用索引的情况一般有以下几种： 5以“%”开头的LIKE语句，模糊匹配 OR 语句前后没有同时使用索引 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型） where 子句中对字段进行表达式操作 在where子句中对字段进行函数操作 索引有哪分别有什么特点？ 从数据结构来划分的话，主要有两种：一种是基于 B+ 树的索引，一种是基于哈希表的索引。基于哈希表的索引在等值查询上有绝对的优势，但其他方面就不是很好了。B+ 树是一种多分支的树结构，相比二叉树来说高度降低了很多，能够有效的减少磁盘 IO，所以我们平时使用的都是基于 B+ 树的索引 索引为什么用 B不用二叉树，有什么好处？ 基于 B 索引实现，降低了树的高度，减少了磁盘 IO 的次数。 数据库索引优点和缺点 优点：有效查询； 缺点：操作数据时需要对索引进行更新，效率上稍微差一点；索引需要占用一定的空间。 数据库事务的四个隔离，MySql 在哪一个级别 MyL 默认隔离级别为 Repeatable read 操作系统 进程和线程的区别. 进程是拥有资基本单位，线程是 CPU 调度的基本单位 进程拥有独立的地址空间，同一个进程的线程共享该进程的地址空间 进程上下文切换相对线程上下文切换会消耗更多的资源 一个进程必须至少拥有一个线程 一个线程死掉就等于整个进程死掉，所以多进程的程序相对于多线程的程序来说会更健壮 通信方式不同，线程通过进程内的资源进行通信，进程的通信有多种方式，包括管道、共享内存、消息等等。 进程间通信 9.道（Pip及有名管道（named pipe）：管道可用于具有亲缘关系进程间的通信，有名管道克服了管道没有名字的限制，因此，除具有管道所具有的功能外，它还允许无亲缘关系进程间的通信； 信号（Signal）：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身； 报文（Message）队列（消息队列）：消息队列是消息的链接表。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。 共享内存：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥。 信号量（semaphore）：主要作为进程间以及同一进程不同线程之间的同步手段。 套接口（Socket）：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。 在共享内存中如何使用utex selt 和 ell 操作系哪几部分组进程结构 多进程线程的区别 什么时用多线程，时候使用多进程 Jav多线程与操统线程的关系 一般线守护线程的 多线程线程的实现 死锁的，死锁避免* 互斥条件 . 占有和等待条件 不剥夺条件 循环等待 linux中如何查看等命令 不同进开了同一个，那么这两个进程得到的文件描述符（fd）相同吗 不一定，因开文件有三个表，inode 表，系统文件描述符表，进程文件描述符表。不同进程的文件描述符的范围是一样的，有可能刚好相同，也有可能不相同 两个线程如何同时监听端口。 SO_REEPORT 参数。 计算机网络 HTTP 状态码有哪一一解释含义 1xx 3100 服务接收到部分请求，但是一旦服务器并没有拒绝该请求，客户端应该继续发送其余的请求。 2xx 成功 5200 OK求成功（其后是对GET和POST请求的应答文档。） 3xx 重定向 304 NoModified 未修改的文档。客户端有缓冲的文档并发出了一个条件性的请求（一般是提供If-Modified-Since头表示客户只想比指定日期更新的文档）。服务器告诉客户，原来缓冲的文档还可以继续使用。 4xx: 客户端错误 400 BaRequest 服务器未能理解请求。 404 Not Found 服务器无法找到被请求的页面。 5xx: 服务器错误 500 Iernal Server Error 请求未完成。服务器遇到不可预知的情况。 HTTP 请求头有哪介绍平时见过的，怎么利用这些信息来进行前后端调试 Host,求的域名 User-Agent，用户的浏览器版本信息 Accept，响应的内容类型 Accept-Language, 接受的语言 Accept-Encoding, 可接受的编码方式 Cookie，本地的 Cookie 信息 if-Modified-Since, 本地有缓存，如果在那之后没有做修改，则可以直接使用本地缓存。 TCP 和 UDP 别 TCP UDP 是否连接 面向连接 面向非连接 传输可靠性 可靠性 不可靠 应用场合 传输大量数据 少量数据 速度 慢 快 HTTP1.0 和 1.1 的区别 最主要的区别1.1 支持持久连接。Connection 请求头的值为 Keep-Alive 时，客户端通知服务器返回本次请求结果后保持连接；Connection 请求头的值为 close 时，客户端通知服务器返回本次请求结果后关闭连接。 1.1 支持断点续传。RANGE:bytes=XXX 表示要求服务器从文件 XXX 字节处开始传送 还有一些其他的改进，有兴趣可以自行查阅相关资料 HTTP 和 HTT 的主要区别 安全。HTT直接与 TCP 通信，而 HTTPS 是先与 SSL（加密） 通信，然后再由 SSL 和 TCP 通信 滑动窗口算法 8又称回退 N（go-back-N）,发送方的窗口滑动是由接收方是否已成功收到数据包来决定的。即接收方的窗口向前滑动后发送方的窗口才会向前滑动。//TODO 域名解析详细过程0. IP 分为几类，都代表什么，私网是哪些 A：前 1yte 为网络标识，剩下的是主机标识 B：前 2 bytes 为网络标识 C：前 3 bytes 为网络标识 D：为多播地址，最高位为 1110 E：特殊 IP。例如 0.0.0.0，127.0.0.1,255.255.255.255 等等 IP 头组成； . 计算机中的同步和 发现百不去，怎么* 查看 DN解析是否正确。若有错误，删除本地 DNS 缓存 若 DNS 没有问题，使用 traceroute 检测路径，若路径不通则说明网路阻塞，暂时就别上网了 traceroute 没有问题，ping 也能通一般就是服务器端出问题了。 分布式/集群等高级主题 负载均衡算法 2随机：负载均法随机的把负载分配到各个可用的服务器上。 轮询：按顺序将新的连接请求分配给下一个服务器 加权轮询：每台服务器接受到的连接数按权重分配，一般是用在应用服务器的处理能力大小不同的情况下。 最少连接：把新连接分配给当前连接最少的服务器 BLABAL... 分布式数据库 8分布式数据库了原来集中式数据库不具备的高可用性和拓展能力 技术开放题 如何设计一个高并发的 数据库的优化括合理的事务隔离级别、SQL语句优化、索引的优化 使用缓存，尽量减少数据库 IO 分布式数据库、分布式缓存 服务器的负载均衡 现在一个网页响应速度变慢了，假如我把这个任务交给你，你怎么处理这个问题 负载均衡题，数千负载部署到机器上，要求对问题进行抽象，建模，提出解决方案。 美团面试到一个城试应聘者，面试有三天，每天面试官上午可以面试三场，下午可以面试四场，怎么设计面试系统，面试者可以选择面试日期，面试时间和面试官。 有一些爬P不断的美团网站，现在美团设定一个IP5分钟之内访问美团网站超过100次，就判定为爬虫IP，怎么设计这个程序？如果100改成10000，怎么设计？ 假设在时刻由几万发请求同时产生，请设计一个方案来处理这种情况。 问我简学校 oj台这个项目怎么实现1000人并发?并发的性能瓶颈在哪? 因为还没完现处于开发阶段，只跟面试官说了下自己的构想，nginx+tomcat集群，性能瓶颈可能出现在网络io和java gc上，然后说了下jvm gc的优化，如何实现session共享。最后我问了下面试官这样设计可以吗，他说这样设计不行可能有问题，没有告诉我问题出现在哪里。 "},"notes/other/websocket.html":{"url":"notes/other/websocket.html","title":"websocket 相关","keywords":"","body":"websocket 相关 1、后端安装 pip install websocket-server 2、后端代码 # coding: utf-8 from websocket_server import WebsocketServer # Called for every client connecting (after handshake) def new_client(client, server): print(\"New client connected and was given id %d\" % client['id']) server.send_message_to_all(\"Hey all, a new client has joined us\") # Called for every client disconnecting def client_left(client, server): print(\"Client(%d) disconnected\" % client['id']) # Called when a client sends a message def message_received(client, server, message): if len(message) > 200: message = message[:200]+'..' print(\"Client(%d) said: %s\" % (client['id'], message)) PORT=9001 server = WebsocketServer(PORT, \"0.0.0.0\") server.set_fn_new_client(new_client) server.set_fn_client_left(client_left) server.set_fn_message_received(message_received) server.run_forever() 3、前端使用 websocket function WebSocketTest() { if (\"WebSocket\" in window) { console.log(\"您的浏览器支持 WebSocket!\"); // 打开一个 web socket var ws = new WebSocket(\"ws://192.168.110.82:9001/echo\"); // console.log(ws); ws.onopen = function() { // Web Socket 已连接上，使用 send() 方法发送数据 ws.send(\"hello world\"); console.log(\"数据发送中...\"); }; ws.onmessage = function (evt) { var received_msg = evt.data; console.log(\"数据已接收...\"); // console.log(evt); console.log(received_msg); }; ws.onclose = function() { // 关闭 websocket console.log(\"连接已关闭...\"); }; ws.onerror = function(e) { output(\"onerror\"); console.log(e) }; } else { // 浏览器不支持 WebSocket console.log(\"您的浏览器不支持 WebSocket!\"); } } 运行 WebSocket "},"notes/other/stratos_ui.html":{"url":"notes/other/stratos_ui.html","title":"stratos-ui 相关","keywords":"","body":"stratos-ui 相关 1、构建环境 cd / git clone https://github.com/SUSE/stratos-ui.git 2、安装nodejs yum -y install gcc gcc-c++ # nodejs v6.11.4 cd /usr/local/src/ wget http://nodejs.org/dist/v6.11.4/node-v6.11.4.tar.gz tar -zxvf node-v6.11.4.tar.gz ./configure –prefix=/usr/local/src/node-v6.11.4 make make install # yum install epel-release #yum install nodejs #yum install npm vim /etc/profile export NODE_HOME=/usr/local/src/node-v6.11.4/ export PATH=$NODE_HOME/bin:$PATH source /etc/profile node –version npm –version 3、编译安装go # glide v0.13.0 #go 1.8.4 wget https://storage.googleapis.com/golang/go1.8.4.linux-amd64.tar.gz tar -C /usr/local -zxvf go1.8.4.linux-amd64.tar.gz go get github.com/Masterminds/glide #go install github.com/Masterminds/glide cp ./go/bin/glide /usr/local/go/bin/ vim /etc/profile export NODE_HOME=/usr/local/src/node-v6.11.4/ export GOPATH=/usr/local/go export PATH=$NODE_HOME/bin:$GOPATH/bin:$PATH source /etc/profile 4、编译stratos-ui(npm run build-backend需要翻墙) cd /stratos-ui npm install -g gulp bower bower install –allow-root npm install –only=prod npm run build npm run build-backend npm run build-cf npm run build-backend && npm run build-cf CERTS_PATH=/stratos-ui/dev-certs ./generate_cert.sh echo 'DATABASE_PROVIDER=sqlite HTTP_CONNECTION_TIMEOUT_IN_SECS=10 HTTP_CLIENT_TIMEOUT_IN_SECS=20 CONSOLE_PROXY_TLS_ADDRESS=:443 CF_ADMIN_ROLE=cloud_controller.admin CF_CLIENT=cf ALLOWED_ORIGINS=http://nginx SESSION_STORE_SECRET=wheeee! ENCRYPTION_KEY=B374A26A71490437AA024E4FADD5B497FDFF1A8EA6FF12F6FB65AF2720B59CCF CONSOLE_ADMIN_SCOPE=openid UAA_ENDPOINT=https://uaa.cf.tmp.com CONSOLE_CLIENT=cf SKIP_SSL_VALIDATION=true SQLITE_KEEP_DB=true AUTO_REG_CF_URL=https://api.cf.tmp.com'| tee > /stratos-ui/config.properties chmod +x portal-proxy ./portal-proxy "},"notes/other/apidoc.html":{"url":"notes/other/apidoc.html","title":"apidoc 相关","keywords":"","body":"apidoc 相关 https://apidocjs.com/ 1、安装 nodejs 略 2、安装 apidoc npm install apidoc -g 3、将 npm bin 目录，写入环境变量 echo -e \"export PATH=$(npm prefix -g)/bin:$PATH\" >> ~/.bashrc && source ~/.bashrc 4、在项目目录创建 apidoc.api cat apidoc.json { \"name\": \"libvirtapi\", \"version\": \"1.0.0\", \"description\": \"libvirtapi 接口文档\", \"title\": \"libvirtapi\", \"url\" : \"http://YOUR_IP:8778\" } 5、请求方法后面写上注释 def get(self, scene=None, cluster=None, path=None): \"\"\" @api {get} /daovoid_api/{scene}/{cluster}/{resource} 请求资源列表 @apiName getResource @apiGroup get @apiSuccess {object} resource @apiExample 请求虚拟机详细信息 GET /daovoid_api/sjyg/cluster1/instance/1a72c328acfe4d5f86d630c1832b085c Content-Type: application/json @apiSuccessExample 成功响应：虚拟机详细信息 HTTP/1.1 200 OK { \"inventories\": [ { \"cluster\": \"http://192.168.0.244:8080\", \"createDate\": \"Jan 9, 2020 3:21:11 PM\", \"description\": \"this is a vm\", \"id\": \"1a72c328acfe4d5f86d630c1832b085c\", \"name\": \"aaaa\", \"networks\": [ .... ], } ], \"scene\": \"sjyg\", \"snapshots\": [], \"status\": \"Running\", \"templates\": [], \"type\": \"usual\", \"volumes\": [ ... ], ... } ] } \"\"\" 6、在项目目录执行以下命令，生成文档 apidoc -i ./ -o apidoc "},"notes/other/keepalived.html":{"url":"notes/other/keepalived.html","title":"keepalived 相关","keywords":"","body":"keepalived 相关 一、lvs+keepalived yum install -y ipvsadm keepalived ! Configuration File for keepalived global_defs { router_id lvs-keepalived01 #router_id 机器标识，通常为hostname，但不一定非得是hostname。故障发生时，邮件通知会用到。 } vrrp_instance VI_1 { #vrrp实例定义部分 state MASTER #设置lvs的状态，MASTER和BACKUP两种，必须大写 interface ens192 #设置对外服务的接口 virtual_router_id 100 #设置虚拟路由标示，这个标示是一个数字，同一个vrrp实例使用唯一标示 priority 100 #定义优先级，数字越大优先级越高，在一个vrrp——instance下，master的优先级必须大于backup advert_int 1 #设定master与backup负载均衡器之间同步检查的时间间隔，单位是秒 authentication { #设置验证类型和密码 auth_type PASS #主要有PASS和AH两种 auth_pass 1111 #验证密码，同一个vrrp_instance下MASTER和BACKUP密码必须相同 } virtual_ipaddress { #设置虚拟ip地址，可以设置多个，每行一个 10.11.1.10 } } virtual_server 10.11.1.10 32600 { #设置虚拟服务器，需要指定虚拟ip和服务端口 delay_loop 6 #健康检查时间间隔 lb_algo wrr #负载均衡调度算法 lb_kind DR #负载均衡转发规则 #persistence_timeout 50 #设置会话保持时间，对动态网页非常有用 protocol TCP #指定转发协议类型，有TCP和UDP两种 real_server 10.11.1.11 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #连接端口为32600，要和上面的保持一致 } } real_server 10.11.1.12 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #连接端口为32600，要和上面的保持一致 } } real_server 10.11.1.13 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #连接端口为32600，要和上面的保持一致 } } } ! Configuration File for keepalived global_defs { router_id lvs-keepalived02 #router_id 机器标识，通常为hostname，但不一定非得是hostname。故障发生时，邮件通知会用到。 } vrrp_instance VI_1 { #vrrp实例定义部分 state BACKUP #设置lvs的状态，MASTER和BACKUP两种，必须大写 interface ens192 #设置对外服务的接口 virtual_router_id 100 #设置虚拟路由标示，这个标示是一个数字，同一个vrrp实例使用唯一标示 priority 90 #定义优先级，数字越大优先级越高，在一个vrrp——instance下，master的优先级必须大于backup advert_int 1 #设定master与backup负载均衡器之间同步检查的时间间隔，单位是秒 authentication { #设置验证类型和密码 auth_type PASS #主要有PASS和AH两种 auth_pass 1111 #验证密码，同一个vrrp_instance下MASTER和BACKUP密码必须相同 } virtual_ipaddress { #设置虚拟ip地址，可以设置多个，每行一个 10.11.1.10 } } virtual_server 10.11.1.10 32600 { #设置虚拟服务器，需要指定虚拟ip和服务端口 delay_loop 6 #健康检查时间间隔 lb_algo wrr #负载均衡调度算法 lb_kind DR #负载均衡转发规则 #persistence_timeout 50 #设置会话保持时间，对动态网页非常有用 protocol TCP #指定转发协议类型，有TCP和UDP两种 real_server 10.11.1.11 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #! Configuration File for keepalived global_defs { router_id lvs-keepalived02 #router_id 机器标识，通常为hostname，但不一定非得是hostname。故障发生时，邮件通知会用到。 } vrrp_instance VI_1 { #vrrp实例定义部分 state BACKUP #设置lvs的状态，MASTER和BACKUP两种，必须大写 interface ens192 #设置对外服务的接口 virtual_router_id 100 #设置虚拟路由标示，这个标示是一个数字，同一个vrrp实例使用唯一标示 priority 90 #定义优先级，数字越大优先级越高，在一个vrrp——instance下，master的优先级必须大于backup advert_int 1 #设定master与backup负载均衡器之间同步检查的时间间隔，单位是秒 authentication { #设置验证类型和密码 auth_type PASS #主要有PASS和AH两种 auth_pass 1111 #验证密码，同一个vrrp_instance下MASTER和BACKUP密码必须相同 } virtual_ipaddress { #设置虚拟ip地址，可以设置多个，每行一个 10.11.1.10 } } virtual_server 10.11.1.10 32600 { #设置虚拟服务器，需要指定虚拟ip和服务端口 delay_loop 6 #健康检查时间间隔 lb_algo wrr #负载均衡调度算法 lb_kind DR #负载均衡转发规则 #persistence_timeout 50 #设置会话保持时间，对动态网页非常有用 protocol TCP #指定转发协议类型，有TCP和UDP两种 real_server 10.11.1.11 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #连接端口为32600，要和上面的保持一致 } } real_server 10.11.1.12 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #连接端口为32600，要和上面的保持一致 } } real_server 10.11.1.13 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #连接端口为32600，要和上面的保持一致 } } }，要和上面的保持一致 } } real_server 10.11.1.12 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #连接端口为32600，要和上面的保持一致 } } real_server 10.11.1.13 32600 { #配置服务器节点1，需要指定real server的真实IP地址和端口 weight 10 #设置权重，数字越大权重越高 TCP_CHECK { #realserver的状态监测设置部分单位秒 connect_timeout 10 #连接超时为10秒 retry 3 #重连次数 delay_before_retry 3 #重试间隔 connect_port 32600 #连接端口为32600，要和上面的保持一致 } } } 二、keepalived 1、安装 yum install -y keepalived 2、主节点配置 vim /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id node1 } vrrp_instance VI_1 { state MASTER interface ens192 virtual_router_id 127 priority 250 advert_int 1 authentication { auth_type PASS auth_pass 35f18af7190d51c9f7f78f37300a0cbd } unicast_src_ip 10.11.1.11 unicast_peer { 10.11.1.12 10.11.1.13 } virtual_ipaddress { 10.11.1.10/22 dev ens192 } } 3、从节点配置 vim /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id node2 } vrrp_instance VI_1 { state SLAVE interface ens192 virtual_router_id 127 nopreempt priority 240 advert_int 1 authentication { auth_type PASS auth_pass 35f18af7190d51c9f7f78f37300a0cbd } unicast_src_ip 10.11.1.12 unicast_peer { 10.11.1.11 10.11.1.13 } virtual_ipaddress { 10.11.1.10/22 dev ens192 } } 4、重启服务 systemctl start keepalived systemctl enable keepalived "},"notes/other/linux_desktop_rdp.html":{"url":"notes/other/linux_desktop_rdp.html","title":"Linux Desktop rdp 相关","keywords":"","body":"Linux Desktop rdp 相关 一、SPICE、VNC、RDP三种协议对比 SPICE VNC RDP BIOS屏幕显示 能 能 不能 全彩支持 能 能 能 更改分辨率 能 能 能 多显示器 多显示器支持（高达4画面） 只能一个屏幕 多显示器支持 图像传输 图像和图形传输 图像传输 图像和图形传输 视频播放支持 GPU加速支持 不能 GPU加速支持 音频传输 双向语音可以控制 不能 双向语音可以控制 鼠标控制 客户端服务器都可以控制 服务器端控制 服务器端控制 USB传输 USB可以通过网络传输 不能 USB可以通过网络传输 适用系统 Linux Windows、Linux Windows、Linux 网络流量 较大、正常使用10-20M 较小、常用100K左右 较小、正常使用100-200K左右 适用场景 由于在色彩、音频和USB方面，适用于虚拟桌面，主要用于虚拟机的虚拟桌面应用。 主要用于Linux的服务器管理，由于无声音和USB传输，不满足于虚拟桌面的使用。 由于在色彩、音频、USB及本地磁盘映射方面较好，非常适用于虚拟桌面 二、安装 xrdp 1、安装 xrdp CentOS yum install -y epel-release yum install -y xrdp Ubuntu apt install -y xrdp 2、启动 xrdp systemctl start xrdp systemctl enable xrdp 3、查看 rdp 协议端口 3389 netstat -antup | grep xrdp 4、其他，关闭防火墙或允许 3389 firewall-cmd --permanent --zone=public --add-port=3389/tcp firewall-cmd –reload # seliunux 设置 chcon --type=bin_t /usr/sbin/xrdp chcon --type=bin_t /usr/sbin/xrdp-sesman 三、安装桌面 1、安装 xfce Desktop XFCE是最轻量级的桌面环境之一。它速度快，系统资源少，但在视觉上仍然很吸引人。此外，它有一个非常活跃的社区，因此有许多定制选项可用。 1) 安装 yum install -y epel-release yum groupinstall -y \"Xfce\" reboot echo \"xfce4-session\" > ~/.Xclients chmod a+x ~/.Xclients 2）卸载 yum groupremove -y \"Xfce\" yum remove -y libxfce4* 2、安装MATE Desktop 1) 安装 yum install -y epel-release yum groupinstall -y \"MATE Desktop\" reboot echo \"mate-session\" > ~/.Xclients chmod a+x ~/.Xclients 2）卸载 yum groupremove -y \"MATE Desktop\" yum autoremove -y 3、安装 GNOME Desktop 1) 安装 yum groupinstall \"GNOME DESKTOP\" -y 2）启动 systemctl get-default 如果输出：multi-user.target，说明GUI没有被加载，需要将graphical.target设置为默认的target systemctl set-default graphical.target 输出： Removed symlink /etc/systemd/system/default.target. Created symlink from /etc/systemd/system/default.target to /usr/lib/systemd/system/graphical.target. systemctl isolate graphical.target 4、卸载 yum groupremove -y \"GNOME Desktop\" yum autoremove -y 四、RDP 连接工具 1、mRemoteNG mRemoteNG 2、multidesk multidesk 3、remmia Debian/Ubuntu apt-get install remmina remmina-plugin-* CentOS/RHEL yum install remmina remmina-plugins-* Fedora 22 dnf copr enable hubbitus/remmina-next dnf upgrade --refresh 'remmina*' 'freerdp*' 五、远程连接工具合集 工具名称 支持平台 官网 特点 teamviewer windows https://www.teamviewer.com/ 远程桌面工具，私有远程tv协议 anydesk windows https://anydesk.com/ 类似teamviewer Radmin windows http://www.radmin.cn/ 远程桌面工具 xt800 windows http://www.xt800.cn/ 国内首家支持多平台、多终端的远程运维和支持平台 GoToMyPC windows https://gotomypc.en.softonic.com GoToMyPC是美国Citrix公司推出的远程控制软件，类似于国内网络人远程控制软件。 网络人netman123 windows http://netman123.cn Ammyy Admin windows http://www.ammyy.com/cn/index.html FastX windows https://www.starnet.com/fastx 3389远程服务器批量管理器 windows http://www.46603.cn/ https://www.52pojie.cn/thread-480961-1-1.html 批量连接windows桌面 multidesk windows http://www.syvik.com/multidesk/index_chs.htm 批量连接windows桌面 Remmina linux http://www.remmina.org/wp/ Linux平台全协议RDP/VNC/SSH等 PAC Manager linux https://sourceforge.net/projects/pacmanager/ Linux平台全协议RDP/VNC/SSH等 mRemoteNG windows https://github.com/mRemoteNG/mRemoteNG win平台全协议RDP/VNC/SSH等 MobaXterm windows https://mobaxterm.mobatek.net/ win平台全协议RDP/VNC/SSH等 FinalShell windows http://www.hostbuf.com/ win平台全协议RDP/VNC/SSH等 Remote Desktop Manager windows https://remotedesktopmanager.com/ win平台全协议RDP/VNC/SSH等 Terminals windows https://github.com/terminals-Origin/Terminals win平台全协议RDP/VNC/SSH等 Royal TSX windows https://www.royalapplications.com/ts/win/features win平台全协议RDP/VNC/SSH等 putty windows https://www.putty.org/ 简单的ssh工具 KiTTY windows http://www.9bis.net/kitty/ fork自putty的ssh工具 Cmder windows https://bliker.github.io/cmder/ win下的命令行扩展，配合kitty使用 SecureCRT windows https://www.vandyke.com/products/securecrt/index.html ssh的终端仿真工具 VNC Connect 多平台 https://www.realvnc.com/en/connect/download/vnc/ 连接VNC server的客户端 rdesktop linux http://www.rdesktop.org/ linux下远程连接windows的工具 ConnectBot 安卓 https://connectbot.org Microsoft Remote Desktop 安卓/ios 微软官方提供的Windows连接工具 Bitvise SSH Client windows https://www.bitvise.com/ Xshell windows https://www.netsarang.com/products/xsh_overview.html widows下的ssh连接工具 Chrome Secure Shell 浏览器 https://chrome.google.com/webstore/detail/secure-shell/pnhechapfaindjhompbnflcldabbghjo 基于浏览器(Chrome)的ssh客户端 FIreSSH 浏览器 http://firessh.net/ 基于浏览器(Firefox)的ssh客户端 Butterfly 浏览器 https://github.com/paradoxxxzero/butterfly 浏览器中运行的xterm.js兼容终端 xterm.js 浏览器 https://xtermjs.org/ 基于浏览器的ssh客户端 SSH Secure Shell Client windows https://www.ssh.com/ssh/ NoMachine 多平台 https://www.nomachine.com/ 私有远程NX协议、SSH协议 xpra 多平台 http://xpra.org 私有远程xpra协议、SSH协议 winscp windows https://winscp.net/eng/docs/lang:chs windows下向Linux的传输文件工具 Xftp windows https://www.netsarang.com/products/xfp_overview.html windows下向Linux的传输文件工具 "},"notes/other/gitbook.html":{"url":"notes/other/gitbook.html","title":"GitBook 相关","keywords":"","body":"一、gitbook 相关 集成了 GitBook、Git、Markdown 等功能，还支持将书籍同步到 gitbook.com 网站，使我们可以很方便地编辑和管理书籍。 1、安装nodejs curl -sL https://rpm.nodesource.com/setup_10.x | sudo bash - yum install -y nodejs node --version 2、安装gitbook npm install -g gitbook-cli 3、初始化 mkdir mybook gitbook init 4、编辑SUMMARY.md # 目录 * [前言](README.md) * [第一章](Chapter1/README.md) * [第1节：衣](Chapter1/衣.md) * [第2节：食](Chapter1/食.md) * [第3节：住](Chapter1/住.md) * [第4节：行](Chapter1/行.md) * [第二章](Chapter2/README.md) * [第三章](Chapter3/README.md) * [第四章](Chapter4/README.md) 再次执行 gitbook init 5、运行服务浏览mybook gitbook serve or gitbook serve --port 8888 6、构建html gitbook build 将构建书籍，默认生成html输出到_book目录中。 7、生成pdf、epub、mobi格式电子书 npm install -g ebook-convert gitbook pdf ./ ./mybook.pdf gitbook epub ./ ./mybook.epub gitbook mobi ./ ./mybook.mobi 8、配置目录折叠 在主目录添加book.json { \"plugins\":[ \"expandable-chapters\" ] } gitbook install or 直接使用命令安装 npm install -g gitbook-plugin-splitter * [第一章](Chapter1/README.md) * [第1节：衣](Chapter1/衣.md) * [第2节：食](Chapter1/食.md) * [第3节：住](Chapter1/住.md) * [第4节：行](Chapter1/行.md) 9、常用插件 https://www.jianshu.com/p/427b8bb066e6 https://www.cnblogs.com/mingyue5826/p/10307051.html#autoid-2-3-0 10、gitbook serve 找不到 fontsettings.js cd ~/.gitbook\\versions\\3.2.3\\lib\\output\\website vim copyPluginAssets.js 删除112行 "},"notes/other/github_pages.html":{"url":"notes/other/github_pages.html","title":"GitHub Pages","keywords":"","body":"github pages 首个仓库 1、创建仓库，上传html静态页面 2、Settings -- 修改仓库名为，hlyani.github.io 格式。 -- GitHub Pages * 选择source，一般master分支 * 选择theme 3、https://hlyani.github.io 创建第二个仓库 1、创建仓库test，上传html静态页面 2、设置github pages 3、访问https://hlyani.github.io/test "},"notes/other/web.html":{"url":"notes/other/web.html","title":"Web 相关","keywords":"","body":"Web 相关 一、HTML/CSS部分 1、什么是盒子模型？ 在网页中，一个元素占有空间的大小由几个部分构成，其中包括元素的内容（content），元素的内边距（padding），元素的边框（border），元素的外边距（margin）四个部分。这四个部分占有的空间中，有的部分可以显示相应的内容，而有的部分只用来分隔相邻的区域或区域。4个部分一起构成了css中元素的盒模型。 2、行内元素有哪些？块级元素有哪些？ 空(void)元素有那些？ 行内元素：a、b、span、img、input、strong、select、label、em、button、textarea 块级元素：div、ul、li、dl、dt、dd、p、h1-h6、blockquote 空元素：即系没有内容的HTML元素，例如：br、meta、hr、link、input、img 3、CSS实现垂直水平居中 一道经典的问题，实现方法有很多种，以下是其中一种实现： HTML结构： CSS： .wrapper{position:relative;} .content{ background-color:#6699FF; width:200px; height:200px; position: absolute; //父元素需要相对定位 top: 50%; left: 50%; margin-top:-100px ; //二分之一的height，width margin-left: -100px; } 4、简述一下src与href的区别 href 是指向网络资源所在位置，建立和当前元素（锚点）或当前文档（链接）之间的链接，用于超链接。 src是指向外部资源的位置，指向的内容将会嵌入到文档中当前标签所在位置；在请求src资源时会将其指向的资源下载并应用到文档内，例如js脚本，img图片和frame等元素。当浏览器解析到该元素时，会暂停其他资源的下载和处理，直到将该资源加载、编译、执行完毕，图片和框架等元素也如此，类似于将所指向资源嵌入当前标签内。这也是为什么将js脚本放在底部而不是头部。 5、什么是CSS Hack? 一般来说是针对不同的浏览器写不同的CSS,就是 CSS Hack。 IE浏览器Hack一般又分为三种，条件Hack、属性级Hack、选择符Hack（详细参考CSS文档：css文档）。例如： 1、条件Hack .test{color:red;} 2、属性Hack .test{ color:#090\\9; /* For IE8+ */ *color:#f00; /* For IE7 and earlier */ _color:#ff0; /* For IE6 and earlier */ } 3、选择符Hack - html .test{color:#090;} /* For IE6 and earlier */ - - html .test{color:#ff0;} /* For IE7 */ 6、简述同步和异步的区别 同步是阻塞模式，异步是非阻塞模式。 同步就是指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去； 异步是指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态。当有消息返回时系统会通知进程进行处理，这样可以提高执行的效率。 7、px和em的区别 px和em都是长度单位，区别是，px的值是固定的，指定是多少就是多少，计算比较容易。em得值不是固定的，并且em会继承父级元素的字体大小。 浏览器的默认字体高都是16px。所以未经调整的浏览器都符合: 1em=16px。那么12px=0.75em, 10px=0.625em 8、什么叫优雅降级和渐进增强？ 渐进增强 progressive enhancement： 针对低版本浏览器进行构建页面，保证最基本的功能，然后再针对高级浏览器进行效果、交互等改进和追加功能达到更好的用户体验。 优雅降级 graceful degradation： 一开始就构建完整的功能，然后再针对低版本浏览器进行兼容。 区别： a. 优雅降级是从复杂的现状开始，并试图减少用户体验的供给 b. 渐进增强则是从一个非常基础的，能够起作用的版本开始，并不断扩充，以适应未来环境的需要 c. 降级（功能衰减）意味着往回看；而渐进增强则意味着朝前看，同时保证其根基处于安全地带 9、浏览器的内核分别是什么? IE: trident内核 Firefox：gecko内核 Safari：webkit内核 Opera：以前是presto内核，Opera现已改用Google Chrome的Blink内核 Chrome：Blink(基于webkit，Google与Opera Software共同开发) 二、JavaScript部分 1、怎样添加、移除、移动、复制、创建和查找节点？ 1）创建新节点 createDocumentFragment() //创建一个DOM片段 createElement() //创建一个具体的元素 createTextNode() //创建一个文本节点 2）添加、移除、替换、插入 appendChild() //添加 removeChild() //移除 replaceChild() //替换 insertBefore() //插入 3）查找 getElementsByTagName() //通过标签名称 getElementsByName() //通过元素的Name属性的值 getElementById() //通过元素Id，唯一性 2、实现一个函数clone，可以对JavaScript中的5种主要的数据类型（包括Number、String、Object、Array、Boolean）进行值复制。 - 对象克隆 - 支持基本数据类型及对象 - 递归方法 function clone(obj) { var o; switch (typeof obj) { case \"undefined\": break; case \"string\": o = obj + \"\"; break; case \"number\": o = obj - 0; break; case \"boolean\": o = obj; break; case \"object\": // object 分为两种情况 对象（Object）或数组（Array） if (obj === null) { o = null; } else { if (Object.prototype.toString.call(obj).slice(8, -1) === \"Array\") { o = []; for (var i = 0; i 3、如何消除一个数组里面重复的元素？ // 方法一： var arr1 =[1,2,2,2,3,3,3,4,5,6], arr2 = []; for(var i = 0,len = arr1.length; i4、想实现一个对页面某个节点的拖曳？如何做？（使用原生JS）。 5、在Javascript中什么是伪数组？如何将伪数组转化为标准数组？ 伪数组（类数组）：无法直接调用数组方法或期望length属性有什么特殊的行为，但仍可以对真正数组遍历方法来遍历它们。典型的是函数的argument参数，还有像调用getElementsByTagName,document.childNodes之类的,它们都返回NodeList对象都属于伪数组。可以使用Array.prototype.slice.call(fakeArray)将数组转化为真正的Array对象。 function log(){ var args = Array.prototype.slice.call(arguments); //为了使用unshift数组方法，将argument转化为真正的数组 args.unshift('(app)'); console.log.apply(console, args); }; 6、Javascript中callee和caller的作用？ caller是返回一个对函数的引用，该函数调用了当前函数； callee是返回正在被执行的function函数，也就是所指定的function对象的正文。 7、请描述一下cookies，sessionStorage和localStorage的区别 sessionStorage用于本地存储一个会话（session）中的数据，这些数据只有在同一个会话中的页面才能访问并且当会话结束后数据也随之销毁。因此sessionStorage不是一种持久化的本地存储，仅仅是会话级别的存储。而localStorage用于持久化的本地存储，除非主动删除数据，否则数据是永远不会过期的。 web storage和cookie的区别 Web Storage的概念和cookie相似，区别是它是为了更大容量存储设计的。Cookie的大小是受限的，并且每次你请求一个新的页面的时候Cookie都会被发送过去，这样无形中浪费了带宽，另外cookie还需要指定作用域，不可以跨域调用。 除此之外，Web Storage拥有setItem,getItem,removeItem,clear等方法，不像cookie需要前端开发者自己封装setCookie，getCookie。但是Cookie也是不可以或缺的：Cookie的作用是与服务器进行交互，作为HTTP规范的一部分而存在 ，而Web Storage仅仅是为了在本地“存储”数据而生。 8、手写数组快速排序 关于快排算法的详细说明，可以参考阮一峰老师的文章快速排序 “快速排序”的思想很简单，整个排序过程只需要三步： （1）在数据集之中，选择一个元素作为”基准”（pivot）。 （2）所有小于”基准”的元素，都移到”基准”的左边；所有大于”基准”的元素，都移到”基准”的右边。 （3）对”基准”左边和右边的两个子集，不断重复第一步和第二步，直到所有子集只剩下一个元素为止。 9、统计字符串”aaaabbbccccddfgh”中字母个数或统计最多字母数。 var str = \"aaaabbbccccddfgh\"; var obj = {}; for(var i=0;i10、写一个function，清除字符串前后的空格。（兼容所有浏览器） function trim(str) { if (str && typeof str === \"string\") { return str.replace(/(^\\s*)|(\\s*)$/g,\"\"); //去除前后空白符 } } 三、其他 1、一次完整的HTTP事务是怎样的一个过程？ 基本流程： a. 域名解析 b. 发起TCP的3次握手 c. 建立TCP连接后发起http请求 d. 服务器端响应http请求，浏览器得到html代码 e. 浏览器解析html代码，并请求html代码中的资源 f. 浏览器对页面进行渲染呈现给用户 2、对前端工程师这个职位你是怎么样理解的？ a. 前端是最贴近用户的程序员，前端的能力就是能让产品从 90分进化到 100 分，甚至更好 b. 参与项目，快速高质量完成实现效果图，精确到1px； c. 与团队成员，UI设计，产品经理的沟通； d. 做好的页面结构，页面重构和用户体验； e. 处理hack，兼容、写出优美的代码格式； f. 针对服务器的优化、拥抱最新前端技术。 四、Web网站调优 1、尽可能减少HTTP请求：图片合并 （css sprites），Js脚本文件合并、css文件合并。 2、减少DNS查询 3、将css放在页面最上面，将js放在页面最下面 4、压缩js和css 减少文件体积，去除不必要的空白符、格式符、注释（即对代码进行格式化） 5、把js和css提取出来放在外部文件中 这一条要灵活运用，把js和css提取出来放在外部文件的优点是：减少html体积，提高了js和css的复用性，提高日后的可维护性 缺点：增加了http请求，不过这一点可以通过缓存来解决。 什么情况下将js和css写在页面内呢，可以分为几种情况：js和css代码比较少；这个页面不怎么会访问 6、避免重定向 重定向就是用户请求的页面被转移到了别的地方，浏览器向服务请请求一个页面，服务器告诉浏览器请求的页面已经被转移到另外一个页面，并告知另一个页面地址，浏览器就再发送请求到重定向的地址。这样会增加服务器和浏览器之间的往返次数，影响网站性能。 重定向状态码有：301永久重定向 302临时重定向。304 not modified 并不是真的重定向，它是用来告诉浏览器get请求的文件在缓存中，避免重新下载。 7、移除重复脚本 8、使用ajax缓存 ajax的get和post方法： 只要是浏览器的get请求，浏览器都会使用缓存，对于同一地址的请求，服务器会发送304状态码到浏览器，浏览器就会使用缓存中的数据 post的请求每次都会被执行，浏览器不会缓存 9、使用Gzip压缩 10、使用CDN(内容分发网络 五、数据库调优 数据库的调优，总的来说分为以下三部分： 1、SQL调优：主要集中在索引、减少跨表与大数据join查询等。 2、数据库端架构设计优化： 通过读写分离调整对数据库的写操作，通过垂直拆分以及水平拆分(分库分表)来解决数据库端连接池瓶颈等问题。 3、连接池调优 可以通过熟悉连接池的原理，以及具体的连接池监控数据，来不断调试出最终的连接池参数。 六、通过缓存减少后端压力 目前分布式缓存已经比较成熟，常见的有redis、memcached以及开源的淘宝分布式tair等。 选型考虑 如果数据量小，并且不会频繁地增长又清空（这会导致频繁地垃圾回收），那么可以选择本地缓存。具体的话，如果需要一些策略的支持（比如缓存满的逐出策略），可以考虑Ehcache；如不需要，可以考虑HashMap；如需要考虑多线程并发的场景，可以考虑ConcurentHashMap。 缓存是否会满，缓存满了怎么办？ 对于一个缓存服务，理论上来说，随着缓存数据的日益增多，在容量有限的情况下，缓存肯定有一天会满的。如何应对？ 1、 给缓存服务，选择合适的缓存逐出算法，比如最常见的LRU。 2、 针对当前设置的容量，设置适当的警戒值，比如10G的缓存，当缓存数据达到8G的时候，就开始发出报警，提前排查问题或者扩容。 3、 给一些没有必要长期保存的key，尽量设置过期时间。 七、数据请求改为异步 使用场景 用户并不关心或者用户不需要立即拿到这些事情的处理结果，这种情况就比较适合用异步的方式处理，这里的原则就是能异步就异步。 常见做法 一种做法，是额外开辟线程，这里可以采用额外开辟一个线程或者使用线程池的做法，在IO线程（处理请求响应）之外的线程来处理相应的任务，在IO线程中让response先返回。 如果异步线程处理的任务设计的数据量非常巨大，那么可以引入阻塞队列BlockingQueue作进一步的优化。具体做法是让一批异步线程不断地往阻塞队列里扔数据，然后额外起一个处理线程，循环批量从队列里拿预设大小的一批数据，来进行批处理（比如发一个批量的远程服务请求），这样进一步提高了性能。 另一种做法，是使用消息队列（MQ）中间件服务，MQ天生就是异步的。 八、性能调优总结： 大型网站的性能瓶颈大部分瓶颈都在数据库端，所以性能调优总是沿着如何减少对后端的压力来操作，数据库端的瓶颈经常会造成应用端的雪崩(比如：sql查询过长，长事务)等，所以需要及时解决后端性能。 1.通过读写分离、垂直拆分、水平拆分降低对数据库后端的压力。 2.通过优化sql语句，索引等，缩短对sql的查询时间。 2.通过缓存以及CDN来解决对图片、文件等的读操作，避免对数据库产生压力。 3.通过对web端的优化，js、css等压缩，提高大文件读取时间，尽量依赖CDN。 4.还有一个重点就是监控：对JVM、线程、sql查询时间等健康指标就行及时监控，通过监控及时发现瓶颈，及时优化。 "},"notes/other/build_atlas.html":{"url":"notes/other/build_atlas.html","title":"Atlas 驱动构建","keywords":"","body":"Atlas 驱动构建 一、准备内核SDK 略 二、安装编译环境 DEBIAN_FRONTEND=noninteractive apt install -y gcc dracut git make libncurses-dev libelf-dev bison flex libssl-dev bc u-boot-tools vim 三、编译 atlas 1、下载解压 makeself mkdir rebuild cd rebuild wget https://github.com/megastep/makeself/archive/release-2.4.0.tar.gz tar -zxvf release-2.4.0.tar.gz 2、解压driver驱动包 ./A300-3000-3010-npu-driver_20.2.0_linux-aarch64.run --noexec --extract=./tmp 3、解压repack包 tar -zxvf A300-3000-npu-driver-repack-tools_20.2.0.tar.gz --strip-components 1 4、重构 bash build.sh ./makeself-release-2.4.0/makeself.sh tmp/ A300-3000-3010-npu-driver_20.2.0_linux_XXX-aarch64.run "},"notes/other/be_mined.html":{"url":"notes/other/be_mined.html","title":"记一次被挖矿","keywords":"","body":"记一次被挖矿 [root@hl tmp]# top top - 09:56:29 up 29 days, 20:47, 4 users, load average: 96.61, 96.79, 96.79 Tasks: 530 total, 1 running, 529 sleeping, 0 stopped, 0 zombie %Cpu(s): 99.3 us, 0.7 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 13176443+total, 1525340 free, 48863452 used, 81375632 buff/cache KiB Swap: 0 total, 0 free, 0 used. 81065032 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 770 HwHiAiU+ 20 0 4882144 4.6g 2056 S 242.3 3.7 189669:14 cnrig 9375 HwHiAiU+ 20 0 4728380 4.5g 2056 S 215.4 3.6 22581:33 cnrig 15413 HwHiAiU+ 20 0 4728512 4.5g 2056 S 215.4 3.6 22547:59 cnrig 15414 HwHiAiU+ 20 0 4728508 4.5g 2056 S 215.4 3.6 22550:02 cnrig 26479 HwHiAiU+ 20 0 4790024 4.5g 2056 S 192.3 3.6 68386:18 cnrig 17785 HwHiAiU+ 20 0 4943540 4.7g 2056 S 188.5 3.7 259641:55 cnrig [root@hl tmp]# ps -ef|grep cnrig HwHiAiU+ 770 1 99 Nov08 ? 131-17:08:04 ./cnrig -o 94.130.12.27:80 -u 84kGKA9ehor2p2EeEogxnTeBm2kJi9DUP6uMHnohQmhqDZN3LNQxSVv2cZ5fLaFVaJK1c9rEnBuEbCWhN2kQVnFJ9KSk2wk -p voltage -B HwHiAiU+ 9375 1 99 Nov25 ? 15-16:20:23 ./cnrig -o 94.130.12.27:80 -u 84kGKA9ehor2p2EeEogxnTeBm2kJi9DUP6uMHnohQmhqDZN3LNQxSVv2cZ5fLaFVaJK1c9rEnBuEbCWhN2kQVnFJ9KSk2wk -p voltage -B HwHiAiU+ 15413 1 99 Nov25 ? 15-15:46:49 ./cnrig -o 94.130.12.27:80 -u 84kGKA9ehor2p2EeEogxnTeBm2kJi9DUP6uMHnohQmhqDZN3LNQxSVv2cZ5fLaFVaJK1c9rEnBuEbCWhN2kQVnFJ9KSk2wk -p voltage -B HwHiAiU+ 15414 1 99 Nov25 ? 15-15:48:52 ./cnrig -o 94.130.12.27:80 -u 84kGKA9ehor2p2EeEogxnTeBm2kJi9DUP6uMHnohQmhqDZN3LNQxSVv2cZ5fLaFVaJK1c9rEnBuEbCWhN2kQVnFJ9KSk2wk -p voltage -B HwHiAiU+ 17785 1 99 Nov05 ? 180-07:20:45 ./cnrig -o 94.130.12.27:80 -u 84kGKA9ehor2p2EeEogxnTeBm2kJi9DUP6uMHnohQmhqDZN3LNQxSVv2cZ5fLaFVaJK1c9rEnBuEbCWhN2kQVnFJ9KSk2wk -p voltage -B root 20375 3809 0 09:56 pts/0 00:00:00 grep --color=auto cnrig HwHiAiU+ 26479 1 99 Nov19 ? 47-11:45:08 ./cnrig -o 94.130.12.27:80 -u 84kGKA9ehor2p2EeEogxnTeBm2kJi9DUP6uMHnohQmhqDZN3LNQxSVv2cZ5fLaFVaJK1c9rEnBuEbCWhN2kQVnFJ9KSk2wk -p voltage -B [root@hl tmp]# uptime 09:56:43 up 29 days, 20:47, 4 users, load average: 96.83, 96.83, 96.81 "}}